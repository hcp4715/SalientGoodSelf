---
title             : "The good person is me: Spontaneous self-referential process prioritizes moral character in perceptual matching"
shorttitle        : "Priorization of good character is self-referential"

author: 
  - name          : "Hu Chuan-Peng"
    affiliation   : "1, 2"
    corresponding : yes    # Define only one corresponding author
    address       : "School of Psychology, Nanjing Normal University, Ninghai Road 122, Gulou District, 210024 Nanjing, China"
    email         : "hcp4715@hotmail.com"
  - name          : "Kaiping Peng"
    affiliation   : "2"
  - name          : "Jie Sui"
    affiliation   : "3"

affiliation:
  - id            : "1"
    institution   : "Nanjing Normal University, 210024 Nanjing, China"
  - id            : "2"
    institution   : "Tsinghua University, 100084 Beijing, China"
  - id            : "3"
    institution   : "University of Aberdeen, Aberdeen, Scotland"

authornote: |
  Hu Chuan-Peng, School of Psychology, Nanjing Normal University, 210024 Nanjing, China.
  Kaiping Peng, Department of Psychology, Tsinghua University, 100084 Beijing, China.
  Jie Sui, School of Psychology, University of Aberdeen, Aberdeen, Scotland.
  Authors contriubtion: HCP, JS, & KP design the study, HCP collected the data, HCP analyzed the data and drafted the manuscript. All authors read and agreed upon the current version of the manuscripts.

abstract: |
 Moral character is central to social evaluation and moral judgment. However, whether moral character information is prioritized in perceptual decision-making was debated. Here we investigated the effect of moral character on perceptual decision-making through an associative learning task. Participants first learned associations between different geometric shapes and moral characters and then performed a simple perceptual matching task. Across five experiments (N = 192 ), we found a robust prioritization effect of good character-related information, i.e., participants responded faster and more accurately to shapes that were associated with good characters than shapes associated with neutral or bad characters. We then examine whether the prioritization of good character was due to valence alone or an interaction between valence and self-reference. Data from three experiments (N = 108) demonstrated that the prioritization effect of good character was robust when the good character referred to the self but weak or non-exist when it referred to others. Additional two experiments (N = 104) further revealed that the mutual facilitation between good character and self-reference occurred even when one of them was task-irrelevant. Together, these results suggested a spontaneous self-referential process as a mechanism of the prioritization effect of good character.
  
 <!-- https://tinyurl.com/ybremelq -->
  
keywords          : "Perceptual decision-making, Self positivity bias, moral character"
wordcount         : "X"

bibliography      : 
  - r-references.bib
  - endnote.bib

floatsintext      : no
figurelist        : no
tablelist         : no
footnotelist      : no
linenumbers       : yes
mask              : no
draft             : no
figsintext        : no

documentclass     : "apa6"
classoption       : "man"
output            : 
  papaja::apa6_pdf:
    latex_engine  : xelatex

header-includes:
  - \usepackage{rotating}
  - \DeclareDelayedFloatFlavor{sidewaysfigure}{figure}
---
 <!-- This documents -->
 
```{r setup, include = FALSE}
# rm(list = ls())
source('Initial.r')

curDir = here::here()              # Get the current directory
figDir = here::here('figures')     # directory for figures.

# Seed for random number generation
set.seed(42)
options(tinytex.verbose = T) # debug the tex
knitr::opts_chunk$set(message = FALSE)
knitr::opts_chunk$set(cache.extra = knitr::rand_seed)
```
 <!-- What is the theoretic meaning of the series study? -->

<!--Alternative title: Self-relevance modulates the prioritization of the good character in perceptual matching -->

# Introduction

 <!--[sentences in bracket are key ideas] -->

<!--[quotes about moral character] -->

<!-- 
Ideas: 
little value in relying on people's self-reported moral principles or moral ideals to predict their real life behaviors, "paradox of morality" (Ellemers, 2019);
moral associative learning as an stronger predictor??
-->

<!--
Big problem in science, 
field domain
what field knows
remaining gap

Summary:
        our approach
        our results
-->

Is moral information prioritized in perception? This question has evoked much heat a few years ago but remains unsolved. On the one hand, morality is a basic dimension in social evaluation [@dunbar_gossip_2004; @ellemers_morality_2018; @goodwin_moral_2014; @goodwin_moral_2015], this importance should grant moral information more salient than morally neutral information and thus prioritized when the attentional resource is limited. This logic is similar to other stimuli that are also important to humans, e.g., threatening stimuli [e.g., @ohman_face_2001], rewards [@anderson_value_2011], or self-related stimuli [@sui_self_attention_2019]. Indeed, previous studies reported bad characters are prioritized in visual processing [@anderson_visual_2011; @eiserbeck_visual_2020], suggesting that bad people are detected faster than neutral or good people. On the other hand, there is evidence against the view that morally bad information is prioritized in perception. First, researchers reported positive bias in processing moral-related information. For example, @shore_social_2013 found that faces with positive interaction in a trust game were prioritized in the pre-attentive process. Second, the negative bias in perceiving moral information is not robust [@stein_no_2017]. Third, the mechanism underlying the reported negative bias in processing moral-related information is debated [@firestone_cognition_2015; @firestone_moral_2016; @jussim_interpretations_2016]. In short, while the importance of morality is widely recognized, whether moral information is prioritized in perceptual decision-making is still an open question. Here we manipulated the moral character by an associated learning task and investigated whether immediately acquired moral character information is prioritized in a perceptual matching task.
 
If moral character information is indeed prioritized, the next question is how? Previous studies explain the effect based on valence. For example, the negative bias toward moral information is explained by aligning moral information with affective stimuli and threat detection was supposed to be the potential mechanism [@anderson_value_2011]. The positive bias toward moral information, on the other hand, is explained by value-based attention [@shore_social_2013]. However, these explanations often ignore the fact the value is subjective *per se* [@juechems_where_2019]. Merely associating with the self can prioritize the stimuli in perception, attention, working memory, and long-term memory [@sui_self_attention_2019; @sui_tics_2015]. Here, we explicitly included self-relevance in our experimental design and tested whether the prioritization of moral character is modulated by self-relevance. We adopted an associative learning task, or self-tagging task, which has been widely used in studying the self-relevance effect. It is based on the well-established fact that humans can quickly learn the associations between symbols via language and change subsequent behaviors accordingly. This associative learning is widely used in aversive learning and value-based learning [@atlas_instructions_2022; @deltomme_instructed_2018]. By explicitly instructing participants on which moral character is self-referencing and which is not, we can test whether the prioritization of moral character is by valence *per se* or by the self-referential of moral valence. 

We address these questions by investigating how immediately acquired moral character information modulates the processing of neutral geometric shapes in a perceptual matching task. Unlike previous studies relies on faces or words as materials, stimuli used in the social associative task are geometric shapes, which acquire moral meaning before the perceptual matching task. Moreover, associations between shapes and different labels of moral characters are counter-balanced between participants, thus eliminating confounding effects by stimuli. Also, because we only used a few stimuli and they were repeatedly presented during the task, the results can not be explained by semantic priming [@unkelbach_chapter_2020], which is the center of the debate on previous results [@gantman_moral_2015; @gantman_see_2016; @firestone_cognition_2015; @Firestone_2016_BBS; @jussim_interpretations_2016]. We examined whether participants’ performance in the perceptual matching task was altered by the immediately acquired moral character of the shapes — in particular, whether the shapes associated with good or bad character are prioritized. We found a robust effect that shapes associated with good character are prioritized in the perceptual matching task. In a series of control experiments, we confirmed that moral content drove the prioritization effect, instead of other factors such as familiarity. In the subsequent experiments, we further tested whether the prioritization of moral character was caused by the valence of moral character alone or the interaction between valence and self-referential processing and found that only shapes associated with both good character and the self are prioritized, suggesting spontaneous moral self-referential as a novel mechanism underlying prioritization of good character in perceptual decision-making.

<!-- 
we attempted to distinguish these two possibilities by a social associative learning task in which physical features had minimal influences — participants performed a perceptual matching task after associated different moral characters (good, neutral, and bad) with different geometric shapes. If there is a positivity effect, there should be an advantage for shapes associated with good character over shapes associated with neutral or bad shapes. If there is a negativity effect, the advantage should be occur on shapes associated with bad characters. The first four experiments and two additional follow-up experiments provided strong evidence for good character effect in the current paradigm.

The positivity effect consistent with previous studies where positivity effect of social trait words were found [@anselmi_positive_2011; @bargh_generality_1992; @unkelbach_good_2010]. However, the effect could not be explained by the similarity hypothesis [@unkelbach_chapter_2020] because we only used three stimuli. There are two possibility explanations. The first one is the value-based attention account, which suggests that stimuli that are valuable to us are prioritized [@anderson_neurobiology_2019]. In our experiments, the good character label "good person" may represent an indirect but valuable stimuli because, in social life, a good other is usually more valuable than an bad other [@abele_agency_2007]. Another possibility is derived from social categorization theory, which suggested that we automatically categorize others as in-group or out-group [@turner_rediscovering_1987]. Moral character is an important criterion for social categorization [@mchugh_moral_2021; @descioli_side_taking_2016]. However, the above four experiments could not distinguish between these two possibilities, because "good person" could both be rewarding and be categorized as in-group member. Given that both rewarding stimuli [e.g., @Sui_2012_JEPHPP] and in-group information [@enock_overlap_2020] are prioritized when using social associative learning paradigm, we further tested these two possibilities in new experiments.

To distinguish the value-based account and the social categorization explanations, we introduced the identity (self- vs. other-referential) of moral character as an addition independent variable in exp 3a, 3b, and 6b. Now moral valence is orthogonal to the identity. In this case, the identity of moral character information become salient and participants are less likely to spontaneously categorize a good-other as an extension of self, but the value of good-person still exists. If the positivity effect was driven by social categorization theory, then participants prioritize good-self but not good-other. If the value-based attention theory is true, then, both good-self and good-other are prioritized, or maybe good-other are even more prioritized. 

Although the introduction of self- and other-referential processing provided evident that value-based account can not explain the good-character effect, it might introduce the good-self effect, i.e., the good-self is prioritized over all the other stimuli. This effect, if true, may suggest underlying mechanisms other than social-categorization. For example, the moral true self account. Moral true self view suggested that moral self if the true self [@strohminger_true_2017]. Therefore, even good-self can be viewed as categorized to in-group, it can also be viewed as the core of the self and it is the anchor of all the other effects.

To test the moral true self view and the social-categorization account, we designed two complementary experiments. In experiment 4a, participants only learned the association between self and other, the words "good-person", "neutral person", and "bad person" were presented as task-irrelevant stimuli, while in experiment 4b, participants learned the associations between "good-person", "neutral-person", and "bad-person", and the "self" and "other" were presented as task-irrelevant stimuli. These two experiments can be used to distinguish the moral-self view and social categorization" account. If moral-self view is true, then, in both experiments, good-self will show advantage over all other stimuli, and there will be no other effects. More specifically, in experiment 4a, where only the self-referential processing is task-relevant, there will be advantage for good as task-irrelevant condition than when bad or neutral character as task-irrelevant for the self conditions, while there is no other effects; in experiment 4b, in the good condition, there will be an advantage for self as task-irrelevant condition over other as task-irrelevant condition, and no other effects. If social categorization is true, then, the prioritization effect will depends on whether the stimuli can be categorized as the same group of good-self. More specifically, in experiment 4a, there will be good effect in self conditions, this prediction is the same as the moral self-view; it predicts a reverse good effect in other condition because good and other a conflict in terms of social-categorization, this prediction is different from the "good-self" anchor account; however, for experiment 4b, it predicts no identity effect in the good-person condition because both self and other are in the good group.


[Good self in self-reported data] 
As an exploration, we also collected participants' self-reported psychological distance between self and good-person, bad-person, and neutral-person, moral identity, moral self-image, and self-esteem. All these data are available [see @Liu_2020_JOPD]. We explored the correlation between self-reported distance and these questionnaires as well as the questionnaires and behavioral data. However, given that the correlation between self-reported score and behavioral data has low correlation [@dang_why_2020], we didn't expect a high correlation between these self-reported measures and the behavioral data.
-->

 <!-- 
Key concepts and discussing points:

**Self-categories** are cognitive groupings of self and some class of stimuli as identical or different from some other class. [Turner et al.]

**Personal identity** refers to self-categories that define the individual as a unique person in terms of his or her individual differences from other (in-group) persons.

**Social identity** refers to the shared social categorical self ("us" vs. "them").

**Variable self**: Who we are, how we see ourselves, how we define our relations to others (indeed whether they are construed as ‘other’ or as part of the extended 'we' self) is different in different settings. 

**Identification**: the degree to which an individual feels connected to an ingroup or includes the ingroup in his or her self-concept. (self is not bad; )

Morality as a way for social-categorization [@mchugh_moral_2021]? People are more likely to identify themselves with trustworthy faces [@verosky_differential_2010] (trustworthy faces has longer RTs).

What is the relation between morally good and self in a semantic network (attractor network) (Freeman & Ambady, 2011)? The psychological essentialism account proposed that the moral good self is perspective independent, i.e., there is a moral good self in all. This perspective free effect is not exist in our effect.

How to deal with the *variable self* (self-categorization theory) vs. *core/true/authentic self* vs. *self-enhancement*

**Limitations**:
The perceptual decision-making will show certain pattern under certain task demand. In our case, it's the forced, speed, two-option choice task.

in experiment 4a and 4b, we didn't have a baseline condition where there is no word inside the shape? -->

# Disclosures
We reported all the measurements, analyses, and results in all the experiments in the current study. Participants whose overall accuracy was lower than 60% were excluded from analyses. Also, accurate responses with less than 200ms reaction times were excluded from the analysis. These excluded data can be found in the shared raw data files.

All the experiments reported were not pre-registered. Most experiments (1a ~ 4b, except experiment 3b) reported in the current study were first finished between 2013 to 2016 at Tsinghua University, Beijing, China. Participants in these experiments were recruited from the local community. To increase the sample size of experiments to 50 or more [@Simmons_2013_life], we recruited additional participants from Wenzhou University, Wenzhou, China, in 2017 for experiments 1a, 1b, 4a, and 4b. Experiment 3b was finished at Wenzhou University in 2017 (See Table 1 for an overview of these experiments). 

All participants received informed consent and were compensated for their time. These experiments were approved by the ethics board in the Department of Psychology, Tsinghua University. 

 <!-- 21-word solution (Simmons, Nelson & Simonsohn, 2012; retrieved from http://ssrn.com/abstract=2160588) -->

```{r loadingData,echo=FALSE,results='hide'}
load("AllData.RData")

### exclude the repeated subj from the raw data

# No repeating subj
df1a.v_meta <- df1a.v

# No repeating subj
df1b.v_meta <- df1b.v

# exclude participant from exp 1a
df1c.v_meta <- df1c.v %>% dplyr::filter(!Subject %in% c(1206, 1207, 1208, 1210))

# exclude participant from exp 1a
df2.v_meta <- df2.v %>% dplyr::filter(Subject > 2000)    

# exclude participants from ex1b, 1c, and 2
df3a.v_meta <- df3a.v %>% dplyr::filter(!Subject %in% c(3013, 3012, 3043, 3046)) 

# No repeating subj
df3b.v_meta <- df3b.v

# No repeating subj
df4a.v_meta <- df4a.v

# exclude participants from ex1b, 1c, and 2
df4b.v_meta <- df4b.v %>% dplyr::filter(!Subject %in% c(4210, 4202, 4201))   

# exclude participants from ex1b, 1c, and 2
df5.v_meta <- df5.v %>% dplyr::filter(!Subject %in% c(5201))   

# exclude participants from ex1b, 1c, and 2
df6a.v_meta <- df6a.v %>% dplyr::filter(!Subject %in% c(6118,6119,6122,6123,6131))   

# exclude participants from ex1b, 1c, and 2
df6b.v_meta <- df6b_d1.v %>% dplyr::filter(!Subject %in% c(6217))   

# exclude participants from ex1b, 1c, and 2
df7a.v_meta <- df7a_m.v %>% dplyr::filter(!Subject %in% c(7020))   

# No repeating subj
df7b.v_meta <- df7b_m.v


df1a.v_meta$ExpID <- 'Exp1a'
df1b.v_meta$ExpID <- 'Exp1b'
df1c.v_meta$ExpID <- 'Exp1c'
df2.v_meta$ExpID  <- 'Exp2'
df3a.v_meta$ExpID <- 'Exp3a'
df3b.v_meta$ExpID <- 'Exp3b'
df4a.v_meta$ExpID <- 'Exp4a'
df4b.v_meta$ExpID <- 'Exp4b'
df5.v_meta$ExpID  <- 'Exp5'
df6a.v_meta$ExpID <- 'Exp6a'
df6b.v_meta$ExpID <- 'Exp6b'
df7a.v_meta$ExpID <- 'Exp7a'
df7b.v_meta$ExpID <- 'Exp7b'
```

  <!-- A general method part describing experimental design and data analysis -->
```{r child = "general_method.rmd"}
```

# Results
## Prioritization of good character
To test whether moral characters are prioritized, we modeled data from experiments 1a, 1b, 1c, 2, 5, and 6a with three-level Bayesian hierarchical models. All these experiments shared similar designs and can be used for testing the prioritization effect of moral character. The valid and unique sample size is 192. Note that for both experiments 1a and 1b, two datasets were collected at different time points and locations, thus we treated them as independent samples. Here we only reported the population-level results of three-level Bayesian models, the detailed results of each experiment can be found in supplementary materials.

```{r remove non-meta data, eval = FALSE, echo=FALSE, results='hide', warning=FALSE}
# remove all unnecessary variables
var_list <- c('df1a.v_meta', 'df1b.v_meta', 'df1c.v_meta', 'df2.v_meta', 'df3a.v_meta', 'df3b.v_meta',
              'df4a.v_meta', 'df4b.v_meta', 'df5.v_meta', 'df6a.v_meta', 'df6b.v_meta', 'df7a.v_meta', 'df7b.v_meta',
              
              'exp_table', 'curDir', 'figDir')
rm(list=ls()[! ls() %in% var_list]) # removed all functions ?
```

```{r prepare data for first meta, echo=FALSE, results='hide', warning=FALSE}
### try meta-analysis 1a, 1b, 1c, 2, 5 and 6a
selected_columns <- c('ExpID', 'Site', 'Subject','Age', 'Sex', 'Matchness','Valence', 'RESP', 'ACC','RT')
df_moral <- dplyr::bind_rows(df1a.v_meta[selected_columns],
                             df1b.v_meta[selected_columns],
                             df1c.v_meta[selected_columns],
                             df2.v_meta[selected_columns],
                             df5.v_meta[selected_columns],
                             df6a.v_meta[selected_columns]) %>%
  dplyr::mutate(ExpID_new = paste(ExpID, Site, sep = "_")) %>%
  dplyr::mutate(Valence = factor(Valence, levels = c('Neutral', 'Bad', 'Good')))

df_moral_subj <- df_moral %>%
  dplyr::group_by(ExpID_new, Site) %>%
  dplyr::summarize(N = n_distinct(Subject),
                   N_trial = length(Subject),
                   Exp_conds = 6,
                   trial_per_cond = round((length(Subject)/6)/N, 0))

df_moral <- df_moral %>%
        dplyr::filter(!is.na(RESP)) %>% # filter trials without response
        dplyr::filter(RT > 200) %>%     # filter trials with RT <= 200 ms
        dplyr::mutate(ismatch = ifelse(Matchness == 'Match', 1, 0),
                      saymatch = ifelse((Matchness == 'Match' & ACC == 1) |
                                                (Matchness == 'Mismatch' & ACC == 0), 1, 0)) %>%
        dplyr::select(ExpID_new, Subject, Valence, Matchness, RESP, ACC, RT, ismatch, saymatch) %>%
        dplyr::mutate(ismatch_num = ifelse(Matchness == 'Match', 0.5, -0.5))

# # plot the nested structure of the data
# with(df_moral, table(Subject, ExpID_new)) %>%
#   image(
#     col = grey.colors(80, start = 1, end = 0),
#     axes = TRUE,
#     xlab = "Subject",
#     ylab = "ExpID"
#   )
```

```{r first meta sdt, echo=FALSE, results='hide', warning=FALSE}
# fit a three-level hierarchical model for SDT, didn't specify the prior; dummy coding
# about 20 hours to finish this sampling using ntel® Xeon(R) CPU E3-1505M v5 @ 2.80GHz × 8 machine.
# 87432.5 = 24.3 hours
sdt_val_m1 <- brms::brm(saymatch ~ 0 + Valence + Valence:ismatch + 
                                (0 + Valence + Valence:ismatch | ExpID_new) + 
                                (0 + Valence + Valence:ismatch  | ExpID_new:Subject),
                        family = bernoulli(link="probit"), 
                        data = df_moral,
                        chains = 4,
                        iter = 4000,
                        thin = 2,
                        control = list(adapt_delta = .95),
                        cores = parallel::detectCores(),
                        backend = 'cmdstanr',  # with cmdstanr
                        file = here::here("glmmModels/sdt_val_DummyCode_3_level"))

df_m1_std_fixed_effect <- bayestestR::describe_posterior(
        sdt_val_m1,
        effects = "fixed",
        component = "all",
        ci = 0.95,
        ci_method = 'hdi',
        test = c("p_direction", "p_significance"),
        centrality = "all") %>%
        dplyr::mutate(Valence = dplyr::case_when(
                grepl("ValenceBad", Parameter) ~ "Bad",
                grepl("ValenceNeutral", Parameter) ~ "Neutral",
                grepl("ValenceGood", Parameter) ~ "Good"),
                params = dplyr::case_when(grepl("ismatch", Parameter) ~ "d prime",
                                           !grepl("ismatch", Parameter) ~"criterion"),
                params = factor(params, levels = c('d prime', 'criterion')),
                Valence = factor(Valence, levels = c('Good', 'Neutral',  'Bad'))) 

df_m1_post_sdt_exp <- sdt_val_m1 %>% 
        tidybayes::spread_draws(r_ExpID_new[condition, term]) %>%
        dplyr::rename(value = r_ExpID_new)

pop_mean <- sdt_val_m1 %>%
        tidybayes::gather_draws(`b_.*`, regex = TRUE) %>%
        group_by(.variable) %>%       # this line not necessary (done automatically by spread_draws)
        tidybayes::mean_hdci(.value)  # get the high density continuous intervals

# extract the population level parameters
df_sdt_m1_pop <- sdt_val_m1 %>% 
        tidybayes::gather_draws(`b_.*`, regex = TRUE) %>%
        dplyr::rename(term = .variable,
                      pop_mean = .value) %>%
        dplyr::ungroup() %>%
        dplyr::mutate(term = gsub("b_", "", term))

# Add the population mean to the experimental level and only keep the value after.
df_m1_post_sdt_exp_update <- merge(df_sdt_m1_pop, df_m1_post_sdt_exp, by = c('term','.chain','.iteration', '.draw'), all = T) %>%
        dplyr::mutate(mean_value = pop_mean + value) %>%  # add the population level value to each experiment
        dplyr::select(condition, term, `.chain`, `.iteration`, `.draw`, mean_value) %>% # select the added value
        dplyr::rename(value = mean_value)  # rename the value

# Plot the difference between conditions 
df_m1_plot_sdt <- df_sdt_m1_pop %>%
        dplyr::mutate(condition = 'Overall') %>%
        dplyr::rename(value = pop_mean) %>%
        dplyr::select(condition, term, `.chain`, `.iteration`, `.draw`, value) %>%
        dplyr::bind_rows(., df_m1_post_sdt_exp_update) %>%
        dplyr::mutate(condition = factor(condition, levels = c("Exp1a_THU", "Exp1a_WZU", "Exp1b_THU", 
                                                               "Exp1b_WZU", "Exp1c_THU", "Exp2_THU" , 
                                                               "Exp5_THU",  "Exp6a_THU","Overall")),
                      condition = forcats::fct_rev(condition), # reverse the order because the plot function auto reverse.
                      term = dplyr::case_when((term == "ValenceBad") ~ "c_Bad",
                                              (term == "ValenceNeutral") ~ "c_Neutral",
                                              (term == "ValenceGood") ~ "c_Good",
                                              (term == "ValenceBad:ismatch") ~ "dprime_Bad",
                                              (term == "ValenceNeutral:ismatch") ~ "dprime_Neutral",
                                              (term == "ValenceGood:ismatch") ~ "dprime_Good"),
                      term = factor(term, levels = c("c_Bad", "c_Neutral", "c_Good",
                                                     "dprime_Bad", "dprime_Neutral", "dprime_Good"))) 

df_m1_plot_sdt_diff_wide <- df_m1_plot_sdt %>%
  tidyr::pivot_wider(names_from = c(term), values_from = value) %>%   # long to wide
  dplyr::mutate(diff_GB_c = c_Good - c_Bad,                           # calculate the differences between conditions
                diff_GN_c = c_Good - c_Neutral,
                diff_BN_c = c_Bad - c_Neutral,
                diff_GB_dprm = dprime_Good - dprime_Bad,
                diff_GN_dprm = dprime_Good - dprime_Neutral,
                diff_BN_dprm = dprime_Bad - dprime_Neutral) %>%
  dplyr::select(condition, `.chain`, `.iteration`, `.draw`,
               diff_GB_c,diff_GN_c, diff_BN_c,
               diff_GB_dprm, diff_GN_dprm, diff_BN_dprm)

df_m1_plot_sdt_diff <- df_m1_plot_sdt_diff_wide%>%
  tidyr::pivot_longer(cols = diff_GB_c:diff_BN_dprm, names_to = "term_diff", values_to =  "value") %>%  # wide to long
  dplyr::mutate(term_diff = factor(term_diff, levels = c('diff_GB_c','diff_GN_c', 'diff_BN_c',
                                                         'diff_GB_dprm', 'diff_GN_dprm', 'diff_BN_dprm')))

# plot the posterior of the d prime
# use the overall mean values as the vlines
vlines_df_m1_sdt <- df_m1_plot_sdt %>% 
        tidyr::separate(term, c('params', 'Valence')) %>% 
        dplyr::mutate(Valence = factor(Valence, levels = c('Good', 'Neutral', 'Bad'))) %>%
        dplyr::filter(params == 'dprime') %>%
        dplyr::filter(condition == "Overall") %>%
        dplyr::group_by(Valence) %>% 
        dplyr::summarize(Mean = mean(value)) 

# THIS is the one which the final plot will based on!!!
p_dprime1 <- df_m1_plot_sdt %>%
        tidyr::separate(term, c('params', 'Valence')) %>%
        dplyr::mutate(Valence = factor(Valence, levels = c('Good', 'Neutral', 'Bad'))) %>%
        dplyr::filter(params == 'dprime') %>%
        dplyr::rename(Experiments = condition) %>%
        ggplot2::ggplot(aes(y = Experiments, x = value, color = Valence)) +
        tidybayes::stat_halfeye(aes(fill = Valence), alpha = 0.7) +
        geom_vline(data = vlines_df_m1_sdt, aes(xintercept = Mean,colour = Valence), linetype = "dashed") +
        labs(x=expression('Posteior distribution of '~italic(d)~' prime')) + 
        scale_colour_brewer(palette = "Dark2") +
        scale_fill_brewer(palette = "Dark2") +
        theme_apa()

# define facet titles
contrast_names_m_sdt <- c(`diff_GN_dprm` = "Good vs Neutral", 
                           `diff_BN_dprm` = "Bad vs Neutral"
                           )
# plot the posterior of the difference between d prime
p_dprime1_diff <- df_m1_plot_sdt_diff %>%
        dplyr::filter(str_detect(term_diff, '_dprm')) %>%
        dplyr::filter(!str_detect(term_diff, '_GB_')) %>%
        dplyr::rename(Experiments = condition) %>%
        ggplot2::ggplot(aes(y = Experiments, x = value, fill = after_stat(x > 0))) +
        tidybayes::stat_halfeye() +
        geom_vline(xintercept = 0, linetype = "dashed") +
        # scale_fill_manual(values = c('gray80', 'skyblue')) +
        scale_fill_manual(values = c('skyblue', 'gray80'),
                    breaks = c(TRUE, FALSE),
                    name = "Effect" , labels = c("Yes", "No")) + 
        xlab(expression(paste("Effect of valence on ", italic("d"), " prime", sep = ' '))) + 
        facet_wrap( ~ term_diff, # scales = "free_y",
                    nrow = 1,
                    labeller = as_labeller(contrast_names_m_sdt)) + # label_parsed
        theme_apa() + 
        theme(strip.text.x = element_text(size = 8)) # colour = "orange", angle = 90))

### get results that will be reported in the text
med_d_m1_good <- df_m1_std_fixed_effect$Median[df_m1_std_fixed_effect$Valence == 'Good' & df_m1_std_fixed_effect$params == 'd prime']
ll_d_m1_good <- df_m1_std_fixed_effect$CI_low[df_m1_std_fixed_effect$Valence == 'Good' & df_m1_std_fixed_effect$params == 'd prime']
ul_d_m1_good <- df_m1_std_fixed_effect$CI_high[df_m1_std_fixed_effect$Valence == 'Good' & df_m1_std_fixed_effect$params == 'd prime']

med_d_m1_neut <- df_m1_std_fixed_effect$Median[df_m1_std_fixed_effect$Valence == 'Neutral' & df_m1_std_fixed_effect$params == 'd prime']
ll_d_m1_neut <- df_m1_std_fixed_effect$CI_low[df_m1_std_fixed_effect$Valence == 'Neutral' & df_m1_std_fixed_effect$params == 'd prime']
ul_d_m1_neut <- df_m1_std_fixed_effect$CI_high[df_m1_std_fixed_effect$Valence == 'Neutral' & df_m1_std_fixed_effect$params == 'd prime']

med_d_m1_bad <- df_m1_std_fixed_effect$Median[df_m1_std_fixed_effect$Valence == 'Bad' & df_m1_std_fixed_effect$params == 'd prime']
ll_d_m1_bad <- df_m1_std_fixed_effect$CI_low[df_m1_std_fixed_effect$Valence == 'Bad' & df_m1_std_fixed_effect$params == 'd prime']
ul_d_m1_bad <- df_m1_std_fixed_effect$CI_high[df_m1_std_fixed_effect$Valence == 'Bad' & df_m1_std_fixed_effect$params == 'd prime']

bayestestR::sexit(df_m1_plot_sdt_diff_wide[, 9:10])
```

For the *d* prime, results from the Bayesian model revealed a robust effect of moral character. Shapes associated with good characters ("good person", "kind person" or a name associated with good behaviors) have higher sensitivity (median = `r med_d_m1_good`, 95% HDI = [`r ll_d_m1_good` `r ul_d_m1_good`]) than shapes associated with neutral characters (median = `r med_d_m1_neut`, 95% HDI = [`r ll_d_m1_neut` `r ul_d_m1_neut`]), the difference ($median_{diff}$ = 0.31, 95% HDI [0, 0.62]) has a 97.31% probability of being positive (> 0), 94.91% of being significant (> 0.05). But we did not find difference between shapes associated with bad characters (median = `r med_d_m1_bad`, 95% HDI = [`r ll_d_m1_bad` `r ul_d_m1_bad`]) and neutral character, the difference ($median_{diff}$ = 0.05, 95% HDI  [-0.27, 0.38]) only has a 60.56% probability of being positive (> 0), 49.34% of being significant (> 0.05).

```{r first meta rt, echo=FALSE, results='hide', warning=FALSE}
# have a look at a few participants' data
# set.seed(123)

# fit a three-level hierarchical model for RT, didn't specify the prior, lognormal, effective coding
RT_val_m1 <- df_moral %>%
        dplyr::mutate(RT_sec = RT/1000) %>% # log RT in seconds
        dplyr::filter(ACC == 1) %>%         # only correct trials
        brms::brm(RT_sec ~ Valence*ismatch_num + 
                          (Valence*ismatch_num | ExpID_new) + 
                          (Valence*ismatch_num | ExpID_new:Subject),
                  family=lognormal(),
                  data = .,
                  chains = 4,
                  iter = 4000,
                  thin = 2,
                  control = list(adapt_delta = .95),
                  cores = parallel::detectCores(),
                  backend = 'cmdstanr',  # with cmdstanr
                  file = here::here("glmmModels/RT_val_EffectCode_3_level"))

#Population-Level Effects: 
#                        Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS
#Intercept                  -0.40      0.06    -0.52    -0.27 1.01      837     1301  # baseline: mismatch:neutral
#ValenceBad                  0. 01      0.00     0.00     0.02 1.00     1752     2540  # mismatch:bad - mismatch:neutral = 0.01
#ValenceGood                -0.03      0.00    -0.04    -0.02 1.00     1237     2219  # mismatch:Good - mismatch:neutral = -0.03
#ismatch_num                -0.07      0.01    -0.09    -0.06 1.00     1638     1957  # match:neutral - mismatch:neutral = -0.07
#ValenceBad:ismatch_num      0.02      0.01     0.00     0.04 1.00     1597     2380  # match:bad - ValenceBad -ismatch_num = 0.02
#ValenceGood:ismatch_num    -0.05      0.01    -0.07    -0.03 1.00     1424     1775  # match:good - ValenceGood- ismatch_num = -0.05

# Mismatch:Neutral - Intercept = -0.4
# Mismatch:Bad     - Intercept  + ValenceBad = -0.4 + 0.01 = -0.39
# Mismatch:Good    - Intercept  + ValenceGood = -0.4 - 0.03 = -0.43
# Match: Neutral   - Intercept  + ismatch_num = -0.4 - 0.07 = -0.47
# Match: Bad       - Intercept  + ismatch_num + ValenceBad+ ValenceBad:ismatch_num = -0.4 + 0.01 + 0.02 =  -0.37 
# Match: Good      - Intercept  + ismatch_num + ValenceGood+ ValenceGood:ismatch_num = -0.4 + (-0.03) + (-0.05) = -0.48

# Get the variables in the model 1
# RT_var_name_m1 <- tidybayes::get_variables(RT_val_m1)

df_m1_post_rt_exp <- RT_val_m1 %>% 
        tidybayes::spread_draws(r_ExpID_new[condition, term]) %>%
        dplyr::rename(value = r_ExpID_new)

df_rt_m1_pop_mean <- RT_val_m1 %>%
        tidybayes::gather_draws(`b_.*`, regex = TRUE) %>%
        group_by(.variable) %>%       # this line not necessary (done automatically by spread_draws)
        tidybayes::mean_hdci(.value)  # get the high density continuous intervals

# extract the population level parameters
df_rt_m1_pop <- RT_val_m1 %>% 
        tidybayes::gather_draws(`b_.*`, regex = TRUE) %>%
        dplyr::rename(term = .variable,
                      pop_mean = .value) %>%
        #tidyr::separate(term, c(NA, 'term'), "_") 
        dplyr::ungroup() %>%
        dplyr::mutate(term = gsub("b_", "", term))

# Add the population mean to the experimental level and only keep the value after.
df_m1_post_rt_exp_update <- merge(df_rt_m1_pop, df_m1_post_rt_exp, 
                                  by = c('term','.chain','.iteration', '.draw'), all = T) %>%
        dplyr::mutate(mean_value = pop_mean + value) %>%  # add the population leve value to each experiment
        dplyr::select(condition, term, `.chain`, `.iteration`, `.draw`, mean_value) %>% # select the added value
        dplyr::rename(value = mean_value)  # rename the value

# Plot the difference between conditions 
df_m1_plot_rt <- df_rt_m1_pop %>%
        dplyr::mutate(condition = 'Overall') %>%
        dplyr::rename(value = pop_mean) %>% # chagne the `pop_mean` as `value` for data frame merge
        dplyr::select(condition, term, `.chain`, `.iteration`, `.draw`, value) %>%
        dplyr::bind_rows(., df_m1_post_rt_exp_update) %>%
        dplyr::mutate(condition = factor(condition, levels = c("Exp1a_THU", "Exp1a_WZU", "Exp1b_THU", 
                                                         "Exp1b_WZU", "Exp1c_THU", "Exp2_THU" , 
                                                         "Exp5_THU",  "Exp6a_THU","Overall")),
                      condition = forcats::fct_rev(condition)) %>%
        tidyr::pivot_wider(names_from = c(term), values_from = value) %>%   # long to wide
        dplyr::mutate(Neutral_NM = Intercept,               # calculate the differences between coditions
                Bad_NM = Intercept  + ValenceBad,
                Good_NM = Intercept  + ValenceGood ,
                Neutral_M = Intercept  + ismatch_num,
                Bad_M = Intercept  + ismatch_num + ValenceBad + `ValenceBad:ismatch_num`,
                Good_M = Intercept  + ismatch_num + ValenceGood+ `ValenceGood:ismatch_num`) %>%
        dplyr::select(condition, `.chain`, `.iteration`, `.draw`,
                      Neutral_NM, Bad_NM, Good_NM,
                      Neutral_M, Bad_M, Good_M) %>%
        tidyr::pivot_longer(cols = Neutral_NM:Good_M, names_to = "term", values_to =  "value") %>%  # wide to long
        dplyr::mutate(term = factor(term, levels = c('Good_NM', 'Neutral_NM', 'Bad_NM',
                                                     'Good_M',  'Neutral_M',  'Bad_M')),
                      value = exp(value),
                      value = value * 1000) 

# plot the posterior of the d prime
# use the overall mean values as the vlines
vlines <- df_m1_plot_rt %>% 
        tidyr::separate(term, c('Valence', 'Match')) %>% 
        dplyr::mutate(Valence = factor(Valence, levels = c('Good', 'Neutral', 'Bad'))) %>%
        dplyr::filter(Match == 'M') %>%
        dplyr::filter(condition == "Overall") %>%
        dplyr::group_by(Valence) %>% 
        dplyr::summarize(Mean = mean(value)) # %>%
        # dplyr::arrange(Mean)

df_m1_rt_fixed_effect <- df_m1_plot_rt %>%
        dplyr::filter(condition == "Overall") %>%
        tidyr::pivot_wider(.,
                           id_cols = c('.chain', '.iteration', '.draw'),
                           names_from = term ) %>% 
        dplyr::select(-c('.chain', '.iteration', '.draw')) %>%
        bayestestR::describe_posterior(.,
                                       ci = 0.95,
                                       ci_method = 'hdi',
                                       test = c("p_direction", "p_significance"),
                                       centrality = "median") %>%
        tidyr::separate(Parameter, c('Valence', 'Match')) %>% 
        dplyr::mutate(Valence = factor(Valence, levels = c('Good', 'Neutral', 'Bad')))

# THIS is the one which the final plot will based on!!!
p_rt1 <- df_m1_plot_rt %>%
        tidyr::separate(term, c('Valence', 'Match')) %>% 
        dplyr::mutate(Valence = factor(Valence, levels = c('Good', 'Neutral', 'Bad'))) %>%
        dplyr::filter(Match == 'M') %>%
        dplyr::rename(Experiments = condition) %>%
        ggplot2::ggplot(aes(y = Experiments, x = value, color = Valence)) +
        tidybayes::stat_halfeye(aes(fill = Valence), alpha = 0.7) +
        geom_vline(data = vlines, aes(xintercept = Mean,colour = Valence), linetype = "dashed") +
        labs(x=expression('Posteior distribution of reaction times')) + 
        scale_colour_brewer(palette = "Dark2") +
        scale_fill_brewer(palette = "Dark2") +
        theme_apa()

df_m1_plot_rt_diff_wide <- df_m1_plot_rt %>%
        tidyr::pivot_wider(names_from = c(term), values_from = value) %>%   # long to wide
        dplyr::mutate(diff_GB_NM = Good_NM - Bad_NM,               # calculate the differences between conditions
                diff_GN_NM = Good_NM - Neutral_NM,
                diff_BN_NM = Bad_NM - Neutral_NM,
                diff_GB_M = Good_M - Bad_M, 
                diff_GN_M = Good_M - Neutral_M,
                diff_BN_M = Bad_M - Neutral_M,) %>%
        dplyr::select(condition, `.chain`, `.iteration`, `.draw`,
               diff_GB_NM,diff_GN_NM, diff_BN_NM,
               diff_GB_M, diff_GN_M, diff_BN_M) 

df_m1_plot_rt_diff <- df_m1_plot_rt_diff_wide%>%
        tidyr::pivot_longer(cols = diff_GB_NM:diff_BN_M, names_to = "term_diff", values_to =  "value") %>%  # wide to long
        dplyr::mutate(term_diff = factor(term_diff, levels = c('diff_GB_NM','diff_GN_NM', 'diff_BN_NM',
                                                         'diff_GB_M', 'diff_GN_M', 'diff_BN_M')))

# plot the posterior of matching trials
# define face titles
contrast_names_m_rt <- c(`diff_GN_M` = "Good vs Neutral", 
                         `diff_BN_M` = "Bad vs Neutral"
                           )

p_rt1_diff <- df_m1_plot_rt_diff %>%
        dplyr::filter(str_detect(term_diff, '_M')) %>%
        dplyr::filter(!str_detect(term_diff, '_GB_')) %>%
        dplyr::rename(Experiments = condition) %>%
        ggplot2::ggplot(aes(y = Experiments, x = value, fill = stat(x < 0))) +
        tidybayes::stat_halfeye() +
        geom_vline(xintercept = 0, linetype = "dashed") +
        # scale_fill_manual(values = c('gray80', 'skyblue')) +
        scale_fill_manual(values = c('skyblue', 'gray80'),
                    breaks = c(TRUE, FALSE),
                    name = "Effect" , labels = c("Yes", "No")) + 
        xlab("Effect of valence on RT (Match trials)") +
        facet_wrap( ~ term_diff,
              # scales = "free_y", 
              nrow = 1,
              labeller = as_labeller(contrast_names_m_rt))  + # label_parsed
        theme_apa() + 
        theme(strip.text.x = element_text(size = 8)) # colour = "orange", angle = 90))

df_rt1_diff_hdi <- df_m1_plot_rt_diff %>%
        # dplyr::filter(str_detect(term_diff, '_M')) %>%
        dplyr::filter(condition == "Overall") %>%
        tidyr::pivot_wider(.,
                           id_cols = c('.chain', '.iteration', '.draw'),
                           names_from = term_diff) %>%
        dplyr::select(-c('.chain', '.iteration', '.draw')) %>%
        bayestestR::describe_posterior(.,
                                       ci = 0.95,
                                       ci_method = 'hdi',
                                       test = c("p_direction", "p_significance"),
                                       centrality = "median")

### get results that will be reported in the text
## match trials
med_rt_m_m1_good <- round(df_m1_rt_fixed_effect$Median[df_m1_rt_fixed_effect$Valence == 'Good' & df_m1_rt_fixed_effect$Match == 'M'], digits = 0)
ll_rt_m_m1_good <- round(df_m1_rt_fixed_effect$CI_low[df_m1_std_fixed_effect$Valence == 'Good' & df_m1_rt_fixed_effect$Match == 'M'], digits = 0)
ul_rt_m_m1_good <- round(df_m1_rt_fixed_effect$CI_high[df_m1_rt_fixed_effect$Valence == 'Good' & df_m1_rt_fixed_effect$Match == 'M'], digits = 0)

med_rt_m_m1_neut <- round(df_m1_rt_fixed_effect$Median[df_m1_rt_fixed_effect$Valence == 'Neutral' & df_m1_rt_fixed_effect$Match == 'M'], digits = 0)
ll_rt_m_m1_neut <- round(df_m1_rt_fixed_effect$CI_low[df_m1_std_fixed_effect$Valence == 'Neutral' & df_m1_rt_fixed_effect$Match == 'M'], digits = 0)
ul_rt_m_m1_neut <- round(df_m1_rt_fixed_effect$CI_high[df_m1_rt_fixed_effect$Valence == 'Neutral' & df_m1_rt_fixed_effect$Match == 'M'], digits = 0)

med_rt_m_m1_bad <- round(df_m1_rt_fixed_effect$Median[df_m1_rt_fixed_effect$Valence == 'Bad' & df_m1_rt_fixed_effect$Match == 'M'], digits = 0)
ll_rt_m_m1_bad <- round(df_m1_rt_fixed_effect$CI_low[df_m1_std_fixed_effect$Valence == 'Bad' & df_m1_rt_fixed_effect$Match == 'M'], digits = 0)
ul_rt_m_m1_bad <- round(df_m1_rt_fixed_effect$CI_high[df_m1_rt_fixed_effect$Valence == 'Bad' & df_m1_rt_fixed_effect$Match == 'M'], digits = 0)

med_diff_rt_m_m1_GN <- round(df_rt1_diff_hdi$Median[df_rt1_diff_hdi$Parameter == 'diff_GN_M'], digits = 0)
ll_diff_rt_m_m1_GN <- round(df_rt1_diff_hdi$CI_low[df_rt1_diff_hdi$Parameter == 'diff_GN_M'], digits = 0)
ul_diff_rt_m_m1_GN <- round(df_rt1_diff_hdi$CI_high[df_rt1_diff_hdi$Parameter == 'diff_GN_M'], digits = 0)

med_diff_rt_m_m1_BN <- round(df_rt1_diff_hdi$Median[df_rt1_diff_hdi$Parameter == 'diff_BN_M'], digits = 0)
ll_diff_rt_m_m1_BN <- round(df_rt1_diff_hdi$CI_low[df_rt1_diff_hdi$Parameter == 'diff_BN_M'], digits = 0)
ul_diff_rt_m_m1_BN <- round(df_rt1_diff_hdi$CI_high[df_rt1_diff_hdi$Parameter == 'diff_BN_M'], digits = 0)

## nonmatch trials
med_rt_nm_m1_good <- round(df_m1_rt_fixed_effect$Median[df_m1_rt_fixed_effect$Valence == 'Good' & df_m1_rt_fixed_effect$Match == 'NM'], digits = 0)
ll_rt_nm_m1_good <- round(df_m1_rt_fixed_effect$CI_low[df_m1_std_fixed_effect$Valence == 'Good' & df_m1_rt_fixed_effect$Match == 'NM'], digits = 0)
ul_rt_nm_m1_good <- round(df_m1_rt_fixed_effect$CI_high[df_m1_rt_fixed_effect$Valence == 'Good' & df_m1_rt_fixed_effect$Match == 'NM'], digits = 0)

med_rt_nm_m1_neut <- round(df_m1_rt_fixed_effect$Median[df_m1_rt_fixed_effect$Valence == 'Neutral' & df_m1_rt_fixed_effect$Match == 'NM'], digits = 0)
ll_rt_nm_m1_neut <- round(df_m1_rt_fixed_effect$CI_low[df_m1_std_fixed_effect$Valence == 'Neutral' & df_m1_rt_fixed_effect$Match == 'NM'], digits = 0)
ul_rt_nm_m1_neut <- round(df_m1_rt_fixed_effect$CI_high[df_m1_rt_fixed_effect$Valence == 'Neutral' & df_m1_rt_fixed_effect$Match == 'NM'], digits = 0)

med_rt_nm_m1_bad <- round(df_m1_rt_fixed_effect$Median[df_m1_rt_fixed_effect$Valence == 'Bad' & df_m1_rt_fixed_effect$Match == 'NM'], digits = 0)
ll_rt_nm_m1_bad <- round(df_m1_rt_fixed_effect$CI_low[df_m1_std_fixed_effect$Valence == 'Bad' & df_m1_rt_fixed_effect$Match == 'NM'], digits = 0)
ul_rt_nm_m1_bad <- round(df_m1_rt_fixed_effect$CI_high[df_m1_rt_fixed_effect$Valence == 'Bad' & df_m1_rt_fixed_effect$Match == 'NM'], digits = 0)

med_diff_rt_nm_m1_GN <- round(df_rt1_diff_hdi$Median[df_rt1_diff_hdi$Parameter == 'diff_GN_NM'], digits = 0)
ll_diff_rt_nm_m1_GN <- round(df_rt1_diff_hdi$CI_low[df_rt1_diff_hdi$Parameter == 'diff_GN_NM'], digits = 0)
ul_diff_rt_nm_m1_GN <- round(df_rt1_diff_hdi$CI_high[df_rt1_diff_hdi$Parameter == 'diff_GN_NM'], digits = 0)

med_diff_rt_nm_m1_BN <- round(df_rt1_diff_hdi$Median[df_rt1_diff_hdi$Parameter == 'diff_BN_NM'], digits = 0)
ll_diff_rt_nm_m1_BN <- round(df_rt1_diff_hdi$CI_low[df_rt1_diff_hdi$Parameter == 'diff_BN_NM'], digits = 0)
ul_diff_rt_nm_m1_BN <- round(df_rt1_diff_hdi$CI_high[df_rt1_diff_hdi$Parameter == 'diff_BN_NM'], digits = 0)

bayestestR::sexit(df_m1_plot_rt_diff_wide[,c(6:7,9:10)])
```

  <!-- plot all graphs form the first part together -->
  
```{r plot-bayes-meta-1, fig.cap="Effect of moral character on perceptual matching", fig.height=15, fig.width=18, warning=FALSE}
library(patchwork)
p_rt1 + p_dprime1 +
        p_rt1_diff + p_dprime1_diff + plot_annotation(tag_levels = 'A')  + plot_layout(nrow = 2, byrow = TRUE, guides = 'collect')

# p <- p_rt1 + p_dprime1 + plot_annotation(tag_levels = 'A') + plot_layout(guides = "collect") 

#  ggsave('part1_plot_posterior.png', p, width = 15, height = 7.5)
```

The results from reaction times data also found a robust effect of moral character for both match trials (see figure \@ref(fig:plot-bayes-meta-1) C) and nonmatch trials (**see supplementary materials**). For match trials, shapes associated with good characters were faster (median = `r med_rt_m_m1_good` ms, 95% HDI = [`r ll_rt_m_m1_good` `r ul_rt_m_m1_good`]) than shapes associated with neutral characters (median = `r med_rt_m_m1_neut` ms, 95% HDI = [`r ll_rt_m_m1_neut` `r ul_rt_m_m1_neut`]), the effect ($median_{diff}$ =  -44, 95% HDI  [-67, -24]) has a 99.94% probability of being negative (< 0), 99.94% of being significant (< -0.05). We also found that RTs to shapes associated with bad characters (median = `r med_rt_m_m1_bad` ms, 95% HDI = [`r ll_rt_m_m1_bad` `r ul_rt_m_m1_bad`]) were slower as compared to the neutral character, the effect ($median_{diff}$ = 17, 95% HDI  [-6, 36]) has a 93.58% probability of being positive (> 0), 93.55% of being significant (> 0.05).

For the nonmatch trials, we found similar pattern but much smaller effect size. Shapes associated with good characters (median = `r med_rt_nm_m1_good` ms, 95% HDI = [`r ll_rt_nm_m1_good` `r ul_rt_nm_m1_good`]) were faster than shapes associated with neutral characters (median = `r med_rt_nm_m1_neut` ms, 95% HDI = [`r ll_rt_nm_m1_neut` `r ul_rt_nm_m1_neut`]), the difference ($median_{diff}$ = -18, 95% HDI  [-27, -8]) has a 99.91% probability of being negative (< 0), 99.91% of being significant (< -0.05). In contrast, the shapes associated with bad characters (median = `r med_rt_nm_m1_bad` ms, 95% HDI = [`r ll_rt_nm_m1_bad` `r ul_rt_nm_m1_bad`]) were slower than shapes associated with neutral characters, the effect ($median_{diff}$ = 5, 95% HDI  [-3, 13]) has a 92.43% probability of being positive (> 0), 92.31% of being significant (> 0.05).

```{r model comp for rt, eval = FALSE, echo=FALSE, results='hide' }
#### model comparison etc, will be in supplementary materials
#### Here we tried three models for RT data
#### M1: lognormal
#### M2: shift_lognormal
#### M3: truncated lognormal

#### Morde comparison was presented in the supplementary
# 
# log normal distribution, dummy coding
RT_val_m2 <- df_moral %>%
  dplyr::mutate(RT_sec = RT/1000) %>% # log RT in seconds
  dplyr::filter(ACC == 1) %>%
  brms::brm(RT_sec ~ Valence*ismatch + 
              (Valence*ismatch | ExpID_new) +   
              (Valence*ismatch | ExpID_new:Subject),
            family=shifted_lognormal(),
            data = .,
            iter = 3000,
            control = list(adapt_delta = .98),
            cores = parallel::detectCores(),
            backend = 'cmdstanr',  # with cmdstanr
            file = here::here("glmmModels/RT_val_EffectCode_3_level_m2"))
summary(RT_val_m2)

RT_val_m1 <- add_criterion(RT_val_m1, "loo")
RT_val_m2 <- add_criterion(RT_val_m2, "loo")

loo_compare(RT_val_m1, RT_val_m2, criterion = "loo")
bayes_factor(RT_val_m1,RT_val_m2)

# log normal distribution, with truncated distribution, dummy coding
RT_val_m3_trunc <- df_moral %>%
        dplyr::mutate(RT_sec = RT/1000) %>% # log RT in seconds
        dplyr::filter(ACC == 1) %>%
        brms::brm(RT_sec|trunc(lb = 0.2, ub = 1.1) ~ Valence*ismatch + 
                          (Valence*ismatch | ExpID_new) + 
                          (Valence*ismatch | ExpID_new:Subject),
                  family=lognormal(),
                  data = .,
                  iter = 3000,
                  control = list(adapt_delta = .98),
                  cores = parallel::detectCores(),
                  file = here::here("glmmModels/RT_val_EffectCode_3_level_m3_trunc"))
summary(RT_val_m3_trunc)
# pp_check(RT_val_m3_trunc)
# plot(RT_val_m3_trunc, "b_")

# compare three models
loo(RT_val_m1, RT_val_m2, RT_val_m3_trunc) # takes about XX mins
# bayes_factor(RT_val_m1,RT_val_m2)

# Model comparisons:
#           elpd_diff se_diff
# RT_val_m1  0.0       0.0   
# RT_val_m2 -7.9       3.1 
```

## Modulation effect self-referential processing

To test the modulation effect of self-referential processing, we also modeled data from three experiments (3a, 3b, and 6b) with three-level Bayesian models. These three experiments included 108 unique participants. We focused on the population-level effect of the interaction between self-referential processing and moral valence. Also, we examined the differences of differences, i.e., how the differences between good/bad characters and the neutral character under the self-referential conditions differ from that under other-referential conditions. The detailed results of each experiment can be found in supplementary materials.

```{r prepare data for second meta, echo=FALSE, results='hide', warning=FALSE}
### try meta-analysis 1a, 1b, 1c, 2, 5 and 6a
selected_columns <- c('ExpID', 'Site', 'Subject','Age', 'Sex', 'Matchness', 'Identity', 'Valence', 'RESP', 'ACC','RT')
df_ms <- dplyr::bind_rows(df3a.v_meta[selected_columns],
                          df3b.v_meta[selected_columns],
                          df6b.v_meta[selected_columns]) %>%
  dplyr::mutate(ExpID_new = paste(ExpID, Site, sep = "_")) %>%
  dplyr::mutate(Valence = factor(Valence, levels = c('Neutral', 'Bad', 'Good')))

df_ms_subj <- df_ms %>%
  dplyr::group_by(ExpID_new, Site) %>%
  dplyr::summarize(N = n_distinct(Subject),
                   N_trial = length(Subject),
                   Exp_conds = 6,
                   trial_per_cond = round((length(Subject)/6)/N, 0))

df_ms <- df_ms %>%
        dplyr::filter(!is.na(RESP)) %>% # filter trials without response
        dplyr::filter(RT >= 200) %>% # filter trials without response
        dplyr::mutate(ismatch = ifelse(Matchness == 'Match', 1, 0),
                      saymatch = ifelse((Matchness == 'Match' & ACC == 1) | 
                                                (Matchness == 'Mismatch' & ACC == 0), 1, 0),
                      Valence = factor(Valence, levels = c('Neutral', 'Bad', 'Good')),
                      Identity = factor(Identity, levels = c('Self', 'Other'))) %>%
        dplyr::select(ExpID_new, Subject, Matchness, Identity,Valence, RESP, ACC, RT, ismatch, saymatch) %>%
        dplyr::mutate(ismatch_num = ifelse(Matchness == 'Match', 0.5, -0.5))

# plot the nested structure of the data
# with(df_ms, table(Subject, ExpID_new)) %>%
#   image(
#     col = grey.colors(80, start = 1, end = 0),
#     axes = TRUE,
#     xlab = "Subject",
#     ylab = "ExpID"
#   )
```

```{r second meta sdt, echo=FALSE, results='hide', warning=FALSE}
# fit a three-level hierarchical model for SDT of moral self, didn't specify the prior; dummy coding
# 
# Note: initialization failed for a few times for full model. need to re-consider the model
sdt_ms_m1 <- df_ms %>%
        dplyr::mutate(Subject = as.factor(Subject),
                      ExpID_new = as.factor(ExpID_new)) %>%
        brms::brm(saymatch ~ 0 + Identity:Valence + ismatch:Identity:Valence + 
                          (0 + Identity:Valence + ismatch:Identity:Valence | ExpID_new) + 
                          (0 + Identity:Valence + ismatch:Identity:Valence | ExpID_new:Subject),
                  family = bernoulli(link="probit"),
                  data = .,
                  chains = 4,
                  iter = 6000,
                  thin = 2,
                  control = list(adapt_delta = .90),
                  cores = parallel::detectCores(),
                  backend = 'cmdstanr',  # with cmdstanr
                  file = here::here("glmmModels/sdt_ms_DummyCode_3_level"))

#### plot both overall parameters and experimental levels.
df_ms_sdt_m1_post_exp <- sdt_ms_m1 %>% 
  tidybayes::spread_draws(r_ExpID_new[condition, term]) %>%
  dplyr::rename(value = r_ExpID_new)

df_ms_sdt_m1_pop_mean <- sdt_ms_m1 %>%
  tidybayes::gather_draws(`b_.*`, regex = TRUE) %>%
  group_by(.variable) %>%       # this line not necessary (done automatically by spread_draws)
  tidybayes::mean_hdci(.value)  # get the high density continuous intervals

# extract the population level parameters
df_ms_sdt_m1_pop <- sdt_ms_m1 %>% 
  tidybayes::gather_draws(`b_.*`, regex = TRUE) %>%
  dplyr::rename(term = .variable,
                pop_mean = .value) %>%
  #tidyr::separate(term, c(NA, 'term'), "_") 
  dplyr::ungroup() %>%
  dplyr::mutate(term = gsub("b_", "", term))

# Add the population mean to the experimental level and only keep the value after.
df_ms_sdt_m1_post_exp_update <- merge(df_ms_sdt_m1_pop, df_ms_sdt_m1_post_exp, by = c('term','.chain','.iteration', '.draw'), all = T) %>%
  dplyr::mutate(mean_value = pop_mean + value) %>%  # add the population level value to each experiment
  dplyr::select(condition, term, `.chain`, `.iteration`, `.draw`, mean_value) %>% # select the added value
  dplyr::rename(value = mean_value)  # rename the value

# Plot the difference between conditions 
df_ms_sdt_m1_plot <- df_ms_sdt_m1_pop %>%
  dplyr::mutate(condition = 'Overall') %>%
  dplyr::rename(value = pop_mean) %>%
  dplyr::select(condition, term, `.chain`, `.iteration`, `.draw`, value) %>%
  dplyr::bind_rows(., df_ms_sdt_m1_post_exp_update) %>%
  dplyr::mutate(condition = factor(condition, levels = c("Exp3a_THU", "Exp3b_WZU", "Exp6b_THU",
                                                         "Overall")),
                condition = forcats::fct_rev(condition), # reverse the order because the plot function auto reverse.
                Valence = dplyr::case_when(grepl("Neutral", term) ~ "Neutral",
                                           grepl("Bad", term) ~"Bad",
                                           grepl("Good", term) ~"Good"),
                Identity = dplyr::case_when(grepl("Self", term) ~ "Self",
                                           grepl("Other", term) ~"Other"),
                params = dplyr::case_when(grepl("ismatch", term) ~ "dprime",
                                           !grepl("ismatch", term) ~"c"),
                params = factor(params, levels = c('dprime', 'c')),
                Identity = factor(Identity, levels = c("Self", "Other")),
                Valence = factor(Valence, levels = c('Good', 'Neutral',  'Bad'))
                ) 

# plot the posterior of the d prime
# use the overall mean values as the vlines
p_ms_sdt_vlines <- df_ms_sdt_m1_plot %>% 
        #tidyr::separate(term, c('params', 'Identity', 'Valence')) %>% 
        dplyr::mutate(Valence = factor(Valence, levels = c('Good', 'Neutral', 'Bad'))) %>%
        dplyr::filter(params == 'dprime') %>%
        dplyr::filter(condition == "Overall") %>%
        dplyr::group_by(Identity, Valence) %>% 
        dplyr::summarize(Mean = mean(value)) # %>%
        # dplyr::arrange(Mean)

# THIS is the one which the final plot will based on!!!
p_ms_dprime1 <- df_ms_sdt_m1_plot %>%
        # tidyr::separate(term, c('params', 'Valence')) %>%
        dplyr::mutate(Valence = factor(Valence, levels = c('Good', 'Neutral', 'Bad')),
                      Identity = factor(Identity, levels = c("Self", "Other"))) %>%
        dplyr::filter(params == 'dprime') %>%
        dplyr::rename(Experiments = condition) %>%
        dplyr::filter((value >= 0) & (value <= 4)) %>%  # limit the x-axis's value
        ggplot2::ggplot(aes(y = Experiments, x = value, color = Valence)) +
        tidybayes::stat_halfeye(aes(fill = Valence), alpha = 0.7) +
        facet_wrap(~Identity) + 
        geom_vline(data = p_ms_sdt_vlines, aes(xintercept = Mean,colour = Valence), linetype = "dashed") +
        labs(x=expression('Posteior distribution of '~italic(d)~' prime')) + 
        theme_apa()

# plot the posterior of the difference between d prime
df_ms_sdt_m1_plot_diff <- df_ms_sdt_m1_plot %>%
        tidyr::unite(term, c('params', 'Identity', 'Valence')) %>%
        tidyr::pivot_wider(names_from = c(term), values_from = value) %>%   # long to wide
        dplyr::mutate(
                diff_GN_dprm_S = dprime_Self_Good - dprime_Self_Neutral,
                diff_BN_dprm_S = dprime_Self_Bad - dprime_Self_Neutral,
                diff_GN_dprm_O = dprime_Other_Good - dprime_Other_Neutral,
                diff_BN_dprm_O = dprime_Other_Bad - dprime_Other_Neutral,
                diff_SO_dprm_G = dprime_Self_Good - dprime_Other_Good,
                diff_SO_dprm_N =  dprime_Self_Neutral - dprime_Other_Neutral,
                diff_SO_dprm_B =  dprime_Self_Bad - dprime_Other_Bad,
                diff_diff_GN = diff_GN_dprm_S - diff_GN_dprm_O,
                diff_diff_BN = diff_BN_dprm_S - diff_BN_dprm_O
                ) %>%
        dplyr::select(condition, `.chain`, `.iteration`, `.draw`,
                      diff_GN_dprm_S, diff_BN_dprm_S, diff_GN_dprm_O, diff_BN_dprm_O,
                      diff_SO_dprm_G, diff_SO_dprm_N, diff_SO_dprm_B, diff_diff_GN, diff_diff_BN) 

# define face titles
contrast_names_ms_sdt <- c(`diff_GN_dprm_S` = "Self: Good vs Neutral", 
                           `diff_BN_dprm_S` = "Self: Bad vs Neutral", 
                           `diff_GN_dprm_O` = "Other: Good vs Neutral", 
                           `diff_BN_dprm_O` = "Other: Bad vs Neutral"
                           )

p_ms_dprime1_diff_val <- df_ms_sdt_m1_plot_diff %>%
        tidyr::pivot_longer(cols = diff_GN_dprm_S:diff_diff_BN, names_to = "term_diff", values_to =  "value") %>%  # wide to long
        dplyr::filter(str_detect(term_diff, '_dprm_S|_dprm_O')) %>%
        dplyr::mutate(term_diff = factor(term_diff, levels = c('diff_GN_dprm_S', 'diff_BN_dprm_S', 
                                                               'diff_GN_dprm_O', 'diff_BN_dprm_O'))) %>%
        dplyr::filter((value >= -1.5) & (value <= 1.5)) %>%
        dplyr::rename(Experiments = condition) %>%
        ggplot2::ggplot(aes(y = Experiments, x = value, fill = after_stat(x > 0))) +
        tidybayes::stat_halfeye() +
        geom_vline(xintercept = 0, linetype = "dashed") +
        scale_fill_manual(values = c('skyblue', 'gray80'),
                    breaks = c(TRUE, FALSE),
                    name = "Effect" , labels = c("Yes", "No")) + 
        # scale_fill_manual(values = c('gray80', 'skyblue')) +
        xlab(expression(paste("Valence effect on ", italic("d"), " prime", sep = ' '))) + 
        facet_wrap( ~ term_diff, # scales = "free_y",
               nrow = 1,
               labeller = as_labeller(contrast_names_ms_sdt)) + # label_parsed
        theme_apa() + 
        theme(strip.text.x = element_text(size = 8)) # colour = "orange", angle = 90))

# another way to present the results, not presented in the manuscript
p_ms_dprime1_diff_id <- df_ms_sdt_m1_plot_diff %>%
        tidyr::pivot_longer(cols = diff_GN_dprm_S:diff_diff_BN, names_to = "term_diff", values_to =  "value") %>%  # wide to long
        dplyr::filter(str_detect(term_diff, '_SO_')) %>%
        dplyr::mutate(term_diff = factor(term_diff, levels = c('diff_SO_dprm_G', 'diff_SO_dprm_N', 'diff_SO_dprm_B'))) %>%
        #dplyr::filter(!str_detect(term_diff, '_GB_')) %>%
        # tidyr::unite(term_diff, c('Identity', 'term_diff')) %>%
        dplyr::filter((value >= -1.5) & (value <= 1.5)) %>%
        dplyr::rename(Experiments = condition) %>%
        ggplot2::ggplot(aes(y = Experiments, x = value, fill = after_stat(x > 0))) +
        tidybayes::stat_halfeye() +
        geom_vline(xintercept = 0, linetype = "dashed") +
        scale_fill_manual(values = c('skyblue', 'gray80'),
                    breaks = c(TRUE, FALSE),
                    name = "Effect" , labels = c("Yes", "No")) + 
        # scale_fill_manual(values = c('gray80', 'skyblue')) +
        xlab(expression(paste("Self-referential effect on ", italic("d"), " prime", sep = ' '))) + 
        facet_wrap( ~ term_diff, # scales = "free_y",
               nrow = 1,
               labeller = label_parsed)

test_res_sdt_ms <- bayestestR::sexit(df_ms_sdt_m1_plot_diff[5:13])
test_res_sdt_ms
```

For the *d* prime, we found an interaction between the moral valence and self-referential processing: the good-neutral differences are larger for the self-referential condition than for the other-referential condition: The difference ($median_{diff}$ = 0.48, 95% HDI [-0.62, 1.65]) has a 93.04% probability of being positive (> 0), 91.92% of being significant (> 0.05). However, the bad-neutral differences ($median_{diff}$ = 0.0087, 95% HDI [-0.96, 1.00]) only has a 51.85% probability of being positive (> 0), 41.29% of being significant (> 0.05). Further analyses revealed that the prioritization effect of good character (as compared to neutral) only appeared for self-referential conditions but not other-referential conditions. The estimated *d* prime for good-self was greater than neutral-self ($median_{diff}$ = 0.54, 95% HDI [-0.30, 1.41]), with a 95.99% probability of being positive (> 0), 95.36% of being significant (> 0.05). The differences between bad-self and neutral-self, good-other and neutral-other, and bad-other and neutral-other are all centered around zero (see Figure \@ref(fig:plot-bayes2), B, D). 

```{r second meta rt, echo=FALSE, results='hide', warning=FALSE}
# fit a three-level hierarchical model for RT, didn't specify the prior, lognormal, dummy coding, both match and nonmatch trials
# may not converge event after 10000 samples
# Only matched trials
RT_ms_m1_match <- df_ms %>%
        dplyr::mutate(RT_sec = RT/1000) %>% # log RT in seconds
        dplyr::filter(ACC == 1) %>%         # only correct trials
        dplyr::filter(Matchness == "Match") %>%
        brms::brm(RT_sec ~ Identity*Valence + 
              (Identity*Valence | ExpID_new) +   
              (Identity*Valence | ExpID_new:Subject),
            family=lognormal(),
            data = .,
            chains = 4,
            control = list(adapt_delta = .90),
            iter = 10000,
            thin = 1,
            cores = parallel::detectCores(),
            backend = 'cmdstanr',  # with cmdstanr
            file = here::here("glmmModels/RT_ms_DummyCode_3_level_match"))
# summary(RT_ms_m1_match) # R-hat still has problem

# Get the variables in the model 1
# RT_var_name_m1 <- tidybayes::get_variables(RT_val_m1)
df_ms_m1_post_rt_exp <- RT_ms_m1_match %>% 
  tidybayes::spread_draws(r_ExpID_new[condition, term]) %>%
  dplyr::rename(value = r_ExpID_new)

df_ms_m1_rt_pop_mean <- RT_ms_m1_match %>%
  tidybayes::gather_draws(`b_.*`, regex = TRUE) %>%
  group_by(.variable) %>%       # this line not necessary (done automatically by spread_draws)
  tidybayes::mean_hdci(.value)  # get the high density continuous intervals

# extract the population level parameters
df_ms_m1_rt_pop <- RT_ms_m1_match %>% 
  tidybayes::gather_draws(`b_.*`, regex = TRUE) %>%
  dplyr::rename(term = .variable,
                pop_mean = .value) %>%
  dplyr::ungroup() %>%
  dplyr::mutate(term = gsub("b_", "", term))

# Add the population mean to the experimental level and only keep the value after.
df_ms_m1_rt_exp_update <- merge(df_ms_m1_rt_pop, df_ms_m1_post_rt_exp, 
                     by = c('term','.chain','.iteration', '.draw'), all = T) %>%
  dplyr::mutate(mean_value = pop_mean + value) %>%  # add the population level value to each experiment
  dplyr::select(condition, term, `.chain`, `.iteration`, `.draw`, mean_value) %>% # select the added value
  dplyr::rename(value = mean_value)  # rename the value

# Plot the difference between conditions 
df_ms_m1_plot_rt <- df_ms_m1_rt_pop %>%
        dplyr::mutate(condition = 'Overall') %>%
        dplyr::rename(value = pop_mean) %>%              # change the `pop_mean` as `value` for data frame merge
        dplyr::select(condition, term, `.chain`, `.iteration`, `.draw`, value) %>%
        dplyr::bind_rows(., df_ms_m1_rt_exp_update) %>%
        dplyr::mutate(condition = factor(condition, levels = c("Exp3a_THU", "Exp3b_WZU", "Exp6b_THU",
                                                               "Overall")),
                      condition = forcats::fct_rev(condition)) %>%
        tidyr::pivot_wider(names_from = c(term), values_from = value) %>%   # long to wide
        dplyr::mutate(M_Self_Neutral = Intercept,
                      M_Self_Bad = Intercept + ValenceBad,
                      M_Self_Good = Intercept  + ValenceGood,
                      M_Other_Neutral = Intercept  + IdentityOther,
                      M_Other_Bad = Intercept  + `IdentityOther:ValenceBad`,
                      M_Other_Good = Intercept + `IdentityOther:ValenceGood`) %>%
        dplyr::select(condition, `.chain`, `.iteration`, `.draw`,
               contains('M_')) %>%
  tidyr::pivot_longer(cols = M_Self_Neutral:M_Other_Good, names_to = "term", values_to =  "value") %>%  # wide to long
  dplyr::mutate(term = factor(term, levels = c('M_Self_Neutral', 'M_Self_Bad', 'M_Self_Good',
                                               'M_Other_Neutral', 'M_Other_Bad', 'M_Other_Good')),
                value = exp(value),
                value = value * 1000) 

# plot the posterior of the d prime
# use the overall mean values as the vlines
p_ms_rt1_vlines <- df_ms_m1_plot_rt %>% 
        tidyr::separate(term, c('Match', 'Identity', 'Valence')) %>% 
        dplyr::mutate(Valence = factor(Valence, levels = c('Good', 'Neutral', 'Bad')),
                      Identity = factor(Identity, levels = c("Self", "Other"))) %>%
        dplyr::filter(Match == 'M') %>%
        dplyr::filter(condition == "Overall") %>%
        dplyr::filter((value >=200) & (value <=1200)) %>%
        dplyr::group_by(Identity, Valence) %>% 
        tidybayes::median_hdci(value)

# THIS is the one which the final plot will based on!!!
p_ms_rt1 <- df_ms_m1_plot_rt %>%
        tidyr::separate(term, c('Match', 'Identity', 'Valence')) %>% 
        dplyr::mutate(Valence = factor(Valence, levels = c('Good', 'Neutral', 'Bad')),
                      Identity = factor(Identity, levels = c("Self", "Other"))) %>%
        dplyr::filter(Match == 'M') %>%
        dplyr::rename(Experiments = condition) %>%
        dplyr::filter((value >=200) & (value <=1200)) %>%
        ggplot2::ggplot(aes(y = Experiments, x = value, color = Valence)) +
        tidybayes::stat_halfeye(aes(fill = Valence), alpha = 0.7) +
        geom_vline(data = p_ms_rt1_vlines, aes(xintercept = value, colour = Valence), linetype = "dashed") +
        labs(x=expression('Posteior distribution of RTs')) + 
        facet_wrap(~Identity) + 
        theme_apa()

df_ms_m1_plot_rt_diff <- df_ms_m1_plot_rt %>%
        tidyr::pivot_wider(names_from = c(term), values_from = value) %>%       # long to wide
        # calculate the difference between conditions, matched trials only
        dplyr::mutate(diff_GN_M_S = M_Self_Good - M_Self_Neutral,               # calculate the differences between conditions
                      diff_BN_M_S = M_Self_Bad - M_Self_Neutral,
                      diff_GN_M_O = M_Other_Good - M_Other_Neutral,  
                      diff_BN_M_O = M_Other_Bad - M_Other_Neutral,
                      diff_SO_G   = M_Self_Good - M_Other_Good, 
                      diff_SO_N   = M_Self_Neutral - M_Other_Neutral,
                      diff_SO_B   = M_Self_Bad - M_Other_Bad,
                      diff_diff_GN = diff_GN_M_S - diff_GN_M_O,
                      diff_diff_BN = diff_BN_M_S - diff_BN_M_O) %>%
        dplyr::select(condition, `.chain`, `.iteration`, `.draw`,
                      diff_GN_M_S, diff_BN_M_S, diff_GN_M_O,
                      diff_BN_M_O, diff_SO_G, diff_SO_N, diff_SO_B,
                      diff_diff_GN, diff_diff_BN) #  %>%

df_ms_rt1_diff_diff <- df_ms_m1_plot_rt_diff %>%
        tidyr::pivot_longer(cols = diff_GN_M_S:diff_diff_BN, names_to = "term_diff", values_to =  "value") %>%  # wide to long
        dplyr::filter(str_detect(term_diff, 'diff_diff')) %>%
        dplyr::mutate(term_diff = factor(term_diff, levels = c('diff_diff_GN', 'diff_diff_BN'))) %>% 
        dplyr::filter(condition == "Overall") %>%
        tidyr::pivot_wider(.,
                           id_cols = c('.chain', '.iteration', '.draw'),
                           names_from = term_diff) %>%
        dplyr::select(-c('.chain', '.iteration', '.draw')) %>%
        bayestestR::describe_posterior(.,
                                       ci = 0.95,
                                       ci_method = 'hdi',
                                       test = c("p_direction", "p_significance"),
                                       centrality = "median")

# define face titles
contrast_names_ms_rt <- c(`diff_GN_M_S` = "Self: Good vs Neutral", 
                           `diff_BN_M_S` = "Self: Bad vs Neutral", 
                           `diff_GN_M_O` = "Other: Good vs Neutral", 
                           `diff_BN_M_O` = "Other: Bad vs Neutral"
                           )
# plot the posterior of matching trials of valence
p_ms_rt1_diff_val <- df_ms_m1_plot_rt_diff %>%
        tidyr::pivot_longer(cols = diff_GN_M_S:diff_SO_B, names_to = "term_diff", values_to =  "value")  %>%  # wide to long
        dplyr::filter(str_detect(term_diff, '_M_')) %>%
        dplyr::mutate(term_diff = factor(term_diff, levels = c('diff_GN_M_S','diff_BN_M_S', 
                                                               'diff_GN_M_O', 'diff_BN_M_O'))) %>%
        dplyr::filter((value >= -200) & (value <= 300)) %>%
        dplyr::rename(Experiments = condition) %>%
        ggplot2::ggplot(aes(y = Experiments, x = value, fill = after_stat(x < 0))) +
        tidybayes::stat_halfeye() +
        geom_vline(xintercept = 0, linetype = "dashed") +
        scale_fill_manual(values = c('skyblue', 'gray80'),
                    breaks = c(TRUE, FALSE),
                    name = "Effect" , labels = c("Yes", "No")) + 
        # scale_fill_manual(values = c('gray80', 'skyblue')) + 
        xlab("Valence effect on RTs (Match trials)") +
        scale_x_continuous(breaks=seq(-200, 300, 200)) +  
        facet_wrap( ~ term_diff,
              # scales = "free_y", 
              nrow = 1,
              labeller = as_labeller(contrast_names_ms_rt)) + # label_parsed
        theme_apa() + 
        theme(strip.text.x = element_text(size = 8)) 

df_ms_rt1_diff_val <- df_ms_m1_plot_rt_diff %>%
        tidyr::pivot_longer(cols = diff_GN_M_S:diff_SO_B, names_to = "term_diff", values_to =  "value")  %>%  # wide to long
        dplyr::filter(str_detect(term_diff, '_M_')) %>%
        dplyr::filter(condition == "Overall") %>%
        tidyr::pivot_wider(.,
                           id_cols = c('.chain', '.iteration', '.draw'),
                           names_from = term_diff) %>%
        dplyr::select(-c('.chain', '.iteration', '.draw')) %>%
        bayestestR::describe_posterior(.,
                                       ci = 0.95,
                                       ci_method = 'hdi',
                                       test = c("p_direction", "p_significance"),
                                       centrality = "median")

# plot the posterior of matching trials, diff between self and other
p_ms_rt1_diff_id <- df_ms_m1_plot_rt_diff %>%
        tidyr::pivot_longer(cols = diff_GN_M_S:diff_SO_B, names_to = "term_diff", values_to =  "value")  %>%  # wide to long
        dplyr::filter(str_detect(term_diff, '_SO_')) %>%
        dplyr::mutate(term_diff = factor(term_diff, levels = c('diff_SO_G', 'diff_SO_N', 'diff_SO_B'))) %>%
        dplyr::filter((value >= -250) & (value <= 250)) %>%
        dplyr::rename(Experiments = condition) %>%
        ggplot2::ggplot(aes(y = Experiments, x = value, fill = after_stat(x < 0))) +
        tidybayes::stat_halfeye() +
        geom_vline(xintercept = 0, linetype = "dashed") +
        scale_fill_manual(values = c('skyblue', 'gray80'),
                    breaks = c(TRUE, FALSE),
                    name = "Effect" , labels = c("Yes", "No")) + 
        # scale_fill_manual(values = c('gray80', 'skyblue')) + 
        xlab("Self-referential effect on RTs (Match trials)") +
        facet_wrap( ~ term_diff,
              # scales = "free_y", 
              nrow = 1,
              labeller = label_parsed)

df_ms_rt1_diff_id <- df_ms_m1_plot_rt_diff %>%
        tidyr::pivot_longer(cols = diff_GN_M_S:diff_SO_B, names_to = "term_diff", values_to =  "value")  %>%  # wide to long
        dplyr::filter(str_detect(term_diff, '_SO_')) %>%
        dplyr::mutate(term_diff = factor(term_diff, levels = c('diff_SO_G', 'diff_SO_N', 'diff_SO_B'))) %>%
        dplyr::filter(condition == "Overall") %>%
        tidyr::pivot_wider(.,
                           id_cols = c('.chain', '.iteration', '.draw'),
                           names_from = term_diff) %>%
        dplyr::select(-c('.chain', '.iteration', '.draw')) %>%
        bayestestR::describe_posterior(.,
                                       ci = 0.95,
                                       ci_method = 'hdi',
                                       test = c("p_direction", "p_significance"),
                                       centrality = "median")

test_res_rt_ms <- bayestestR::sexit(df_ms_m1_plot_rt_diff[,5:13])
test_res_rt_ms
```

```{r plot-bayes2, fig.cap="Interaction between moral character and self-referential", fig.height=12, fig.width=18, warning=FALSE}
library(patchwork)
# (p_rt1 | p_dprime1)
p_ms_rt1 + p_ms_dprime1 +
        p_ms_rt1_diff_val + p_ms_dprime1_diff_val + plot_annotation(tag_levels = 'A')  + plot_layout(nrow = 2, byrow = TRUE, guides = "collect") 
        # p_ms_rt1_diff_id + p_ms_dprime1_diff_id + 
```

For the RTs of matched trials, we also found an interaction between moral valence and self-referential processing: the good-neutral differences were larger for the self- than the other-referential conditions ($median_{diff}$ = -148, 95% HDI [-413, 73]) has a 96.05% probability of being negative (< 0), 96.05% of being significant (< -0.05). However, this pattern was much weaker for bad-neutral differences ($median_{diff}$ = -47, 95% HDI [-280, 182]) has a 79.91% probability of being negative (< 0), 79.88% of being significant (< -0.05). Bayes analyses revealed a robust good-self prioritization effect as compared to neutral-self ($median_{diff}$ = -59, 95% HDI [-115, -22]) has a 98.87% probability of being negative (< 0), 98.87% of being significant (< -0.05)) and good-other ($median_{diff}$ = -109, 95% HDI [-227, -31]) has a 98.65% probability of being negative (< 0), 98.65% of being significant (< -0.05)) conditions. Similar to the results of *d'*, we found that participants responded slower for both good character than for the neutral character when they referred to others, $median_{diff}$ = 85.01, 95% HDI [-112, 328]) has a 92.16% probability of being positive (> 0), 92.15% of being significant (> 0.05). A similar pattern was also found for bad character when refereed to others: bad-other was responded slower than neutral-other,  $median_{diff}$ = 44, 95% HDI [-146, 268]) has a 80.03% probability of being positive (> 0), 79.99% of being significant (> 0.05). See Figure \@ref(fig:plot-bayes2).

These results suggested that the prioritization of good character is not solely driven by the valence of moral character. Instead, the self-referential processing modulated the prioritization of good character: good character was prioritized only when it was self-referential. When the moral character was other-referential, responses to both good and bad characters were slowed down. 

## Spontaneous binding between the good character and the self

Experiments 4a and 4b were designed to test whether the good character and self-referential processing bind together spontaneously. Because these two experiments have different experimental designs, we model their data separately.   

In experiment 4a, where “self” vs. “other” were task-relevant and moral character were task-irrelevant, we found the “self” conditions performed better than the “other” conditions for both *d* prime and reaction times. This pattern is consistent with previous studies (e.g., @Sui_2012_JEPHPP).

```{r 4a_BGLMM_sdt, echo=FALSE, results='hide', warning=FALSE, message=FALSE}
exp_name <- '4a'
exp4a_sdt_m1 <- fun_sdt_val_id(exp_name)

#summary(exp4a_sdt_m1)    # check summary

# check fixed and varying effect using bayestestR
# bayestestR::describe_posterior(
#   exp4a_sdt_m1,
#   effects = "all",
#   component = "all",
#   test = c("p_direction", "p_significance"),
#   centrality = "all"
# )
#pp_check(exp4a_sdt_m1)   # posterior predictive check
# extract the population level parameters
# criteria

df_exp4a_sdt_m1_plot <- exp4a_sdt_m1 %>%
  tidybayes::gather_draws(`b_.*`, regex = TRUE) %>% # get the traces of population level parameters
  # create two columns for two independent factors.
  dplyr::mutate(Valence = dplyr::case_when(grepl("Neutral", .variable) ~ "Neutral",
                                           grepl("Bad", .variable) ~"Bad",
                                           grepl("Good", .variable) ~"Good"),
                Identity = dplyr::case_when(grepl("Self", .variable) ~ "Self",
                                           grepl("Other", .variable) ~"Other"),
                params = dplyr::case_when(grepl("ismatch", .variable) ~ "dprime",
                                           !grepl("ismatch", .variable) ~"criterion"),
                params = factor(params, levels = c('dprime', 'criterion')),
                Identity = factor(Identity, levels = c("Self", "Other")),
                Valence = factor(Valence, levels = c('Good', 'Neutral',  'Bad'))) 

exp4a_sdt_p <- df_exp4a_sdt_m1_plot %>%
  dplyr::filter(params == 'dprime') %>%  # select only d prime
  ggplot2::ggplot(aes(x = Identity, y = .value, color = Valence)) +
  tidybayes::stat_halfeye(aes(fill = Valence), alpha = 0.7) + # position=position_dodge(width = 0.1)
  # geom_slabinterval(ymin = 0, ymax = 4) +
  stat_summary(aes(group = Valence, color = Valence), fun = mean, geom = "line") +
  labs(x = expression("Self-Referential"), 
      y = expression(paste("Posteior of sensitivity ", italic("d'"), sep = ' '))) +
  scale_colour_brewer(palette = "Dark2") +
  scale_fill_brewer(palette = "Dark2") +
  # facet_grid(~ params , scales = "free_y") +
  theme_apa() 

df_exp4a_sdt_hdi <- df_exp4a_sdt_m1_plot %>%
        dplyr::ungroup() %>%
        dplyr::filter(params == 'dprime') %>%
        dplyr::select(-c('.variable', 'params')) %>%
        # dplyr::mutate(term_diff = stringr::str_sub(.$term_diff, end = -8)) %>%
        tidyr::pivot_wider(.,
                           id_cols = c('.chain', '.iteration', '.draw'),
                           values_from = '.value',
                           names_from = c('Valence', 'Identity')) %>%
        dplyr::select(-c('.chain', '.iteration', '.draw')) %>%
        bayestestR::describe_posterior(.,
                                       ci = 0.95,
                                       ci_method = 'hdi',
                                       test = c("p_direction", "p_significance"),
                                       BF = 1,
                                       centrality = "median")

### prepare results to report
# neutral self
rep_m_d_exp4a_neut_s <- df_exp4a_sdt_hdi %>% dplyr::filter(Parameter == "Neutral_Self") %>% dplyr::select(Median) %>% 
        dplyr::pull() %>%
        round(., digits = 3)
rep_ll_d_exp4a_neut_s <- df_exp4a_sdt_hdi %>% dplyr::filter(Parameter == "Neutral_Self") %>% dplyr::select(CI_low) %>% 
        dplyr::pull() %>%
        round(., digits = 3)
rep_ul_d_exp4a_neut_s <- df_exp4a_sdt_hdi %>% dplyr::filter(Parameter == "Neutral_Self") %>% dplyr::select(CI_high) %>% 
        dplyr::pull() %>%
        round(., digits = 3)

# good self
rep_m_d_exp4a_good_s <- df_exp4a_sdt_hdi %>% dplyr::filter(Parameter == "Good_Self") %>% dplyr::select(Median) %>% 
        dplyr::pull() %>%
        round(., digits = 3)
rep_ll_d_exp4a_good_s <- df_exp4a_sdt_hdi %>% dplyr::filter(Parameter == "Good_Self") %>% dplyr::select(CI_low) %>% 
        dplyr::pull() %>%
        round(., digits = 3)
rep_ul_d_exp4a_good_s <- df_exp4a_sdt_hdi %>% dplyr::filter(Parameter == "Good_Self") %>% dplyr::select(CI_high) %>% 
        dplyr::pull() %>%
        round(., digits = 3)

# bad self
rep_m_d_exp4a_bad_s <- df_exp4a_sdt_hdi %>% dplyr::filter(Parameter == "Bad_Self") %>% dplyr::select(Median) %>% 
        dplyr::pull() %>%
        round(., digits = 3)
rep_ll_d_exp4a_bad_s <- df_exp4a_sdt_hdi %>% dplyr::filter(Parameter == "Bad_Self") %>% dplyr::select(CI_low) %>% 
        dplyr::pull() %>%
        round(., digits = 3)
rep_ul_d_exp4a_bad_s <- df_exp4a_sdt_hdi %>% dplyr::filter(Parameter == "Bad_Self") %>% dplyr::select(CI_high) %>% 
        dplyr::pull() %>%
        round(., digits = 3)

# Neutral other
rep_m_d_exp4a_neut_o <- df_exp4a_sdt_hdi %>% dplyr::filter(Parameter == "Neutral_Other") %>% dplyr::select(Median) %>% 
        dplyr::pull() %>%
        round(., digits = 3)
rep_ll_d_exp4a_neut_o <- df_exp4a_sdt_hdi %>% dplyr::filter(Parameter == "Neutral_Other") %>% dplyr::select(CI_low) %>% 
        dplyr::pull() %>%
        round(., digits = 3)
rep_ul_d_exp4a_neut_o <- df_exp4a_sdt_hdi %>% dplyr::filter(Parameter == "Neutral_Other") %>% dplyr::select(CI_high) %>% 
        dplyr::pull() %>%
        round(., digits = 3)

# good other
rep_m_d_exp4a_good_o <- df_exp4a_sdt_hdi %>% dplyr::filter(Parameter == "Good_Other") %>% dplyr::select(Median) %>% 
        dplyr::pull() %>%
        round(., digits = 3)
rep_ll_d_exp4a_good_o <- df_exp4a_sdt_hdi %>% dplyr::filter(Parameter == "Good_Other") %>% dplyr::select(CI_low) %>% 
        dplyr::pull() %>%
        round(., digits = 3)
rep_ul_d_exp4a_good_o <- df_exp4a_sdt_hdi %>% dplyr::filter(Parameter == "Good_Other") %>% dplyr::select(CI_high) %>% 
        dplyr::pull() %>%
        round(., digits = 3)

# bad other
rep_m_d_exp4a_bad_o <- df_exp4a_sdt_hdi %>% dplyr::filter(Parameter == "Bad_Other") %>% dplyr::select(Median) %>% 
        dplyr::pull() %>%
        round(., digits = 3)
rep_ll_d_exp4a_bad_o <- df_exp4a_sdt_hdi %>% dplyr::filter(Parameter == "Bad_Other") %>% dplyr::select(CI_low) %>% 
        dplyr::pull() %>%
        round(., digits = 3)
rep_ul_d_exp4a_bad_o <- df_exp4a_sdt_hdi %>% dplyr::filter(Parameter == "Bad_Other") %>% dplyr::select(CI_high) %>% 
        dplyr::pull() %>%
        round(., digits = 3)

df_exp4a_sdt_m1_plot_diff_wide <- df_exp4a_sdt_m1_plot %>%
        tidyr::unite(term, c('params', 'Identity', 'Valence')) %>%
        dplyr::ungroup() %>%
        dplyr::select(term, `.chain`, `.iteration`, `.draw`, `.value`) %>%
        tidyr::pivot_wider(names_from = c(term), values_from = .value) %>%   # long to wide
        dplyr::mutate(
                diff_GN_dprm_S = dprime_Self_Good - dprime_Self_Neutral,
                diff_BN_dprm_S = dprime_Self_Bad - dprime_Self_Neutral,
                diff_GB_dprm_S = dprime_Self_Good - dprime_Self_Bad,
                diff_GN_dprm_O = dprime_Other_Good - dprime_Other_Neutral,
                diff_BN_dprm_O = dprime_Other_Bad - dprime_Other_Neutral,
                diff_GB_dprm_O = dprime_Other_Good - dprime_Other_Bad,
                diff_SO_dprm_G = dprime_Self_Good - dprime_Other_Good,
                diff_SO_dprm_N =  dprime_Self_Neutral - dprime_Other_Neutral,
                diff_SO_dprm_B =  dprime_Self_Bad - dprime_Other_Bad,
                diff_diff_GN_SO = diff_GN_dprm_S - diff_GN_dprm_O,
                diff_diff_GB_SO = diff_GB_dprm_S - diff_GB_dprm_O,
                diff_diff_BN_SO = diff_BN_dprm_S - diff_BN_dprm_O
                ) %>%
  dplyr::select(`.chain`, `.iteration`, `.draw`,
               diff_GN_dprm_S:diff_diff_BN_SO) 
## plot:
df_exp4a_sdt_m1_plot_diff <- df_exp4a_sdt_m1_plot_diff_wide %>%
  tidyr::pivot_longer(cols = diff_GN_dprm_S:diff_diff_BN_SO, names_to = "term_diff", values_to =  "value") %>%  # wide to long
  dplyr::mutate(term_diff = factor(term_diff, levels = c('diff_GB_dprm_S', 'diff_GN_dprm_S', 'diff_BN_dprm_S', 
                                                         'diff_GB_dprm_O', 'diff_GN_dprm_O', 'diff_BN_dprm_O', 
                                                         'diff_SO_dprm_G', 'diff_SO_dprm_N', 'diff_SO_dprm_B',
                                                         'diff_diff_GN_SO', 'diff_diff_GB_SO', 'diff_diff_BN_SO')))

p_exp4a_dprime1_diff_val <- df_exp4a_sdt_m1_plot_diff %>%
        dplyr::filter(str_detect(term_diff, '_dprm_S|_dprm_O')) %>%
        dplyr::mutate(Identity = dplyr::case_when(grepl("_S", term_diff) ~ "Self",
                                           grepl("_O", term_diff) ~"Other"),
                      Identity = factor(Identity, levels = c("Self", "Other"))) %>%
        dplyr::filter(!str_detect(term_diff, '_GB_|_BN')) %>%
        dplyr::mutate(term_diff = stringr::str_sub(.$term_diff, end = -8)) %>%
        dplyr::mutate(term_diff = dplyr::case_when(#grepl("_GB", term_diff) ~ "Good vs. Bad",
                                                   grepl("_GN", term_diff) ~"Good vs. Neutral",
                                                   #grepl("_BN", term_diff) ~ "Bad vs. Neutral",
                                                   ),
                      ) %>%
        dplyr::filter((value >= -1.5) & (value <= 1.5)) %>%
        dplyr::rename(conditions = term_diff) %>%
        ggplot2::ggplot(aes(x = value, fill = after_stat(x > 0))) + # y = fct_rev(conditions)
        tidybayes::stat_halfeye() +
        geom_vline(xintercept = 0, linetype = "dashed") +
        scale_fill_manual(values = c('skyblue', 'gray80'),
                    breaks = c(TRUE, FALSE),
                    name = "Effect" , labels = c("Yes", "No")) + 
        # scale_fill_manual(values = c('gray80', 'skyblue')) +
        labs(x = expression(paste("Valence effect on ", italic("d"), " prime", sep = ' ')),
             y = expression("Good vs. Neutral")) + 
        scale_x_continuous(breaks=seq(-0.25, 0.75, 0.5)) +
        facet_wrap( ~ Identity, nrow = 1) +
        theme_apa()

df_exp4a_dprime1_diff_val <- df_exp4a_sdt_m1_plot_diff %>%
        dplyr::filter(str_detect(term_diff, '_dprm_S|_dprm_O')) %>%
        dplyr::mutate(Identity = dplyr::case_when(grepl("_S", term_diff) ~ "Self",
                                           grepl("_O", term_diff) ~"Other"),
                      Identity = factor(Identity, levels = c("Self", "Other"))) %>%
        dplyr::filter(!str_detect(term_diff, '_GB_')) %>%
        dplyr::mutate(term_diff = stringr::str_sub(.$term_diff, end = -8)) %>%
        tidyr::pivot_wider(.,
                           id_cols = c('.chain', '.iteration', '.draw'),
                           names_from = c('term_diff', 'Identity')) %>%
        dplyr::select(-c('.chain', '.iteration', '.draw')) %>%
        bayestestR::describe_posterior(.,
                                       ci = 0.95,
                                       ci_method = 'hdi',
                                       test = c("p_direction", "p_significance"),
                                       BF = 1,
                                       centrality = "median")
# the self-other difference is obvious and not presented in the manuscript
# p_exp4a_dprime1_diff_id <- df_exp4a_sdt_m1_plot_diff %>%
#         dplyr::filter(str_detect(term_diff, '_SO_')) %>%
#         dplyr::mutate(term_diff = dplyr::case_when(grepl("_B", term_diff) ~ "Bad",
#                                                    grepl("_N", term_diff) ~"Neutral",
#                                                    grepl("_G", term_diff) ~ "Good"),
#                       term_diff = factor(term_diff, levels = c("Good", "Neutral", "Bad"))) %>%
#         #dplyr::filter(!str_detect(term_diff, '_GB_')) %>%
#         # tidyr::unite(term_diff, c('Identity', 'term_diff')) %>%
#         dplyr::filter((value >= -1.5) & (value <= 1.5)) %>%
#         dplyr::rename(conditions = term_diff) %>%
#         ggplot2::ggplot(aes(x = value, y = fct_rev(conditions), fill = after_stat(x > 0))) +
#         tidybayes::stat_halfeye() +
#         geom_vline(xintercept = 0, linetype = "dashed") +
#         scale_fill_manual(values = c('skyblue', 'gray80'),
#                     breaks = c(TRUE, FALSE),
#                     name = "Effect" , labels = c("Yes", "No")) +
#         # scale_fill_manual(values = c('skyblue')) +   # note: only one value here, used blue here
#         labs(x = expression(paste("Self-referential effect on ", italic("d"), " prime", sep = ' ')),
#              y = expression("Contrasts (Self vs. Other)"))

p_exp4a_dprime1_diff_diff <- df_exp4a_sdt_m1_plot_diff %>%
        dplyr::filter(str_detect(term_diff, '_diff_')) %>%
        # dplyr::filter(!str_detect(term_diff, 'GB')) %>%
        dplyr::mutate(term_diff = dplyr::case_when(grepl("_GB", term_diff) ~ "GB",
                                                   grepl("_GN", term_diff) ~"GN",
                                                   grepl("_BN", term_diff) ~ "BN"),
                      term_diff = factor(term_diff, levels = c("GN", "GB", "BN"))) %>%
        dplyr::filter((value >= -1.5) & (value <= 1.5)) %>%
        dplyr::rename(conditions = term_diff) %>%
        ggplot2::ggplot(aes(x = value, y = fct_rev(conditions), fill = after_stat(x > 0))) +
        tidybayes::stat_halfeye() +
        geom_vline(xintercept = 0, linetype = "dashed") +
        scale_fill_manual(values = c('skyblue', 'gray80'),
                    breaks = c(TRUE, FALSE),
                    name = "Effect" , labels = c("Yes", "No")) +
        # scale_fill_manual(values = c('skyblue')) +   # note: only one value here, used blue here
        labs(x = expression(paste("Self-referential effect on ", italic("d"), " prime", sep = ' ')),
             y = expression("Difference (Self vs. Other) of difference (Valence)"))

df_exp4a_dprime1_diff_diff <- df_exp4a_sdt_m1_plot_diff %>%  # difference between differences
        dplyr::filter(str_detect(term_diff, '_diff_')) %>%
        # dplyr::filter(!str_detect(term_diff, 'GB')) %>%
        dplyr::mutate(term_diff = dplyr::case_when(grepl("_GB", term_diff) ~ "GB",
                                                   grepl("_GN", term_diff) ~"GN",
                                                   grepl("_BN", term_diff) ~ "BN"),
                      term_diff = factor(term_diff, levels = c("GN", "GB", "BN"))) %>%
        tidyr::pivot_wider(.,
                           id_cols = c('.chain', '.iteration', '.draw'),
                           names_from = term_diff) %>%
        dplyr::select(-c('.chain', '.iteration', '.draw')) %>%
        bayestestR::describe_posterior(.,
                                       ci = 0.95,
                                       ci_method = 'hdi',
                                       test = c("p_direction", "p_significance"),
                                       centrality = "median")

test_res_sdt_4a <- bayestestR::sexit(df_exp4a_sdt_m1_plot_diff_wide[, c(4, 7, 13)])
test_res_sdt_4a
```

More importantly, we found evidence, albeit weak, that task-irrelevant moral character also played a role. For shapes associated with “self”, *d'* was greater when shapes had a good character inside (median = `r rep_m_d_exp4a_good_s`, 95% HDI [`r rep_ll_d_exp4a_good_s` `r rep_ul_d_exp4a_good_s`]) than shapes that have neutral character (median = `r rep_m_d_exp4a_neut_s`, 95% HDI [`r rep_ll_d_exp4a_neut_s` `r rep_ul_d_exp4a_neut_s`]), the difference (median = 0.08, 95% HDI [-0.10, 0.27]) has a 81.60% probability of being positive (> 0), 64.33% of being significant (> 0.05). For shapes associated with “other”, the pattern reversed: *d* prime was smaller when shapes had a good character inside (median = `r rep_m_d_exp4a_good_o`, 95% HDI [`r rep_ll_d_exp4a_good_o` `r rep_ul_d_exp4a_good_o`]) than had neutral (median = `r rep_m_d_exp4a_neut_o`, 95% HDI [`r rep_ll_d_exp4a_neut_o` `r rep_ul_d_exp4a_neut_o`]), the difference (median = -0.09, 95% HDI [-0.25, 0.05]) has a 89.03% probability of being negative (< 0), 71.38% of being significant (< -0.05). The difference between theses two effects (median = 0.18, 95% HDI [-0.06, 0.43]) has a 92.88% probability of being positive (> 0), 85.08% of being significant (> 0.05). See Figure \@ref(fig:plot-exp4-all).

```{r 4a_BGLMM_rt, echo=FALSE, results='hide', warning=FALSE, message=FALSE}
exp_name <- '4a'
exp4a_rt_m1 <- fun_rt_val_id(exp_name)

df_exp4a_m1_plot_rt <- exp4a_rt_m1 %>%
        tidybayes::gather_draws(`b_.*`, regex = TRUE)  %>%
        dplyr::mutate(.variable = gsub("b_", "", .variable)) %>%
        tidyr::pivot_wider(names_from = c(.variable), values_from = .value) %>%
        dplyr::mutate(NM_Self_Neutral = Intercept,               # calculate the differences between conditions
                      NM_Self_Bad = Intercept  + ValenceBad,
                      NM_Self_Good = Intercept  + ValenceGood ,
                      NM_Other_Neutral = Intercept  + IdentityOther,               # calculate the differences between conditions
                      NM_Other_Bad = Intercept  + ValenceBad  + `ValenceBad:IdentityOther`,
                      NM_Other_Good = Intercept  + ValenceGood + `ValenceGood:IdentityOther`,
                      M_Self_Neutral = Intercept  + ismatch,
                      M_Self_Bad = Intercept  + ismatch + `ismatch:ValenceBad`,
                      M_Self_Good = Intercept  + ismatch + `ismatch:ValenceGood`,
                      M_Other_Neutral = Intercept  + ismatch + `ismatch:IdentityOther`,
                      M_Other_Bad = Intercept  + ismatch + `ismatch:IdentityOther` + `ismatch:ValenceBad:IdentityOther`,
                      M_Other_Good = Intercept  + ismatch + `ismatch:IdentityOther` + `ismatch:ValenceGood:IdentityOther`) %>%
        dplyr::select(`.chain`, `.iteration`, `.draw`, contains('M_')) %>%
        tidyr::pivot_longer(cols = NM_Self_Neutral:M_Other_Good, names_to = "term", values_to =  "value") %>%  # wide to long
  dplyr::mutate(term = factor(term, levels = c('NM_Self_Neutral', 'NM_Self_Bad', 'NM_Self_Good',
                                               'NM_Other_Neutral', 'NM_Other_Bad', 'NM_Other_Good',
                                               'M_Self_Neutral', 'M_Self_Bad', 'M_Self_Good',
                                               'M_Other_Neutral', 'M_Other_Bad', 'M_Other_Good')),
                value = exp(value),
                value = value * 1000) 

p_exp4a_rt1 <- df_exp4a_m1_plot_rt %>%
        tidyr::separate(term, c('Match', 'Identity', 'Valence')) %>% 
        dplyr::mutate(Valence = factor(Valence, levels = c('Good', 'Neutral', 'Bad')),
                      Identity = factor(Identity, levels = c("Self", "Other"))) %>%
        dplyr::filter(Match == 'M') %>%
        # dplyr::rename(Experiments = condition) %>%
        dplyr::filter((value >=200) & (value <=1200)) %>%
        ggplot2::ggplot(aes(y = value, x = Identity, color = Valence)) +
        tidybayes::stat_halfeye(aes(fill = Valence), alpha = 0.7) +
        stat_summary(aes(group = Valence, color = Valence), fun = mean, geom = "line") +
        # geom_vline(data = p_ms_rt1_vlines, aes(xintercept = value, colour = Valence), linetype = "dashed") +
        labs(x = expression("Self-Referential"), 
             y = expression('Posteior of reaction times')) +
        scale_colour_brewer(palette = "Dark2") +
        scale_fill_brewer(palette = "Dark2") +
        # facet_grid(~ params , scales = "free_y") +
        theme_apa()
        # facet_wrap(~Identity) + 
        #theme_apa()

df_exp4a_rt1_hdi <- df_exp4a_m1_plot_rt %>%
        tidyr::separate(term, c('Match', 'Identity', 'Valence')) %>% 
        dplyr::mutate(Valence = factor(Valence, levels = c('Good', 'Neutral', 'Bad')),
                      Identity = factor(Identity, levels = c("Self", "Other"))) %>%
        dplyr::filter(Match == 'M') %>%
        dplyr::select(-c('Match')) %>%
        # dplyr::mutate(term_diff = stringr::str_sub(.$term_diff, end = -8)) %>%
        tidyr::pivot_wider(.,
                           id_cols = c('.chain', '.iteration', '.draw'),
                           values_from = 'value',
                           names_from = c('Valence', 'Identity')) %>%
        dplyr::select(-c('.chain', '.iteration', '.draw')) %>%
        bayestestR::describe_posterior(.,
                                       ci = 0.95,
                                       ci_method = 'hdi',
                                       test = c("p_direction", "p_significance"),
                                       BF = 1,
                                       centrality = "median")
# prepare for reporting results:
# neutral self
rep_m_exp4a_rt_neut_s <- df_exp4a_rt1_hdi %>% dplyr::filter(Parameter == "Neutral_Self") %>% dplyr::select(Median) %>% 
        dplyr::pull() %>%
        round(., digits = 0)
rep_ll_exp4a_rt_neut_s <- df_exp4a_rt1_hdi %>% dplyr::filter(Parameter == "Neutral_Self") %>% dplyr::select(CI_low) %>% 
        dplyr::pull() %>%
        round(., digits = 0)
rep_ul_exp4a_rt_neut_s <- df_exp4a_rt1_hdi %>% dplyr::filter(Parameter == "Neutral_Self") %>% dplyr::select(CI_high) %>% 
        dplyr::pull() %>%
        round(., digits = 0)

# good self
rep_m_exp4a_rt_good_s <- df_exp4a_rt1_hdi %>% dplyr::filter(Parameter == "Good_Self") %>% dplyr::select(Median) %>% 
        dplyr::pull() %>%
        round(., digits = 0)
rep_ll_exp4a_rt_good_s <- df_exp4a_rt1_hdi %>% dplyr::filter(Parameter == "Good_Self") %>% dplyr::select(CI_low) %>% 
        dplyr::pull() %>%
        round(., digits = 0)
rep_ul_exp4a_rt_good_s <- df_exp4a_rt1_hdi %>% dplyr::filter(Parameter == "Good_Self") %>% dplyr::select(CI_high) %>% 
        dplyr::pull() %>%
        round(., digits = 0)

# bad self
rep_m_exp4a_rt_bad_s <- df_exp4a_rt1_hdi %>% dplyr::filter(Parameter == "Bad_Self") %>% dplyr::select(Median) %>% 
        dplyr::pull() %>%
        round(., digits = 0)
rep_ll_exp4a_rt_bad_s <- df_exp4a_rt1_hdi %>% dplyr::filter(Parameter == "Bad_Self") %>% dplyr::select(CI_low) %>% 
        dplyr::pull() %>%
        round(., digits = 0)
rep_ul_exp4a_rt_bad_s <- df_exp4a_rt1_hdi %>% dplyr::filter(Parameter == "Bad_Self") %>% dplyr::select(CI_high) %>% 
        dplyr::pull() %>%
        round(., digits = 0)

# Neutral other
rep_m_exp4a_rt_neut_o <- df_exp4a_rt1_hdi %>% dplyr::filter(Parameter == "Neutral_Other") %>% dplyr::select(Median) %>% 
        dplyr::pull() %>%
        round(., digits = 0)
rep_ll_exp4a_rt_neut_o <- df_exp4a_rt1_hdi %>% dplyr::filter(Parameter == "Neutral_Other") %>% dplyr::select(CI_low) %>% 
        dplyr::pull() %>%
        round(., digits = 0)
rep_ul_exp4a_rt_neut_o <- df_exp4a_rt1_hdi %>% dplyr::filter(Parameter == "Neutral_Other") %>% dplyr::select(CI_high) %>% 
        dplyr::pull() %>%
        round(., digits = 0)

# good other
rep_m_exp4a_rt_good_o <- df_exp4a_rt1_hdi %>% dplyr::filter(Parameter == "Good_Other") %>% dplyr::select(Median) %>% 
        dplyr::pull() %>%
        round(., digits = 0)
rep_ll_exp4a_rt_good_o <- df_exp4a_rt1_hdi %>% dplyr::filter(Parameter == "Good_Other") %>% dplyr::select(CI_low) %>% 
        dplyr::pull() %>%
        round(., digits = 0)
rep_ul_exp4a_rt_good_o <- df_exp4a_rt1_hdi %>% dplyr::filter(Parameter == "Good_Other") %>% dplyr::select(CI_high) %>% 
        dplyr::pull() %>%
        round(., digits = 0)

# bad other
rep_m_exp4a_rt_bad_o <- df_exp4a_rt1_hdi %>% dplyr::filter(Parameter == "Bad_Other") %>% dplyr::select(Median) %>% 
        dplyr::pull() %>%
        round(., digits = 0)
rep_ll_exp4a_rt_bad_o <- df_exp4a_rt1_hdi %>% dplyr::filter(Parameter == "Bad_Other") %>% dplyr::select(CI_low) %>% 
        dplyr::pull() %>%
        round(., digits = 0)
rep_ul_exp4a_rt_bad_o <- df_exp4a_rt1_hdi %>% dplyr::filter(Parameter == "Bad_Other") %>% dplyr::select(CI_high) %>% 
        dplyr::pull() %>%
        round(., digits = 0)

df_exp4a_m1_plot_rt_diff_wide <- df_exp4a_m1_plot_rt %>%
        # tidyr::unite(term, c('params', 'Identity', 'Valence')) %>%
        dplyr::ungroup() %>%
        dplyr::select(term, `.chain`, `.iteration`, `.draw`, `value`) %>%
        tidyr::pivot_wider(names_from = c(term), values_from = value) %>%   # long to wide
        dplyr::mutate(
                diff_GN_S_RT_M = M_Self_Good - M_Self_Neutral,
                diff_BN_S_RT_M = M_Self_Bad - M_Self_Neutral,
                diff_GB_S_RT_M = M_Self_Good - M_Self_Bad,
                diff_GN_O_RT_M = M_Other_Good - M_Other_Neutral,
                diff_BN_O_RT_M = M_Other_Bad - M_Other_Neutral,
                diff_GB_O_RT_M = M_Other_Good - M_Other_Bad,
                diff_SO_G_RT_M = M_Self_Good - M_Other_Good,
                diff_SO_N_RT_M = M_Self_Neutral - M_Other_Neutral,
                diff_SO_B_RT_M = M_Self_Bad - M_Other_Bad,
                diff_diff_GN_SO = diff_GN_S_RT_M - diff_GN_O_RT_M,
                diff_diff_GB_SO = diff_GB_S_RT_M - diff_GB_O_RT_M,
                diff_diff_BN_SO = diff_BN_S_RT_M - diff_BN_O_RT_M
                ) %>%
  dplyr::select(`.chain`, `.iteration`, `.draw`,
               diff_GN_S_RT_M:diff_diff_BN_SO) 

df_exp4a_m1_plot_rt_diff<- df_exp4a_m1_plot_rt_diff_wide %>%
  tidyr::pivot_longer(cols = diff_GN_S_RT_M:diff_diff_BN_SO, names_to = "term_diff", values_to =  "value") %>%  # wide to long
  dplyr::mutate(term_diff = factor(term_diff, levels = c('diff_GN_S_RT_M', 'diff_BN_S_RT_M', 'diff_GB_S_RT_M', 
                                                         'diff_GN_O_RT_M', 'diff_BN_O_RT_M', 'diff_GB_O_RT_M', 
                                                         'diff_SO_G_RT_M', 'diff_SO_N_RT_M', 'diff_SO_B_RT_M',
                                                         'diff_diff_GN_SO', 'diff_diff_GB_SO', 'diff_diff_BN_SO')))
p_exp4a_rt_diff_val <- df_exp4a_m1_plot_rt_diff %>%
        dplyr::filter(str_detect(term_diff, '_S_RT_M|_O_RT_M')) %>%
        dplyr::mutate(Identity = dplyr::case_when(grepl("_S", term_diff) ~ "Self",
                                                  grepl("_O", term_diff) ~"Other"),
                      Identity = factor(Identity, levels = c("Self", "Other"))) %>%
        dplyr::filter(!str_detect(term_diff, '_GB_|_BN')) %>%
        dplyr::mutate(term_diff = stringr::str_sub(.$term_diff, end = -8)) %>%
        dplyr::mutate(term_diff = dplyr::case_when(#grepl("_GB", term_diff) ~ "Good vs. Bad",
                                                   grepl("_GN", term_diff) ~"Good vs. Neutral",
                                                   #grepl("_BN", term_diff) ~ "Bad vs. Neutral",
                                                   ),
                      #term_diff = factor(term_diff, levels = c("Good vs. Bad", "Good vs. Neutral", "Bad vs. Neutral")),
                      ) %>%
        # dplyr::filter(str_detect(term_diff, '_SO_')) %>%
        #dplyr::filter(!str_detect(term_diff, '_GB_')) %>%
        # tidyr::unite(term_diff, c('Identity', 'term_diff')) %>%
        # dplyr::filter((value >= -1.5) & (value <= 1.5)) %>%
        dplyr::rename(conditions = term_diff) %>%
        ggplot2::ggplot(aes(x = value, fill = after_stat(x < 0))) + # y = fct_rev(conditions), 
        tidybayes::stat_halfeye() +
        geom_vline(xintercept = 0, linetype = "dashed") +
        scale_fill_manual(values = c('skyblue', 'gray80'),
                    breaks = c(TRUE, FALSE),
                    name = "Effect" , labels = c("Yes", "No")) + 
        # scale_fill_manual(values = c('gray80', 'skyblue')) +
        labs(x = expression("Valence effect on RTs "),
             y = expression("Good vs. Neutral")) + 
        scale_x_continuous(breaks=seq(-25, 50, 50)) +
        facet_wrap( ~ Identity, nrow = 1) + 
        theme_apa()

p_exp4a_rt_diff_id <- df_exp4a_m1_plot_rt_diff %>%
        dplyr::filter(str_detect(term_diff, '_SO_')) %>%
        dplyr::mutate(term_diff = dplyr::case_when(grepl("_B", term_diff) ~ "Bad",
                                                   grepl("_N", term_diff) ~"Neutral",
                                                   grepl("_G", term_diff) ~ "Good"),
                      term_diff = factor(term_diff, levels = c("Good", "Neutral", "Bad"))) %>%
        # dplyr::filter(!str_detect(term_diff, '_GB_')) %>%
        # tidyr::unite(term_diff, c('Identity', 'term_diff')) %>%
        # dplyr::filter((value >= -1.5) & (value <= 1.5)) %>%
        dplyr::rename(conditions = term_diff) %>%
        ggplot2::ggplot(aes(x = value, y = fct_rev(conditions), fill = after_stat(x < 0))) +
        tidybayes::stat_halfeye() +
        geom_vline(xintercept = 0, linetype = "dashed") +
        scale_fill_manual(values = c('skyblue', 'gray80'),
                    breaks = c(TRUE, FALSE),
                    name = "Effect" , labels = c("Yes", "No")) + 
        # scale_fill_manual(values = c('skyblue')) +   # note: only one value here, used blue here
        labs(x = expression("Self-referential effect on RTs "),
             y = expression("Contrasts (Self vs. Other)"))

p_exp4a_rt1_diff_diff <- df_exp4a_m1_plot_rt_diff %>%
        dplyr::filter(str_detect(term_diff, '_diff_')) %>%
        dplyr::mutate(term_diff = dplyr::case_when(grepl("_GB", term_diff) ~ "GB",
                                                   grepl("_GN", term_diff) ~"GN",
                                                   grepl("_BN", term_diff) ~ "BN"),
                      term_diff = factor(term_diff, levels = c("GN", "GB", "BN"))) %>%
        #dplyr::filter(!str_detect(term_diff, '_GB_')) %>%
        # tidyr::unite(term_diff, c('Identity', 'term_diff')) %>%
        # dplyr::filter((value >= -1.5) & (value <= 1.5)) %>%
        dplyr::rename(conditions = term_diff) %>%
        ggplot2::ggplot(aes(x = value, y = fct_rev(conditions), fill = after_stat(x > 0))) +
        tidybayes::stat_halfeye() +
        geom_vline(xintercept = 0, linetype = "dashed") +
        scale_fill_manual(values = c('skyblue', 'gray80'),
                    breaks = c(TRUE, FALSE),
                    name = "Effect" , labels = c("Yes", "No")) +
        # scale_fill_manual(values = c('skyblue')) +   # note: only one value here, used blue here
        labs(x = expression("Interaction effect on RT"),
             y = expression("Difference (Self vs. Other) of difference (Valence)")) +
        theme_apa()

test_res_rt_4a <- bayestestR::sexit(df_exp4a_m1_plot_rt_diff_wide[, c(4, 7, 13)])
test_res_rt_4a

```

```{r plot-exp4a-BGLM, fig.cap="exp4a: Results of Bayesian GLM analysis.",  fig.height=9, fig.width=12, warning=FALSE}
library(patchwork)
p1 <- (p_exp4a_rt1 + exp4a_sdt_p) #+ plot_layout(nrow = 1, guides = "collect") 
p2 <- (p_exp4a_rt_diff_val + p_exp4a_dprime1_diff_val) + plot_layout(nrow = 1, byrow = FALSE, guides = "collect") 
# p3 <- p_exp4a_rt_diff_id + p_exp4a_dprime1_diff_id + plot_layout(nrow = 1, byrow = FALSE, guides = "collect") 
#p1/ 
#        p2  +  plot_layout(nrow = 2)  
        # p3 +  plot_layout(nrow = 3) 
```

A similar pattern was found for RTs in matched trials. For the “self” condition, when a good character was presented inside the shapes, the RTs (median = `r rep_m_exp4a_rt_good_s`, 95% HDI [`r rep_ll_exp4a_rt_good_s` `r rep_ul_exp4a_rt_good_s`]) were faster than when a neutral character (median = `r rep_m_exp4a_rt_neut_s`, 95% HDI [`r rep_ll_exp4a_rt_neut_s` `r rep_ul_exp4a_rt_neut_s`]) were inside, the effect (median = -7.87, 95% HDI [-17.49, 2.00]) has a 94.55% probability of being negative (< 0), 94.50% of being significant (< -0.05), and 93.97% of being large (< -0.30). In contrast, RTs for shapes associated with good character inside (median = `r rep_m_exp4a_rt_good_o`, 95% HDI [`r rep_ll_exp4a_rt_good_o` `r rep_ul_exp4a_rt_good_o`]) were slower than those with neutral character (median =  `r rep_m_exp4a_rt_neut_o`, 95% HDI [`r rep_ll_exp4a_rt_neut_o` `r rep_ul_exp4a_rt_neut_o`]) inside, the effect (median = 11.64, 95% HDI [-3.99, 27.83]) has a 93.00% probability of being positive (> 0), 92.83% of being significant (> 0.05), and 92.40% of being large (> 0.30). The difference between the effects (median = -19.46, 95% HDI [-42.93, 3.72]) has a 94.90% probability of being negative (< 0), 94.88% of being significant (< -0.05), and 94.58% of being large (< -0.30).

In experiment 4b, where moral characters were task-relevant and “self” vs “other” were task-irrelevant, we found a main effect of moral character: performance for shapes associated with good characters was better than other-related conditions on both *d'* and reaction times. This pattern, again, shows a robust prioritization effect of good character.
```{r 4b_BGLMM_sdt, echo=FALSE, results='hide', warning=FALSE, message=FALSE}
exp_name <- '4b'
exp4b_sdt_m1 <- fun_sdt_val_id(exp_name)

# extract the population level parameters
# criteria
df_exp4b_sdt_m1_plot <- exp4b_sdt_m1 %>%
  tidybayes::gather_draws(`b_.*`, regex = TRUE) %>% # get the traces of population level parameters
  # create two columns for two independent factors.
  dplyr::mutate(Valence = dplyr::case_when(grepl("Neutral", .variable) ~ "Neutral",
                                           grepl("Bad", .variable) ~"Bad",
                                           grepl("Good", .variable) ~"Good"),
                Identity = dplyr::case_when(grepl("Self", .variable) ~ "Self",
                                           grepl("Other", .variable) ~"Other"),
                params = dplyr::case_when(grepl("ismatch", .variable) ~ "dprime",
                                           !grepl("ismatch", .variable) ~"criterion"),
                params = factor(params, levels = c('dprime', 'criterion')),
                Identity = factor(Identity, levels = c("Self", "Other")),
                Valence = factor(Valence, levels = c('Good', 'Neutral',  'Bad'))) 

exp4b_sdt_p <- df_exp4b_sdt_m1_plot %>%
  dplyr::filter(params == 'dprime') %>%  # select only d prime
  ggplot2::ggplot(aes(x = Valence, y = .value, color = Identity)) +
  tidybayes::stat_halfeye(aes(fill = Identity), alpha = 0.7) + # position=position_dodge(width = 0.1)
  stat_summary(aes(group = Identity, color = Identity), fun = mean, geom = "line") +
  labs(x = expression("Valence"), 
      y = expression(paste("Posteior of sensitivity ", italic("d'"), sep = ' '))) +
  #scale_colour_brewer(palette = "Dark2") +
  #scale_fill_brewer(palette = "Dark2") +
  theme_apa() 

df_exp4b_sdt_hdi <- df_exp4b_sdt_m1_plot %>%
        dplyr::ungroup() %>%
        dplyr::filter(params == 'dprime') %>%
        dplyr::select(-c('.variable', 'params')) %>%
        # dplyr::mutate(term_diff = stringr::str_sub(.$term_diff, end = -8)) %>%
        tidyr::pivot_wider(.,
                           id_cols = c('.chain', '.iteration', '.draw'),
                           values_from = '.value',
                           names_from = c('Valence', 'Identity')) %>%
        dplyr::select(-c('.chain', '.iteration', '.draw')) %>%
        bayestestR::describe_posterior(.,
                                       ci = 0.95,
                                       ci_method = 'hdi',
                                       test = c("p_direction", "p_significance"),
                                       BF = 1,
                                       centrality = "median") %>%
        dplyr::select(Parameter, Median, CI_low, CI_high) %>%
        tidyr::pivot_longer(cols = Median:CI_high, names_to = "Index", values_to = "Values") %>%
        tidyr::unite("Indicies", Parameter:Index) %>%
        dplyr::mutate(Values = round(Values, digits = 3))

df_exp4b_sdt_m1_plot_diff_wide <- df_exp4b_sdt_m1_plot %>%
        tidyr::unite(term, c('params', 'Identity', 'Valence')) %>%
        dplyr::ungroup() %>%
        dplyr::select(term, `.chain`, `.iteration`, `.draw`, `.value`) %>%
        tidyr::pivot_wider(names_from = c(term), values_from = .value) %>%   # long to wide
        dplyr::mutate(
                diff_GN_dprm_S = dprime_Self_Good - dprime_Self_Neutral,
                diff_BN_dprm_S = dprime_Self_Bad - dprime_Self_Neutral,
                diff_GB_dprm_S = dprime_Self_Good - dprime_Self_Bad,
                diff_GN_dprm_O = dprime_Other_Good - dprime_Other_Neutral,
                diff_BN_dprm_O = dprime_Other_Bad - dprime_Other_Neutral,
                diff_GB_dprm_O = dprime_Other_Good - dprime_Other_Bad,
                diff_SO_dprm_G = dprime_Self_Good - dprime_Other_Good,
                diff_SO_dprm_N =  dprime_Self_Neutral - dprime_Other_Neutral,
                diff_SO_dprm_B =  dprime_Self_Bad - dprime_Other_Bad,
                #diff_diff_SO_GN = diff_SO_dprm_G - diff_SO_dprm_N
                ) %>%
  dplyr::select(`.chain`, `.iteration`, `.draw`,
               diff_GN_dprm_S, diff_BN_dprm_S, diff_GB_dprm_S,
               diff_GN_dprm_O, diff_BN_dprm_O, diff_GB_dprm_O, 
               diff_SO_dprm_G, diff_SO_dprm_N, diff_SO_dprm_B,
               # diff_diff_SO_GN
               )

df_exp4b_sdt_m1_plot_diff <- df_exp4b_sdt_m1_plot_diff_wide %>%
  tidyr::pivot_longer(cols = diff_GN_dprm_S:diff_SO_dprm_B, names_to = "term_diff", values_to =  "value") %>%  # wide to long
  dplyr::mutate(term_diff = factor(term_diff, levels = c('diff_GB_dprm_S', 'diff_GN_dprm_S', 'diff_BN_dprm_S', 
                                                         'diff_GB_dprm_O', 'diff_GN_dprm_O', 'diff_BN_dprm_O', 
                                                         'diff_SO_dprm_G', 'diff_SO_dprm_N', 'diff_SO_dprm_B')))

df_exp4b_sdt_m1_diff_hdi <- df_exp4b_sdt_m1_plot_diff %>%
        dplyr::group_by(term_diff) %>%
        median_hdi(value) %>%
        dplyr::ungroup() %>%
        dplyr::select(term_diff, value, `.lower`, `.upper`) %>%
        tidyr::pivot_longer(cols = value:`.upper`, names_to = "Index", values_to = "Values") %>%
        tidyr::unite("Indicies", term_diff:Index) %>%
        dplyr::mutate(Values = round(Values, digits = 3),
                      Indicies = str_remove_all(Indicies, "\\."))
        
p_exp4b_dprime1_diff_val <- df_exp4b_sdt_m1_plot_diff %>%
        dplyr::filter(str_detect(term_diff, '_dprm_S|_dprm_O')) %>%
        dplyr::mutate(Identity = dplyr::case_when(grepl("_S", term_diff) ~ "Self",
                                           grepl("_O", term_diff) ~"Other"),
                      Identity = factor(Identity, levels = c("Self", "Other"))) %>%
        dplyr::mutate(term_diff = stringr::str_sub(.$term_diff, end = -8)) %>%
        dplyr::mutate(term_diff = dplyr::case_when(grepl("_GB", term_diff) ~ "Good vs. Bad",
                                                   grepl("_GN", term_diff) ~"Good vs. Neutral",
                                                   grepl("_BN", term_diff) ~ "Bad vs. Neutral"),
                      term_diff = factor(term_diff, levels = c("Good vs. Bad", "Good vs. Neutral", "Bad vs. Neutral"))) %>%
        #dplyr::filter((value >= -1.5) & (value <= 1.5)) %>%
        dplyr::rename(conditions = term_diff) %>%
        ggplot2::ggplot(aes(x = value, y = fct_rev(conditions), fill = after_stat(x > 0))) +
        tidybayes::stat_halfeye() +
        geom_vline(xintercept = 0, linetype = "dashed") +
        scale_fill_manual(values = c('skyblue', 'gray80'),
                    breaks = c(TRUE, FALSE),
                    name = "Effect" , labels = c("Yes", "No")) + 
        # scale_fill_manual(values = c('gray80', 'skyblue')) +
        labs(x = expression(paste("Valence effect on ", italic("d"), " prime", sep = ' ')),
             y = expression("Contrasts")) + 
        facet_wrap( ~ Identity, nrow = 1) +
        theme_apa()

p_exp4b_dprime1_diff_id <- df_exp4b_sdt_m1_plot_diff %>%
        dplyr::filter(str_detect(term_diff, '_SO_')) %>%
        dplyr::filter(!str_detect(term_diff, '_B')) %>%
        dplyr::mutate(term_diff = dplyr::case_when(# grepl("_B", term_diff) ~ "Bad",
                                                   grepl("_N", term_diff) ~"Neutral",
                                                   grepl("_G", term_diff) ~ "Good"),
                      term_diff = factor(term_diff, levels = c("Good", "Neutral", "Bad"))) %>%
        #dplyr::filter(!str_detect(term_diff, '_GB_')) %>%
        # tidyr::unite(term_diff, c('Identity', 'term_diff')) %>%
        dplyr::filter((value >= -1.5) & (value <= 1.5)) %>%
        dplyr::rename(conditions = term_diff) %>%
        ggplot2::ggplot(aes(x = value, fill = after_stat(x > 0))) + #y = fct_rev(conditions), 
        tidybayes::stat_halfeye() +
        geom_vline(xintercept = 0, linetype = "dashed") +
        scale_fill_manual(values = c('skyblue', 'gray80'),
                    breaks = c(TRUE, FALSE),
                    name = "Effect" , labels = c("Yes", "No")) +
        # scale_fill_manual(values = c('skyblue')) +   # note: only one value here, used blue here
        labs(x = expression(paste("Self-referential effect on ", italic("d"), " prime", sep = ' ')),
             y = expression("Contrasts (Self vs. Other)"))+ 
        scale_x_continuous(breaks=seq(-0.25, 0.75, 0.5)) +
        facet_wrap( ~ conditions, nrow = 1) +
        theme_apa()

test_res_sdt_4b <- bayestestR::sexit(df_exp4b_sdt_m1_plot_diff_wide[, c(10, 11, 12)])
test_res_sdt_4b
```

Most importantly, we found evidence that task-irrelevant labels, “self” or “other”, also played a role. For shapes associated with good character, the *d* prime was greater when shapes had a "self" inside than with "other" inside ($mean_{diff}$ = 0.14, 95% HDI [-0.05, 0.34]) has a 92.35% probability of being positive (> 0), 81.80% of being significant (> 0.05). However, the difference did not occur when the target shape where associated with "neutral" ($mean_{diff}$ = 0.04, 95% HDI [-0.13, 0.22]) has a 67.20% probability of being positive (> 0), 44.80% of being significant (> 0.05). Neither for the "bad" person condition: $mean_{diff}$  = 0.10, 95% HDI [-0.16, 0.37]) has a 77.03% probability of being positive (> 0), 64.62% of being significant (> 0.05). 

```{r 4b_BGLMM_rt, echo=FALSE, results='hide', warning=FALSE, message=FALSE}
exp_name <- '4b'
exp4b_rt_m1 <- fun_rt_val_id(exp_name)

df_exp4b_m1_plot_rt <- exp4b_rt_m1 %>%
        tidybayes::gather_draws(`b_.*`, regex = TRUE)  %>%
        dplyr::mutate(.variable = gsub("b_", "", .variable)) %>%
        tidyr::pivot_wider(names_from = c(.variable), values_from = .value) %>%
        dplyr::mutate(NM_Self_Neutral = Intercept,               # calculate the differences between conditions
                      NM_Self_Bad = Intercept  + ValenceBad,
                      NM_Self_Good = Intercept  + ValenceGood ,
                      NM_Other_Neutral = Intercept  + IdentityOther,               # calculate the differences between conditions
                      NM_Other_Bad = Intercept  + ValenceBad  + `IdentityOther:ValenceBad`,
                      NM_Other_Good = Intercept  + ValenceGood + `IdentityOther:ValenceGood`,
                      M_Self_Neutral = Intercept  + ismatch,
                      M_Self_Bad = Intercept  + ismatch + `ismatch:ValenceBad`,
                      M_Self_Good = Intercept  + ismatch + `ismatch:ValenceGood`,
                      M_Other_Neutral = Intercept  + ismatch + `ismatch:IdentityOther`,
                      M_Other_Bad = Intercept  + ismatch + `ismatch:IdentityOther` + `ismatch:IdentityOther:ValenceBad`,
                      M_Other_Good = Intercept  + ismatch + `ismatch:IdentityOther` + `ismatch:IdentityOther:ValenceGood`) %>%
        dplyr::select(`.chain`, `.iteration`, `.draw`, contains('M_')) %>%
        tidyr::pivot_longer(cols = NM_Self_Neutral:M_Other_Good, names_to = "term", values_to =  "value") %>%  # wide to long
  dplyr::mutate(term = factor(term, levels = c('NM_Self_Neutral', 'NM_Self_Bad', 'NM_Self_Good',
                                               'NM_Other_Neutral', 'NM_Other_Bad', 'NM_Other_Good',
                                               'M_Self_Neutral', 'M_Self_Bad', 'M_Self_Good',
                                               'M_Other_Neutral', 'M_Other_Bad', 'M_Other_Good')),
                value = exp(value),
                value = value * 1000) 

p_exp4b_rt1 <- df_exp4b_m1_plot_rt %>%
        tidyr::separate(term, c('Match', 'Identity', 'Valence')) %>% 
        dplyr::mutate(Valence = factor(Valence, levels = c('Good', 'Neutral', 'Bad')),
                      Identity = factor(Identity, levels = c("Self", "Other"))) %>%
        dplyr::filter(Match == 'M') %>%
        # dplyr::rename(Experiments = condition) %>%
        dplyr::filter((value >=200) & (value <=1200)) %>%
        ggplot2::ggplot(aes(y = value, x = Valence, color = Identity)) +
        tidybayes::stat_halfeye(aes(fill = Identity), alpha = 0.7) +
        stat_summary(aes(group = Identity, color = Identity), fun = mean, geom = "line") +
        # geom_vline(data = p_ms_rt1_vlines, aes(xintercept = value, colour = Valence), linetype = "dashed") +
        labs(x = expression("Valence"), 
             y = expression('Posteior of reaction times')) +
        #scale_colour_brewer(palette = "Dark2") +
        #scale_fill_brewer(palette = "Dark2")  +
        theme_apa()

df_exp4b_m1_plot_rt_diff_wide <- df_exp4b_m1_plot_rt %>%
        # tidyr::unite(term, c('params', 'Identity', 'Valence')) %>%
        dplyr::ungroup() %>%
        dplyr::select(term, `.chain`, `.iteration`, `.draw`, `value`) %>%
        tidyr::pivot_wider(names_from = c(term), values_from = value) %>%   # long to wide
        dplyr::mutate(
                diff_GN_S_RT_M = M_Self_Good - M_Self_Neutral,
                diff_BN_S_RT_M = M_Self_Bad - M_Self_Neutral,
                diff_GB_S_RT_M = M_Self_Good - M_Self_Bad,
                diff_GN_O_RT_M = M_Other_Good - M_Other_Neutral,
                diff_BN_O_RT_M = M_Other_Bad - M_Other_Neutral,
                diff_GB_O_RT_M = M_Other_Good - M_Other_Bad,
                diff_SO_G_RT_M = M_Self_Good - M_Other_Good,
                diff_SO_N_RT_M = M_Self_Neutral - M_Other_Neutral,
                diff_SO_B_RT_M = M_Self_Bad - M_Other_Bad
                ) %>%
        dplyr::select(`.chain`, `.iteration`, `.draw`,
                       diff_GN_S_RT_M:diff_SO_B_RT_M) 

df_exp4b_m1_plot_rt_diff <- df_exp4b_m1_plot_rt_diff_wide %>%
  tidyr::pivot_longer(cols = diff_GN_S_RT_M:diff_SO_B_RT_M, names_to = "term_diff", values_to =  "value") %>%  # wide to long
  dplyr::mutate(term_diff = factor(term_diff, levels = c('diff_GN_S_RT_M', 'diff_BN_S_RT_M', 'diff_GB_S_RT_M', 
                                                         'diff_GN_O_RT_M', 'diff_BN_O_RT_M', 'diff_GB_O_RT_M', 
                                                         'diff_SO_G_RT_M', 'diff_SO_N_RT_M', 'diff_SO_B_RT_M')))

df_exp4b_m1_rt_diff_hdi <- df_exp4b_m1_plot_rt_diff %>%
        dplyr::group_by(term_diff) %>%
        median_hdi(value) %>%
        dplyr::ungroup() %>%
        dplyr::select(term_diff, value, `.lower`, `.upper`) %>%
        tidyr::pivot_longer(cols = value:`.upper`, names_to = "Index", values_to = "Values") %>%
        tidyr::unite("Indicies", term_diff:Index) %>%
        dplyr::mutate(Values = round(Values, digits = 0),
                      Indicies = str_remove_all(Indicies, "\\."))

p_exp4b_rt_diff_val <- df_exp4b_m1_plot_rt_diff %>%
        dplyr::filter(str_detect(term_diff, '_S_RT_M|_O_RT_M')) %>%
        dplyr::mutate(Identity = dplyr::case_when(grepl("_S", term_diff) ~ "Self",
                                                  grepl("_O", term_diff) ~"Other"),
                      Identity = factor(Identity, levels = c("Self", "Other"))) %>%
        dplyr::mutate(term_diff = stringr::str_sub(.$term_diff, end = -8)) %>%
        dplyr::mutate(term_diff = dplyr::case_when(grepl("_GB", term_diff) ~ "Good vs. Bad",
                                                   grepl("_GN", term_diff) ~"Good vs. Neutral",
                                                   grepl("_BN", term_diff) ~ "Bad vs. Neutral"),
                      term_diff = factor(term_diff, levels = c("Good vs. Bad", "Good vs. Neutral", "Bad vs. Neutral"))) %>%
        # dplyr::filter((value >= -1.5) & (value <= 1.5)) %>%
        dplyr::rename(conditions = term_diff) %>%
        ggplot2::ggplot(aes(x = value, y = fct_rev(conditions), fill = after_stat(x < 0))) +
        tidybayes::stat_halfeye() +
        geom_vline(xintercept = 0, linetype = "dashed") +
        scale_fill_manual(values = c('skyblue', 'gray80'),
                    breaks = c(TRUE, FALSE),
                    name = "Effect" , labels = c("Yes", "No")) + 
        # scale_fill_manual(values = c('gray80', 'skyblue')) +
        labs(x = expression("Valence effect on RTs "),
             y = expression("Contrasts")) + 
        facet_wrap( ~ Identity, nrow = 1) +
        theme_apa()

p_exp4b_rt_diff_id <- df_exp4b_m1_plot_rt_diff %>%
        dplyr::filter(str_detect(term_diff, '_SO_')) %>%
        dplyr::filter(!str_detect(term_diff, '_B')) %>%
        dplyr::mutate(term_diff = dplyr::case_when(# grepl("_B", term_diff) ~ "Bad",
                                                   grepl("_N", term_diff) ~"Neutral",
                                                   grepl("_G", term_diff) ~ "Good"),
                      term_diff = factor(term_diff, levels = c("Good", "Neutral"))) %>%
        # dplyr::filter(!str_detect(term_diff, '_GB_')) %>%
        # tidyr::unite(term_diff, c('Identity', 'term_diff')) %>%
        # dplyr::filter((value >= -1.5) & (value <= 1.5)) %>%
        dplyr::rename(conditions = term_diff) %>%
        ggplot2::ggplot(aes(x = value, fill = after_stat(x < 0))) + # , y = fct_rev(conditions)
        tidybayes::stat_halfeye() +
        geom_vline(xintercept = 0, linetype = "dashed") +
        scale_fill_manual(values = c('skyblue', 'gray80'),
                    breaks = c(TRUE, FALSE),
                    name = "Effect" , labels = c("Yes", "No")) + 
        # scale_fill_manual(values = c('skyblue')) +   # note: only one value here, used blue here
        labs(x = expression("Self-referential effect on RTs "),
             y = expression("Contrasts (Self vs. Other)")) +
        scale_x_continuous(breaks=seq(-75, 50, 50)) +
        facet_wrap( ~ conditions, nrow = 1) +
        theme_apa()

test_res_rt_4b <- bayestestR::sexit(df_exp4b_m1_plot_rt_diff_wide[, c(10, 11, 12)])
test_res_rt_4b
```

```{r plot-exp4b-BGLM, fig.cap="exp4a: Results of Bayesian GLM analysis.",  fig.height=9, fig.width=12, warning=FALSE}
library(patchwork)
p3 <- (p_exp4b_rt1 + exp4b_sdt_p ) + plot_layout(nrow = 1, guides = "collect") 
# p2 <- p_exp4b_rt_diff_val + p_exp4b_dprime1_diff_val + plot_layout(nrow = 1, byrow = FALSE, guides = "collect") 
p4 <- (p_exp4b_rt_diff_id + p_exp4b_dprime1_diff_id) + plot_layout(nrow = 1, byrow = FALSE, guides = "collect") 
```

```{r plot-exp4-all, fig.cap="Experiment 4: Implicit binding between good character and the self.",  fig.height=9, fig.width=15, warning=FALSE}
library(patchwork)

design <- "
  1234
  5678
"
p_exp4a_rt1 + exp4a_sdt_p + p_exp4b_rt1 + exp4b_sdt_p + p_exp4a_rt_diff_val + p_exp4a_dprime1_diff_val + p_exp4b_rt_diff_id + p_exp4b_dprime1_diff_id + plot_annotation(tag_levels = 'A') + plot_layout(design = design, guides = "collect")

# ((p1 +plot_layout(guides = "collect")) + p3 + plot_layout(guides = "collect")) + plot_layout(ncol = 3, widths = c(1, 1, 4), guides = 'keep')
# 
# (((p_exp4a_rt1 + exp4a_sdt_p + plot_layout(guides = "collect", nrow = 1)) + 
#                 (p_exp4b_rt1 + exp4b_sdt_p + plot_layout(guides = "collect", nrow = 1)) + plot_layout(nrow = 1)) + 
#         ((p_exp4a_rt_diff_val + p_exp4a_dprime1_diff_val + plot_layout(guides = "collect", nrow = 1)) 
#          + (p_exp4b_rt_diff_id + p_exp4b_dprime1_diff_id + plot_layout(guides = "collect", nrow = 1)) + plot_layout(nrow = 1))) + 
#         plot_layout(design = design, guides = "keep") +  plot_annotation(tag_levels = 'A')  

```

The same trend appeared for the RT data. For shapes associated with good character, having a "self" inside shapes reduced the reaction times as compared to having an "other" inside the shapes ($mean_{diff}$ = -55.10, 95% HDI [-75.27, -35.11]) has a 100% probability of being negative (< 0), 100.00% of being significant (< -0.05). However, when the shapes were associated neutral character, having a "self" inside shapes increased the RTs: $mean_{diff}$ = 10.88, 95% HDI [0.67, 20.84]) has a 98.20% probability of being positive (> 0), 98.15% of being significant (> 0.05). While having "self" slightly increased the RT than having "other" inside the shapes for the bad character: $mean_{diff}$ = 5.39, 95% HDI [-16.59, 27.37]) has a 69.45% probability of being positive (> 0), 69.27% of being significant (> 0.05), See Figure \@ref(fig:plot-exp4-all).

# Discussion

Across nine experiments, we explored the prioritization effect of moral character and the underlying mechanism by a combination of social associative learning and perceptual matching task. First, we found a robust effect that good character was prioritized in the shape-label matching task across five experiments. Second, across three experiments, we found that the prioritization of good character was not solely driven by moral valence itself, i.e., "good" vs "bad". Instead, this effect was modulated by self-referential processing: prioritization only occurred when moral characters are self-referential. Finally, the prioritization of the combination of good character and self occurred, albeit weak, even when either the self- or character-related information was irrelevant to the experimental task (experiment 4a and 4b). In contrast, performance to the combination of good character and “other”, explicitly or implicitly, was worse than the combination of neutral character and “other”. Together, these results highlighted the importance of the self in perceiving information related to moral characters, suggesting a spontaneous self-referential process when making perceptual decision-making for moral characters. These results are in line with a growing literature on the social and relational nature of perception (@xiao_perceiving_2016; @freeman_chapter_2020; ＠hafri_perception_2021) and deepened our understanding of mechanisms of perceptual decision-making of moral information.

The current study provided robust evidence for the prioritization of good character in perceptual decision-making. The existence of the effect of moral valence on perception has been disputed. For instance, [@anderson_visual_2011] reported that faces associated with bad social behavior capture attention more rapidly, however, an independent team failed to replicate the effect [@stein_no_2017]. Another study by @gantman_moral_2014 found that moral words are more likely to be judged as words when it was presented subliminally, however, this effect may be caused by semantic priming instead of morality [@firestone_cognition_2015; @jussim_interpretations_2016]. In the current study, we found the prioritization effect across five experiments, the sample size of individual experiments and combined provide strong evidence for the existence of the effect. Moreover, the associative learning task allowed us to eliminate the semantic priming effect for two reasons. First, associations between shapes and moral characters were acquired right before the perceptual matching task, semantic priming from pre-existed knowledge was impossible. Second, there were only a few pairs of stimuli were used and each stimulus represented different conditions, making it impossible for priming between trials. Importantly, a series of control experiments (1b, 1c, and 2) further excluded other confounding factors such as familiarity, presenting sequence, or words-based associations, suggesting that it was the moral content that drove the prioritization of good character. 

The robust prioritization of good character found in the current study was incongruent with previous moral perception studies, which usually reported a negativity effect, i.e., information related to bad character is processed preferentially [@anderson_visual_2011; @eiserbeck_visual_2020]. This discrepancy may be caused by the experimental task: while in many previous moral perception studies, the participants were asked to detect the existence of a stimulus, the current task asked participants to recognize a pattern. In other words, previous studies targeted early stages of perception while the current task focused more on decision-making at a relatively later stage of information processing. This discrepancy is consistent with the pattern found in studies with emotional stimuli [@pool_attentional_2016].

We expanded previous moral perception studies by focusing on the agent who made the perceptual decision-making and examined the interaction between moral valence and self-referential processing. Our results revealed that prioritization of good character is modulated by self-referential processing: the good character was prioritized when it was related to the “self”, even when the self-relatedness was task-irrelevant. By contrast, good character information was not prioritized when it was associated with “other”. The modulation effect of self-referential processing was large when the relationship between moral character and the self was explicit, which is consistent with previous studies that only positive aspects of the self are prioritized [@Hu_2020_GoodSelf]. More importantly, the effect persisted when the relationship between moral character and self-information was implicit, suggesting spontaneous self-referential processing when both pieces of information were presented. A possible explanation for this spontaneous self-referential of good character is that the positive moral self-view is central to our identity [@freitas_origins_2017; @strohminger_true_2017] and the motivation to maintain a moral self-view influences how we perceive [e.g., @Ma_JEPHPP_2010] and remember [e.g., @carlson_nat_comm_2020;@stanley_remembering_2019]. 

Although the results here revealed the prioritization of good character in perceptual decision-making, we did not claim that the motivation of a moral self-view *penetrates* perception. The perceptual decision-making process involves processes more than just encoding the sensory inputs. To fully account for the nuance of behavioral data and/or related data collected from other modules [e.g., @sui_electrophysiological_2023], we need computational models and an integrative experimental approach [@Almaatouq_BBS_2022]. For example, sequential sampling models suggest that, when making a perceptual decision, the agent continuously accumulates evidence until the amount of evidence passed a threshold, then a decision is made [@ratcliff_tics_2016; @forstmann_ann_rev_2016; @Hu_hitchhikers_2022]. In these models, the evidence, or decision variable, can accumulate from both sensory information but also memory [@shadlen_decision_2016]. Recently, applications of sequential sample models to perceptual matching tasks also suggest that different processes may contribute to the prioritization effect of self [@golubickis_self-prioritization_2017] or good self [@Hu_2020_GoodSelf]. Similarly, reinforcement learning models also revealed that the key difference between self- and other-referential learning lies in the learning rate [@lockwood_nat_comm_2018]. These studies suggest that computational models are needed to disentangle the cognitive processes underlying the prioritization of good character.

# References
```{r create_r-references, echo=FALSE,results='hide'}
#r_refs(file = "r-references.bib"))
```

\begingroup
\setlength{\parindent}{-0.5in}
\setlength{\leftskip}{0.5in}

<div id = "refs"></div>
\endgroup
