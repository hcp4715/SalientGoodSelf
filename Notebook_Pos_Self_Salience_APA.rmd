---
title             : "Self-referencing prioritizes moral character on perceptual matching"
shorttitle        : "Prioritization of good self"

author: 
  - name          : "Hu Chuan-Peng"
    affiliation   : "1, 2"
    corresponding : yes    # Define only one corresponding author
    address       : "School of Psychology, Nanjing Normal University, Ninghai Road 122, Gulou District, 210024 Nanjing, China"
    email         : "hcp4715@hotmail.com"
  - name          : "Kaiping Peng"
    affiliation   : "2"
  - name          : "Jie Sui"
    affiliation   : "3"

affiliation:
  - id            : "1"
    institution   : "Nanjing Normal University, 210024 Nanjing, China"
  - id            : "2"
    institution   : "Tsinghua University, 100084 Beijing, China"
  - id            : "3"
    institution   : "University of Aberdeen, Aberdeen, Scotland"

authornote: |
  Hu Chuan-Peng, School of Psychology, Nanjing Normal University, 210024 Nanjing, China.
  Kaiping Peng, Department of Psychology, Tsinghua University, 100084 Beijing, China.
  Jie Sui, School of Psychology, University of Aberdeen, Aberdeen, Scotland.
  Authors contriubtion: HCP, JS, & KP design the study, HCP collected the data, HCP analyzed the data and drafted the manuscript. All authors read and agreed upon the current version of the manuscripts.

abstract: |
 Evidence for the prioritization of moral information in cognitive processes is mixed. We examined this question using a series of eleven experiments where participants first learned associations between moral characters and geometric shapes and then performed simple speed tasks. In the first six experiments, we tested and validated prioritized responses to good characters over bad and neutral characters. To pin down the processes that are critical to the prioritization effects, in the remaining five experiments, we examined two opposing hypotheses: the valence hypothesis suggests that a general positivity bias towards all underpins the effects, while the self-binding account posits that self-referencing, rather than other-referencing is the fundamental driver of the effects. The data support the latter. Together, these results show a robust prioritization effect of good character through self-referencing processes, indicating the innate connection between morality and oneself and how humans use self-reference to explore the world and learn morality.
  
 <!-- https://tinyurl.com/ybremelq -->
  
keywords          : "Perceptual matching, self positivity bias, primacy of morality, Bayesian hierarchical models"
wordcount         : "X"

bibliography      : 
  - r-references.bib
  - endnote.bib

floatsintext      : yes
figurelist        : no
tablelist         : no
footnotelist      : no
linenumbers       : yes
mask              : no
draft             : no
figsintext        : no

documentclass     : "apa6"
classoption       : "man"
output            : 
  papaja::apa6_pdf:
    latex_engine  : xelatex

header-includes:
  - \usepackage{rotating}
  - \DeclareDelayedFloatFlavor{sidewaysfigure}{figure}
---
 <!-- This documents -->
 
```{r setup, include = FALSE}
# rm(list = ls())
source('Initial.r')

curDir = here::here()              # Get the current directory
figDir = here::here('figures')     # directory for figures.

# Seed for random number generation
set.seed(42)
options(tinytex.verbose = T) # debug the tex
knitr::opts_chunk$set(message = FALSE)
knitr::opts_chunk$set(cache.extra = knitr::rand_seed)
```
 <!-- What is the theoretic meaning of the series study? -->

<!--Alternative title: Self-relevance modulates the prioritization of the good character in perceptual matching -->

# Introduction

 <!--[sentences in bracket are key ideas] -->

<!--[quotes about moral character] -->

<!-- 
Ideas: 
little value in relying on people's self-reported moral principles or moral ideals to predict their real life behaviors, "paradox of morality" (Ellemers, 2019);
moral associative learning as an stronger predictor??
-->

<!--
Big problem in science, 
field domain
what field knows
remaining gap

Summary:
        our approach
        our results
-->

Morality is central to human life [@haidt_morality_2010]. Thus, gathering information about morality efficiently and accurately is crucial for individuals to navigate the social world [@brambilla_primacy_2021]. The importance of morality naturally leads to the hypothesis that morality-related information is prioritized in information processing, especially when attentional resources are limited. This hypothesis is plausible because a large volume of studies has reported that valuable stimuli are prioritized, e.g., threatening stimuli [e.g., @ohman_face_2001], rewards [@anderson_value_2011; @sui_more_2015], or self-related stimuli [@sui_self_attention_2019]. Consistent with this hypothesis, a few studies reported a prioritization effect of negative moral information in visual processing: negative moral trait words  [@fiske_attention_1980; @ybarra_young_2001; @gantman_moral_2014] and faces associated with bad behaviors [@anderson_visual_2011; @eiserbeck_visual_2020] attracted more attention and were responded faster. 

However, evidence for this negative moral bias effect is mixed. First, the opposite effect was also reported. For example, @shore_social_2013 found that faces with positive interaction in a trust game were prioritized in the pre-attentive process. Also, Abele and Bruckmueller found faster responses to moral words were not moderated by valence [@abele_bigger_2011]. Second, the robustness of the negative moral bias effect is questioned, a direct replication study failed to support the conclusion that faces associated with bad social behaviors dominate visual awareness [eg., @stein_no_2017]. Third, the prioritization effect of morality might be confounded with other factors, such as the priming effect [@firestone_cognition_2015; @firestone_moral_2016; @jussim_interpretations_2016] or differences between lexical characteristics [@larsen_lexical_2006]. As a result, while the importance of morality is widely recognized and there is initial evidence for a negative moral bias, whether moral information is prioritized in perceptual processing is still an open question. 

Here, we conducted a series of well-controlled experiments to examine the prioritization effect of morality and its potential mechanisms. To eliminate the priming effect and other potential confounding factors, we employed a task where participants first acquired moral meanings of geometric shapes during the instruction phase and then performed a simple perceptual matching task. The instruction-based associative learning task is based on the fact that humans can rapidly learn based on verbal instructions [e.g., @cole_nbr_2017]. This instruction-based associative learning task is widely used in aversive learning, value-based learning, and other tasks [@atlas_ann_rev_2023; @deltomme_instructed_2018; @cole_nbr_2017]. Unlike previous studies relies on faces or words [e.g., @bortolon_self-face_2018; @yaoi_does_2021], stimuli in the current study are geometric shapes, whose moral meanings were acquired right before the perceptual matching task. By counter-balancing associations between shapes and valence of moral characters across different participants, we controlled the effect of these shapes on the matching task. Also, in the matching task, we repeatedly present a few pairs of shapes and labels to participants, the results can not be explained by semantic priming [@unkelbach_chapter_2020], which is the center of the debate on previous results [@gantman_moral_2015; @gantman_see_2016; @firestone_cognition_2015; @Firestone_2016_BBS; @jussim_interpretations_2016]. Finally, we conducted a series of control experiments and established that moral content, rather than other factors such as familiarity of stimuli, drove the prioritization effects. 
 
To pin down the factors that are central to the prioritization effects, two competing hypotheses were examined. One is the valence-based account, suggesting that a general positivity bias towards all underpins the prioritization effects. In fact, the account has been applied to explain not only positivity biases but also negativity biases. For example, the negative bias toward moral information was explained by a threat detection mechanism which might be general for all negative information [e.g., @fiske_attention_1980]. The positive bias toward moral information, on the other hand, was explained by the positive valence of the stimuli because the stimuli imply potential benefits [@shore_social_2013]. However, these explanations often ignore the fact that valence is subjective *per se* [@juechems_where_2019]. That is, being related to a person is the premise of a stimulus or outcome being of value to the person. The subjective value is “a broader concept that refers to the personal significance or importance that a person assigns to a particular stimulus or outcome” and when the outcome is affective or emotional, researchers refer to it as “valence”, i.e., positive or negative [@carruthers_valence_2021]. The subjectivity of valence leads to an alternative explanation: self-binding account [@sui_tics_2015]. The self-binding account suggests that merely associating with the self can prioritize stimuli in perception, attention, working memory, and long-term memory [@sui_self_attention_2019; @sui_tics_2015], especially for positive information [@Hu_2020_goodme]. According to the self-binding account, the prioritization of good character is a result of spontaneous self-referencing.

To test the valence account and self-binding account in the prioritization effect of good character, we manipulated self-relevance and instructed participants on which moral character is self-referencing and which is not. We then tested whether the prioritization of moral character is by valence or by the associations between self-relevance and moral valence. The results revealed that the prioritization effect only occurred when shapes of good characters referred to the self of participants. We confirmed these results in the subsequent experiments, where shapes of good characters did not explicitly refer to the self or others but were merely presented with labels of the self or others. Together, these data revealed a mutual facilitation effect of good character and the self, suggesting a spontaneous self-referential process as a novel mechanism underlying the prioritization of good character in perceptual matching.

<!-- 
we attempted to distinguish these two possibilities by a social associative learning task in which physical features had minimal influences — participants performed a perceptual matching task after associated different moral characters (good, neutral, and bad) with different geometric shapes. If there is a positivity effect, there should be an advantage for shapes associated with good character over shapes associated with neutral or bad shapes. If there is a negativity effect, the advantage should be occur on shapes associated with bad characters. The first six experiments and two additional follow-up experiments provided strong evidence for good character effect in the current paradigm.

The positivity effect consistent with previous studies where positivity effect of social trait words were found [@anselmi_positive_2011; @bargh_generality_1992; @unkelbach_good_2010]. However, the effect could not be explained by the similarity hypothesis [@unkelbach_chapter_2020] because we only used three stimuli. There are two possibility explanations. The first one is the value-based attention account, which suggests that stimuli that are valuable to us are prioritized [@anderson_neurobiology_2019]. In our experiments, the good character label "good person" may represent an indirect but valuable stimuli because, in social life, a good other is usually more valuable than an bad other [@abele_agency_2007]. Another possibility is derived from social categorization theory, which suggested that we automatically categorize others as in-group or out-group [@turner_rediscovering_1987]. Moral character is an important criterion for social categorization [@mchugh_moral_2021; @descioli_side_taking_2016]. However, the above four experiments could not distinguish between these two possibilities, because "good person" could both be rewarding and be categorized as in-group member. Given that both rewarding stimuli [e.g., @Sui_2012_JEPHPP] and in-group information [@enock_overlap_2020] are prioritized when using social associative learning paradigm, we further tested these two possibilities in new experiments.

To distinguish the value-based account and the social categorization explanations, we introduced the identity (self- vs. other-) of moral character as an addition independent variable in exp 3a, 3b, and 6b. Now moral valence is orthogonal to the identity. In this case, the identity of moral character information become salient and participants are less likely to spontaneously categorize a good-other as an extension of self, but the value of good-person still exists. If the positivity effect was driven by social categorization theory, then participants prioritize good-self but not good-other. If the value-based attention theory is true, then, both good-self and good-other are prioritized, or maybe good-other are even more prioritized. 

Although the introduction of self- and other-referencing processing provided evident that value-based account can not explain the good-character effect, it might introduce the good-self effect, i.e., the good-self is prioritized over all the other stimuli. This effect, if true, may suggest underlying mechanisms other than social-categorization. For example, the moral true self account. Moral true self view suggested that moral self if the true self [@strohminger_true_2017]. Therefore, even good-self can be viewed as categorized to in-group, it can also be viewed as the core of the self and it is the anchor of all the other effects.

To test the moral true self view and the social-categorization account, we designed two complementary experiments. In experiment 4a, participants only learned the association between self and other, the words "good-person", "neutral person", and "bad person" were presented as task-irrelevant stimuli, while in experiment 4b, participants learned the associations between "good-person", "neutral-person", and "bad-person", and the "self" and "other" were presented as task-irrelevant stimuli. These two experiments can be used to distinguish the moral-self view and social categorization" account. If moral-self view is true, then, in both experiments, good-self will show advantage over all other stimuli, and there will be no other effects. More specifically, in experiment 4a, where only the self-referential processing is task-relevant, there will be advantage for good as task-irrelevant condition than when bad or neutral character as task-irrelevant for the self conditions, while there is no other effects; in experiment 4b, in the good condition, there will be an advantage for self as task-irrelevant condition over other as task-irrelevant condition, and no other effects. If social categorization is true, then, the prioritization effect will depends on whether the stimuli can be categorized as the same group of good-self. More specifically, in experiment 4a, there will be good effect in self conditions, this prediction is the same as the moral self-view; it predicts a reverse good effect in other condition because good and other a conflict in terms of social-categorization, this prediction is different from the "good-self" anchor account; however, for experiment 4b, it predicts no identity effect in the good-person condition because both self and other are in the good group.


[Good self in self-reported data] 
As an exploration, we also collected participants' self-reported psychological distance between self and good-person, bad-person, and neutral-person, moral identity, moral self-image, and self-esteem. All these data are available [see @Liu_2020_JOPD]. We explored the correlation between self-reported distance and these questionnaires as well as the questionnaires and behavioral data. However, given that the correlation between self-reported score and behavioral data has low correlation [@dang_why_2020], we didn't expect a high correlation between these self-reported measures and the behavioral data.
-->

 <!-- 
Key concepts and discussing points:

**Self-categories** are cognitive groupings of self and some class of stimuli as identical or different from some other class. [Turner et al.]

**Personal identity** refers to self-categories that define the individual as a unique person in terms of his or her individual differences from other (in-group) persons.

**Social identity** refers to the shared social categorical self ("us" vs. "them").

**Variable self**: Who we are, how we see ourselves, how we define our relations to others (indeed whether they are construed as ‘other’ or as part of the extended 'we' self) is different in different settings. 

**Identification**: the degree to which an individual feels connected to an ingroup or includes the ingroup in his or her self-concept. (self is not bad; )

Morality as a way for social-categorization [@mchugh_moral_2021]? People are more likely to identify themselves with trustworthy faces [@verosky_differential_2010] (trustworthy faces has longer RTs).

What is the relation between morally good and self in a semantic network (attractor network) (Freeman & Ambady, 2011)? The psychological essentialism account proposed that the moral good self is perspective independent, i.e., there is a moral good self in all. This perspective free effect is not exist in our effect.

How to deal with the *variable self* (self-categorization theory) vs. *core/true/authentic self* vs. *self-enhancement*

**Limitations**:
The perceptual decision-making will show certain pattern under certain task demand. In our case, it's the forced, speed, two-option choice task.

in experiment 4a and 4b, we didn't have a baseline condition where there is no word inside the shape? -->

# Disclosures
We reported all the measurements, analyses, and results in all the experiments in the current study. Participants whose overall accuracy was lower than 60% were excluded from analyses. Also, accurate responses with less than 200ms reaction times were excluded from the analysis. These excluded data can be found in the shared raw data files (see https://doi.org/10.5281/zenodo.8031086). This manuscript is prepared with r pakcage `papaja` [@Aust_2022_papaja].

All the experiments reported were not pre-registered. Most experiments (1a ~ 4b, except experiment 3b) reported in the current study were first finished between 2013 to 2016 at Tsinghua University, Beijing, China. Participants in these experiments were recruited from the local community. To increase the sample size of experiments to 50 or more [@Simmons_2013_life], we recruited additional participants from Wenzhou University, Wenzhou, China, in 2017 for experiments 1a, 1b, 4a, and 4b. Experiment 3b was finished at Wenzhou University in 2017 (See Table 1 for an overview of these experiments). 

All participants received informed consent and were compensated for their time. These experiments were approved by the ethics board in the Department of Psychology, Tsinghua University. 

 <!-- 21-word solution (Simmons, Nelson & Simonsohn, 2012; retrieved from http://ssrn.com/abstract=2160588) -->

```{r loadingData,echo=FALSE,results='hide'}
load("Data4manu.RData")

### exclude the repeated subj from the raw data

# No repeating subj
df1a.v_meta <- df1a.v

# No repeating subj
df1b.v_meta <- df1b.v

# exclude participant from exp 1a
df1c.v_meta <- df1c.v %>% dplyr::filter(!Subject %in% c(1206, 1207, 1208, 1210))

# exclude participant from exp 1a
df2.v_meta <- df2.v %>% dplyr::filter(Subject > 2000)    

# exclude participants from ex1b, 1c, and 2
df3a.v_meta <- df3a.v %>% dplyr::filter(!Subject %in% c(3013, 3012, 3043, 3046)) 

# No repeating subj
df3b.v_meta <- df3b.v

# No repeating subj
df4a.v_meta <- df4a.v

# exclude participants from ex1b, 1c, and 2
df4b.v_meta <- df4b.v %>% dplyr::filter(!Subject %in% c(4210, 4202, 4201))   

# exclude participants from ex1b, 1c, and 2 and only use the data from morality
df5.v_meta <- df5.v %>% dplyr::filter(!Subject %in% c(5201))  %>% dplyr::filter(taskType == "Morality") 

# exclude participants from ex1b, 1c, and 2
df6a.v_meta <- df6a.v %>% dplyr::filter(!Subject %in% c(6118,6119,6122,6123,6131))   

# exclude participants from ex1b, 1c, and 2
df6b.v_meta <- df6b_d1.v %>% dplyr::filter(!Subject %in% c(6217))   
# Note: subject 6129 finished 10 blocks, making the number of trials per condition to 200

# # exclude participants from ex1b, 1c, and 2
# df7a.v_meta <- df7a_m.v %>% dplyr::filter(!Subject %in% c(7020))   
# 
# # No repeating subj
# df7b.v_meta <- df7b_m.v

df1a.v_meta$ExpID <- 'Exp1a'
df1b.v_meta$ExpID <- 'Exp1b'
df1c.v_meta$ExpID <- 'Exp1c'
df2.v_meta$ExpID  <- 'Exp2'
df3a.v_meta$ExpID <- 'Exp3a'
df3b.v_meta$ExpID <- 'Exp3b'
df4a.v_meta$ExpID <- 'Exp4a'
df4b.v_meta$ExpID <- 'Exp4b'
df5.v_meta$ExpID  <- 'Exp5'
df6a.v_meta$ExpID <- 'Exp6a'
df6b.v_meta$ExpID <- 'Exp6b'
```

  <!-- A general method part describing experimental design and data analysis -->
```{r child = "general_method.rmd"}
```

# Results
## Prioritization of good character
To test whether moral characters are prioritized, we modeled data from Experiments 1a, 1b, 1c, 2, 5, and 6a with three-level Bayesian hierarchical models. All these experiments shared similar designs, with a total sample size of 192. Note that for both experiments 1a and 1b, two datasets were collected at different time points and locations, thus we treated them as independent samples. Here we only reported the population-level results of three-level Bayesian models, the results of each experiment can be found in supplementary materials.

```{r prepare data for first meta, echo=FALSE, results='hide', warning=FALSE}
### try meta-analysis 1a, 1b, 1c, 2, 5 and 6a
selected_columns <- c('ExpID', 'Site', 'Subject','Age', 'Sex', 'Matchness','Valence', 'RESP', 'ACC','RT')
df_moral <- dplyr::bind_rows(df1a.v_meta[selected_columns],
                             df1b.v_meta[selected_columns],
                             df1c.v_meta[selected_columns],
                             df2.v_meta[selected_columns],
                             df5.v_meta[selected_columns],
                             df6a.v_meta[selected_columns]) %>%
  dplyr::mutate(ExpID_new = paste(ExpID, Site, sep = "_")) %>%
  dplyr::mutate(Valence = factor(Valence, levels = c('Neutral', 'Bad', 'Good')))

df_moral_subj <- df_moral %>%
  dplyr::group_by(ExpID_new, Site) %>%
  dplyr::summarize(N = n_distinct(Subject),
                   N_trial = n(),
                   Exp_conds = 6,
                   trial_per_cond = round((length(Subject)/6)/N, 0))

df_moral <- df_moral %>%
        dplyr::filter(!is.na(RESP)) %>%                 # filter trials without response
        dplyr::filter(!(RT <= 200 & ACC == 1))  %>%     # filter trials with RT <= 200 ms
        dplyr::mutate(ismatch = ifelse(Matchness == 'Match', 1, 0),
                      saymatch = ifelse((Matchness == 'Match' & ACC == 1) |
                                                (Matchness == 'Mismatch' & ACC == 0), 1, 0)) %>%
        dplyr::select(ExpID_new, Subject, Valence, Matchness, RESP, ACC, RT, ismatch, saymatch) %>%
        dplyr::mutate(ismatch_num = ifelse(Matchness == 'Match', 0.5, -0.5))

# # plot the nested structure of the data
# with(df_moral, table(Subject, ExpID_new)) %>%
#   image(
#     col = grey.colors(80, start = 1, end = 0),
#     axes = TRUE,
#     xlab = "Subject",
#     ylab = "ExpID"
#   )
```

```{r first meta sdt, echo=FALSE, results='hide', warning=FALSE}
# fit a three-level hierarchical model for SDT, didn't specify the prior; dummy coding
# about 20 hours to finish this sampling using ntel® Xeon(R) CPU E3-1505M v5 @ 2.80GHz × 8 machine.
# 87432.5 = 24.3 hours
sdt_val_m1 <- brms::brm(saymatch ~ 0 + Valence + Valence:ismatch + 
                                (0 + Valence + Valence:ismatch | ExpID_new) + 
                                (0 + Valence + Valence:ismatch  | ExpID_new:Subject),
                        family = bernoulli(link="probit"), 
                        data = df_moral,
                        chains = 4,
                        iter = 4000,
                        thin = 2,
                        control = list(adapt_delta = .95),
                        cores = parallel::detectCores(),
                        backend = 'cmdstanr',  # with cmdstanr
                        file = here::here("glmmModels/sdt_val_DummyCode_3_level"))

df_m1_std_fixed_effect <- bayestestR::describe_posterior(
        sdt_val_m1,
        effects = "fixed",
        component = "all",
        ci = 0.95,
        ci_method = 'hdi',
        test = c("p_direction", "p_significance"),
        centrality = "all") %>%
        dplyr::mutate(Valence = dplyr::case_when(
                grepl("ValenceBad", Parameter) ~ "Bad",
                grepl("ValenceNeutral", Parameter) ~ "Neutral",
                grepl("ValenceGood", Parameter) ~ "Good"),
                params = dplyr::case_when(grepl("ismatch", Parameter) ~ "d prime",
                                           !grepl("ismatch", Parameter) ~"criterion"),
                params = factor(params, levels = c('d prime', 'criterion')),
                Valence = factor(Valence, levels = c('Good', 'Neutral',  'Bad'))) 

df_m1_post_sdt_exp <- sdt_val_m1 %>% 
        tidybayes::spread_draws(r_ExpID_new[condition, term]) %>%
        dplyr::rename(value = r_ExpID_new)

pop_mean <- sdt_val_m1 %>%
        tidybayes::gather_draws(`b_.*`, regex = TRUE) %>%
        group_by(.variable) %>%       # this line not necessary (done automatically by spread_draws)
        tidybayes::mean_hdci(.value)  # get the high density continuous intervals

# extract the population level parameters
df_sdt_m1_pop <- sdt_val_m1 %>% 
        tidybayes::gather_draws(`b_.*`, regex = TRUE) %>%
        dplyr::rename(term = .variable,
                      pop_mean = .value) %>%
        dplyr::ungroup() %>%
        dplyr::mutate(term = gsub("b_", "", term))

# Add the population mean to the experimental level and only keep the value after.
df_m1_post_sdt_exp_update <- merge(df_sdt_m1_pop, df_m1_post_sdt_exp, by = c('term','.chain','.iteration', '.draw'), all = T) %>%
        dplyr::mutate(mean_value = pop_mean + value) %>%  # add the population level value to each experiment
        dplyr::select(condition, term, `.chain`, `.iteration`, `.draw`, mean_value) %>% # select the added value
        dplyr::rename(value = mean_value)  # rename the value

# Plot the difference between conditions 
df_m1_plot_sdt <- df_sdt_m1_pop %>%
        dplyr::mutate(condition = 'Overall') %>%
        dplyr::rename(value = pop_mean) %>%
        dplyr::select(condition, term, `.chain`, `.iteration`, `.draw`, value) %>%
        dplyr::bind_rows(., df_m1_post_sdt_exp_update) %>%
        dplyr::mutate(condition = factor(condition, levels = c("Exp1a_THU", "Exp1a_WZU", "Exp1b_THU", 
                                                               "Exp1b_WZU", "Exp1c_THU", "Exp2_THU" , 
                                                               "Exp5_THU",  "Exp6a_THU","Overall")),
                      condition = forcats::fct_rev(condition), # reverse the order because the plot function auto reverse.
                      term = dplyr::case_when((term == "ValenceBad") ~ "c_Bad",
                                              (term == "ValenceNeutral") ~ "c_Neutral",
                                              (term == "ValenceGood") ~ "c_Good",
                                              (term == "ValenceBad:ismatch") ~ "dprime_Bad",
                                              (term == "ValenceNeutral:ismatch") ~ "dprime_Neutral",
                                              (term == "ValenceGood:ismatch") ~ "dprime_Good"),
                      term = factor(term, levels = c("c_Bad", "c_Neutral", "c_Good",
                                                     "dprime_Bad", "dprime_Neutral", "dprime_Good"))) 

df_m1_plot_sdt_diff_wide <- df_m1_plot_sdt %>%
  tidyr::pivot_wider(names_from = c(term), values_from = value) %>%   # long to wide
  dplyr::mutate(diff_GB_c = c_Good - c_Bad,                           # calculate the differences between conditions
                diff_GN_c = c_Good - c_Neutral,
                diff_BN_c = c_Bad - c_Neutral,
                diff_GB_dprm = dprime_Good - dprime_Bad,
                diff_GN_dprm = dprime_Good - dprime_Neutral,
                diff_BN_dprm = dprime_Bad - dprime_Neutral) %>%
  dplyr::select(condition, `.chain`, `.iteration`, `.draw`,
               diff_GB_c,diff_GN_c, diff_BN_c,
               diff_GB_dprm, diff_GN_dprm, diff_BN_dprm)

df_m1_plot_sdt_diff <- df_m1_plot_sdt_diff_wide%>%
  tidyr::pivot_longer(cols = diff_GB_c:diff_BN_dprm, names_to = "term_diff", values_to =  "value") %>%  # wide to long
  dplyr::mutate(term_diff = factor(term_diff, levels = c('diff_GB_c','diff_GN_c', 'diff_BN_c',
                                                         'diff_GB_dprm', 'diff_GN_dprm', 'diff_BN_dprm')))

# plot the posterior of the d prime
# use the overall mean values as the vlines
vlines_df_m1_sdt <- df_m1_plot_sdt %>% 
        tidyr::separate(term, c('params', 'Valence')) %>% 
        dplyr::mutate(Valence = factor(Valence, levels = c('Good', 'Neutral', 'Bad'))) %>%
        dplyr::filter(params == 'dprime') %>%
        dplyr::filter(condition == "Overall") %>%
        dplyr::group_by(Valence) %>% 
        dplyr::summarize(Mean = mean(value)) 

# THIS is the one which the final plot will based on!!!
p_dprime1 <- df_m1_plot_sdt %>%
        tidyr::separate(term, c('params', 'Valence')) %>%
        dplyr::mutate(Valence = factor(Valence, levels = c('Good', 'Neutral', 'Bad'))) %>%
        dplyr::filter(params == 'dprime') %>%
        dplyr::rename(Experiments = condition) %>%
        ggplot2::ggplot(aes(y = Experiments, x = value, color = Valence)) +
        tidybayes::stat_halfeye(aes(fill = Valence), alpha = 0.7) +
        geom_vline(data = vlines_df_m1_sdt, aes(xintercept = Mean,colour = Valence), linetype = "dashed") +
        labs(x=expression('Posteior distribution of '~italic(d)~' prime')) + 
        scale_colour_brewer(palette = "Dark2") +
        scale_fill_brewer(palette = "Dark2") +
        theme_apa(base_size = 20)

# define facet titles
contrast_names_m_sdt <- c(`diff_GN_dprm` = "Good vs Neutral", 
                           `diff_BN_dprm` = "Bad vs Neutral"
                           )
# plot the posterior of the difference between d prime
p_dprime1_diff <- df_m1_plot_sdt_diff %>%
        dplyr::filter(str_detect(term_diff, '_dprm')) %>%
        dplyr::filter(!str_detect(term_diff, '_GB_')) %>%
        dplyr::rename(Experiments = condition) %>%
        ggplot2::ggplot(aes(y = Experiments, x = value, fill = after_stat(x > 0))) +
        tidybayes::stat_halfeye() +
        geom_vline(xintercept = 0, linetype = "dashed") +
        # scale_fill_manual(values = c('gray80', 'skyblue')) +
        scale_fill_manual(values = c('skyblue', 'gray80'),
                    breaks = c(TRUE, FALSE),
                    name = "Effect" , labels = c("Yes", "No")) + 
        xlab(expression(paste("Effect of valence on ", italic("d"), " prime", sep = ' '))) + 
        facet_wrap( ~ term_diff, # scales = "free_y",
                    nrow = 1,
                    labeller = as_labeller(contrast_names_m_sdt)) + # label_parsed
        theme_apa(base_size = 20) + 
        theme(strip.text.x = element_text(size = 8)) # colour = "orange", angle = 90))

### get results that will be reported in the text
med_d_m1_good <- df_m1_std_fixed_effect$Median[df_m1_std_fixed_effect$Valence == 'Good' & df_m1_std_fixed_effect$params == 'd prime']
ll_d_m1_good <- df_m1_std_fixed_effect$CI_low[df_m1_std_fixed_effect$Valence == 'Good' & df_m1_std_fixed_effect$params == 'd prime']
ul_d_m1_good <- df_m1_std_fixed_effect$CI_high[df_m1_std_fixed_effect$Valence == 'Good' & df_m1_std_fixed_effect$params == 'd prime']

med_d_m1_neut <- df_m1_std_fixed_effect$Median[df_m1_std_fixed_effect$Valence == 'Neutral' & df_m1_std_fixed_effect$params == 'd prime']
ll_d_m1_neut <- df_m1_std_fixed_effect$CI_low[df_m1_std_fixed_effect$Valence == 'Neutral' & df_m1_std_fixed_effect$params == 'd prime']
ul_d_m1_neut <- df_m1_std_fixed_effect$CI_high[df_m1_std_fixed_effect$Valence == 'Neutral' & df_m1_std_fixed_effect$params == 'd prime']

med_d_m1_bad <- df_m1_std_fixed_effect$Median[df_m1_std_fixed_effect$Valence == 'Bad' & df_m1_std_fixed_effect$params == 'd prime']
ll_d_m1_bad <- df_m1_std_fixed_effect$CI_low[df_m1_std_fixed_effect$Valence == 'Bad' & df_m1_std_fixed_effect$params == 'd prime']
ul_d_m1_bad <- df_m1_std_fixed_effect$CI_high[df_m1_std_fixed_effect$Valence == 'Bad' & df_m1_std_fixed_effect$params == 'd prime']

bayestestR::sexit(df_m1_plot_sdt_diff_wide[, 9:10])
```

For the *d* prime, results from the Bayesian model revealed a robust effect of moral character. Shapes associated with good characters ("good person", "kind person" or a name associated with good behaviors) have higher sensitivity (median = `r med_d_m1_good`, 95% HDI = [`r ll_d_m1_good` `r ul_d_m1_good`]) than shapes associated with neutral characters (median = `r med_d_m1_neut`, 95% HDI = [`r ll_d_m1_neut` `r ul_d_m1_neut`]), the difference ($median_{diff}$ = 0.31, 95% HDI [0, 0.62]) has a 97.31% probability of being positive (> 0), 94.91% of being significant (> 0.05). But we did not find a difference between shapes associated with bad characters (median = `r med_d_m1_bad`, 95% HDI = [`r ll_d_m1_bad` `r ul_d_m1_bad`]) and neutral character, the difference ($median_{diff}$ = 0.05, 95% HDI  [-0.27, 0.38]) only has a 60.56% probability of being positive (> 0), 49.34% of being significant (> 0.05).

```{r first meta rt, echo=FALSE, results='hide', warning=FALSE}
# have a look at a few participants' data
# set.seed(123)

# fit a three-level hierarchical model for RT, didn't specify the prior, lognormal, effective coding
RT_val_m1 <- df_moral %>%
        dplyr::mutate(RT_sec = RT/1000) %>% # log RT in seconds
        dplyr::filter(ACC == 1) %>%         # only correct trials
        brms::brm(RT_sec ~ Valence*ismatch_num + 
                          (Valence*ismatch_num | ExpID_new) + 
                          (Valence*ismatch_num | ExpID_new:Subject),
                  family=lognormal(),
                  data = .,
                  chains = 4,
                  iter = 4000,
                  thin = 2,
                  control = list(adapt_delta = .95),
                  cores = parallel::detectCores(),
                  backend = 'cmdstanr',  # with cmdstanr
                  file = here::here("glmmModels/RT_val_EffectCode_3_level"))

#Population-Level Effects: 
#                        Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS
#Intercept                  -0.40      0.06    -0.52    -0.27 1.01      837     1301  # baseline: mismatch:neutral
#ValenceBad                  0. 01      0.00     0.00     0.02 1.00     1752     2540  # mismatch:bad - mismatch:neutral = 0.01
#ValenceGood                -0.03      0.00    -0.04    -0.02 1.00     1237     2219  # mismatch:Good - mismatch:neutral = -0.03
#ismatch_num                -0.07      0.01    -0.09    -0.06 1.00     1638     1957  # match:neutral - mismatch:neutral = -0.07
#ValenceBad:ismatch_num      0.02      0.01     0.00     0.04 1.00     1597     2380  # match:bad - ValenceBad -ismatch_num = 0.02
#ValenceGood:ismatch_num    -0.05      0.01    -0.07    -0.03 1.00     1424     1775  # match:good - ValenceGood- ismatch_num = -0.05

# Mismatch:Neutral - Intercept = -0.4
# Mismatch:Bad     - Intercept  + ValenceBad = -0.4 + 0.01 = -0.39
# Mismatch:Good    - Intercept  + ValenceGood = -0.4 - 0.03 = -0.43
# Match: Neutral   - Intercept  + ismatch_num = -0.4 - 0.07 = -0.47
# Match: Bad       - Intercept  + ismatch_num + ValenceBad+ ValenceBad:ismatch_num = -0.4 + 0.01 + 0.02 =  -0.37 
# Match: Good      - Intercept  + ismatch_num + ValenceGood+ ValenceGood:ismatch_num = -0.4 + (-0.03) + (-0.05) = -0.48

# Get the variables in the model 1
# RT_var_name_m1 <- tidybayes::get_variables(RT_val_m1)

df_m1_post_rt_exp <- RT_val_m1 %>% 
        tidybayes::spread_draws(r_ExpID_new[condition, term]) %>%
        dplyr::rename(value = r_ExpID_new)

df_rt_m1_pop_mean <- RT_val_m1 %>%
        tidybayes::gather_draws(`b_.*`, regex = TRUE) %>%
        group_by(.variable) %>%       # this line not necessary (done automatically by spread_draws)
        tidybayes::mean_hdci(.value)  # get the high density continuous intervals

# extract the population level parameters
df_rt_m1_pop <- RT_val_m1 %>% 
        tidybayes::gather_draws(`b_.*`, regex = TRUE) %>%
        dplyr::rename(term = .variable,
                      pop_mean = .value) %>%
        #tidyr::separate(term, c(NA, 'term'), "_") 
        dplyr::ungroup() %>%
        dplyr::mutate(term = gsub("b_", "", term))

# Add the population mean to the experimental level and only keep the value after.
df_m1_post_rt_exp_update <- merge(df_rt_m1_pop, df_m1_post_rt_exp, 
                                  by = c('term','.chain','.iteration', '.draw'), all = T) %>%
        dplyr::mutate(mean_value = pop_mean + value) %>%  # add the population leve value to each experiment
        dplyr::select(condition, term, `.chain`, `.iteration`, `.draw`, mean_value) %>% # select the added value
        dplyr::rename(value = mean_value)  # rename the value

# Plot the difference between conditions 
df_m1_plot_rt <- df_rt_m1_pop %>%
        dplyr::mutate(condition = 'Overall') %>%
        dplyr::rename(value = pop_mean) %>% # chagne the `pop_mean` as `value` for data frame merge
        dplyr::select(condition, term, `.chain`, `.iteration`, `.draw`, value) %>%
        dplyr::bind_rows(., df_m1_post_rt_exp_update) %>%
        dplyr::mutate(condition = factor(condition, levels = c("Exp1a_THU", "Exp1a_WZU", "Exp1b_THU", 
                                                         "Exp1b_WZU", "Exp1c_THU", "Exp2_THU" , 
                                                         "Exp5_THU",  "Exp6a_THU","Overall")),
                      condition = forcats::fct_rev(condition)) %>%
        tidyr::pivot_wider(names_from = c(term), values_from = value) %>%   # long to wide
        dplyr::mutate(Neutral_NM = Intercept,               # calculate the differences between coditions
                Bad_NM = Intercept  + ValenceBad,
                Good_NM = Intercept  + ValenceGood ,
                Neutral_M = Intercept  + ismatch_num,
                Bad_M = Intercept  + ismatch_num + ValenceBad + `ValenceBad:ismatch_num`,
                Good_M = Intercept  + ismatch_num + ValenceGood+ `ValenceGood:ismatch_num`) %>%
        dplyr::select(condition, `.chain`, `.iteration`, `.draw`,
                      Neutral_NM, Bad_NM, Good_NM,
                      Neutral_M, Bad_M, Good_M) %>%
        tidyr::pivot_longer(cols = Neutral_NM:Good_M, names_to = "term", values_to =  "value") %>%  # wide to long
        dplyr::mutate(term = factor(term, levels = c('Good_NM', 'Neutral_NM', 'Bad_NM',
                                                     'Good_M',  'Neutral_M',  'Bad_M')),
                      value = exp(value),
                      value = value * 1000) 

# plot the posterior of the d prime
# use the overall mean values as the vlines
vlines <- df_m1_plot_rt %>% 
        tidyr::separate(term, c('Valence', 'Match')) %>% 
        dplyr::mutate(Valence = factor(Valence, levels = c('Good', 'Neutral', 'Bad'))) %>%
        dplyr::filter(Match == 'M') %>%
        dplyr::filter(condition == "Overall") %>%
        dplyr::group_by(Valence) %>% 
        dplyr::summarize(Mean = mean(value)) # %>%
        # dplyr::arrange(Mean)

df_m1_rt_fixed_effect <- df_m1_plot_rt %>%
        dplyr::filter(condition == "Overall") %>%
        tidyr::pivot_wider(.,
                           id_cols = c('.chain', '.iteration', '.draw'),
                           names_from = term ) %>% 
        dplyr::select(-c('.chain', '.iteration', '.draw')) %>%
        bayestestR::describe_posterior(.,
                                       ci = 0.95,
                                       ci_method = 'hdi',
                                       test = c("p_direction", "p_significance"),
                                       centrality = "median") %>%
        tidyr::separate(Parameter, c('Valence', 'Match')) %>% 
        dplyr::mutate(Valence = factor(Valence, levels = c('Good', 'Neutral', 'Bad')))

# THIS is the one which the final plot will based on!!!
p_rt1 <- df_m1_plot_rt %>%
        tidyr::separate(term, c('Valence', 'Match')) %>% 
        dplyr::mutate(Valence = factor(Valence, levels = c('Good', 'Neutral', 'Bad'))) %>%
        dplyr::filter(Match == 'M') %>%
        dplyr::rename(Experiments = condition) %>%
        ggplot2::ggplot(aes(y = Experiments, x = value, color = Valence)) +
        tidybayes::stat_halfeye(aes(fill = Valence), alpha = 0.7) +
        geom_vline(data = vlines, aes(xintercept = Mean,colour = Valence), linetype = "dashed") +
        labs(x=expression('Posteior distribution of reaction times')) + 
        scale_colour_brewer(palette = "Dark2") +
        scale_fill_brewer(palette = "Dark2") +
        theme_apa(base_size = 20)

df_m1_plot_rt_diff_wide <- df_m1_plot_rt %>%
        tidyr::pivot_wider(names_from = c(term), values_from = value) %>%   # long to wide
        dplyr::mutate(diff_GB_NM = Good_NM - Bad_NM,               # calculate the differences between conditions
                diff_GN_NM = Good_NM - Neutral_NM,
                diff_BN_NM = Bad_NM - Neutral_NM,
                diff_GB_M = Good_M - Bad_M, 
                diff_GN_M = Good_M - Neutral_M,
                diff_BN_M = Bad_M - Neutral_M,) %>%
        dplyr::select(condition, `.chain`, `.iteration`, `.draw`,
               diff_GB_NM,diff_GN_NM, diff_BN_NM,
               diff_GB_M, diff_GN_M, diff_BN_M) 

df_m1_plot_rt_diff <- df_m1_plot_rt_diff_wide%>%
        tidyr::pivot_longer(cols = diff_GB_NM:diff_BN_M, names_to = "term_diff", values_to =  "value") %>%  # wide to long
        dplyr::mutate(term_diff = factor(term_diff, levels = c('diff_GB_NM','diff_GN_NM', 'diff_BN_NM',
                                                         'diff_GB_M', 'diff_GN_M', 'diff_BN_M')))

# plot the posterior of matching trials
# define face titles
contrast_names_m_rt <- c(`diff_GN_M` = "Good vs Neutral", 
                         `diff_BN_M` = "Bad vs Neutral"
                           )

p_rt1_diff <- df_m1_plot_rt_diff %>%
        dplyr::filter(str_detect(term_diff, '_M')) %>%
        dplyr::filter(!str_detect(term_diff, '_GB_')) %>%
        dplyr::rename(Experiments = condition) %>%
        ggplot2::ggplot(aes(y = Experiments, x = value, fill = stat(x < 0))) +
        tidybayes::stat_halfeye() +
        geom_vline(xintercept = 0, linetype = "dashed") +
        # scale_fill_manual(values = c('gray80', 'skyblue')) +
        scale_fill_manual(values = c('skyblue', 'gray80'),
                    breaks = c(TRUE, FALSE),
                    name = "Effect" , labels = c("Yes", "No")) + 
        xlab("Effect of valence on RT (Match trials)") +
        facet_wrap( ~ term_diff,
              # scales = "free_y", 
              nrow = 1,
              labeller = as_labeller(contrast_names_m_rt))  + # label_parsed
        theme_apa(base_size = 20) + 
        theme(strip.text.x = element_text(size = 8)) # colour = "orange", angle = 90))

df_rt1_diff_hdi <- df_m1_plot_rt_diff %>%
        # dplyr::filter(str_detect(term_diff, '_M')) %>%
        dplyr::filter(condition == "Overall") %>%
        tidyr::pivot_wider(.,
                           id_cols = c('.chain', '.iteration', '.draw'),
                           names_from = term_diff) %>%
        dplyr::select(-c('.chain', '.iteration', '.draw')) %>%
        bayestestR::describe_posterior(.,
                                       ci = 0.95,
                                       ci_method = 'hdi',
                                       test = c("p_direction", "p_significance"),
                                       centrality = "median")

### get results that will be reported in the text
## match trials
med_rt_m_m1_good <- round(df_m1_rt_fixed_effect$Median[df_m1_rt_fixed_effect$Valence == 'Good' & df_m1_rt_fixed_effect$Match == 'M'], digits = 0)
ll_rt_m_m1_good <- round(df_m1_rt_fixed_effect$CI_low[df_m1_std_fixed_effect$Valence == 'Good' & df_m1_rt_fixed_effect$Match == 'M'], digits = 0)
ul_rt_m_m1_good <- round(df_m1_rt_fixed_effect$CI_high[df_m1_rt_fixed_effect$Valence == 'Good' & df_m1_rt_fixed_effect$Match == 'M'], digits = 0)

med_rt_m_m1_neut <- round(df_m1_rt_fixed_effect$Median[df_m1_rt_fixed_effect$Valence == 'Neutral' & df_m1_rt_fixed_effect$Match == 'M'], digits = 0)
ll_rt_m_m1_neut <- round(df_m1_rt_fixed_effect$CI_low[df_m1_std_fixed_effect$Valence == 'Neutral' & df_m1_rt_fixed_effect$Match == 'M'], digits = 0)
ul_rt_m_m1_neut <- round(df_m1_rt_fixed_effect$CI_high[df_m1_rt_fixed_effect$Valence == 'Neutral' & df_m1_rt_fixed_effect$Match == 'M'], digits = 0)

med_rt_m_m1_bad <- round(df_m1_rt_fixed_effect$Median[df_m1_rt_fixed_effect$Valence == 'Bad' & df_m1_rt_fixed_effect$Match == 'M'], digits = 0)
ll_rt_m_m1_bad <- round(df_m1_rt_fixed_effect$CI_low[df_m1_std_fixed_effect$Valence == 'Bad' & df_m1_rt_fixed_effect$Match == 'M'], digits = 0)
ul_rt_m_m1_bad <- round(df_m1_rt_fixed_effect$CI_high[df_m1_rt_fixed_effect$Valence == 'Bad' & df_m1_rt_fixed_effect$Match == 'M'], digits = 0)

med_diff_rt_m_m1_GN <- round(df_rt1_diff_hdi$Median[df_rt1_diff_hdi$Parameter == 'diff_GN_M'], digits = 0)
ll_diff_rt_m_m1_GN <- round(df_rt1_diff_hdi$CI_low[df_rt1_diff_hdi$Parameter == 'diff_GN_M'], digits = 0)
ul_diff_rt_m_m1_GN <- round(df_rt1_diff_hdi$CI_high[df_rt1_diff_hdi$Parameter == 'diff_GN_M'], digits = 0)

med_diff_rt_m_m1_BN <- round(df_rt1_diff_hdi$Median[df_rt1_diff_hdi$Parameter == 'diff_BN_M'], digits = 0)
ll_diff_rt_m_m1_BN <- round(df_rt1_diff_hdi$CI_low[df_rt1_diff_hdi$Parameter == 'diff_BN_M'], digits = 0)
ul_diff_rt_m_m1_BN <- round(df_rt1_diff_hdi$CI_high[df_rt1_diff_hdi$Parameter == 'diff_BN_M'], digits = 0)

## mismatch trials
med_rt_nm_m1_good <- round(df_m1_rt_fixed_effect$Median[df_m1_rt_fixed_effect$Valence == 'Good' & df_m1_rt_fixed_effect$Match == 'NM'], digits = 0)
ll_rt_nm_m1_good <- round(df_m1_rt_fixed_effect$CI_low[df_m1_std_fixed_effect$Valence == 'Good' & df_m1_rt_fixed_effect$Match == 'NM'], digits = 0)
ul_rt_nm_m1_good <- round(df_m1_rt_fixed_effect$CI_high[df_m1_rt_fixed_effect$Valence == 'Good' & df_m1_rt_fixed_effect$Match == 'NM'], digits = 0)

med_rt_nm_m1_neut <- round(df_m1_rt_fixed_effect$Median[df_m1_rt_fixed_effect$Valence == 'Neutral' & df_m1_rt_fixed_effect$Match == 'NM'], digits = 0)
ll_rt_nm_m1_neut <- round(df_m1_rt_fixed_effect$CI_low[df_m1_std_fixed_effect$Valence == 'Neutral' & df_m1_rt_fixed_effect$Match == 'NM'], digits = 0)
ul_rt_nm_m1_neut <- round(df_m1_rt_fixed_effect$CI_high[df_m1_rt_fixed_effect$Valence == 'Neutral' & df_m1_rt_fixed_effect$Match == 'NM'], digits = 0)

med_rt_nm_m1_bad <- round(df_m1_rt_fixed_effect$Median[df_m1_rt_fixed_effect$Valence == 'Bad' & df_m1_rt_fixed_effect$Match == 'NM'], digits = 0)
ll_rt_nm_m1_bad <- round(df_m1_rt_fixed_effect$CI_low[df_m1_std_fixed_effect$Valence == 'Bad' & df_m1_rt_fixed_effect$Match == 'NM'], digits = 0)
ul_rt_nm_m1_bad <- round(df_m1_rt_fixed_effect$CI_high[df_m1_rt_fixed_effect$Valence == 'Bad' & df_m1_rt_fixed_effect$Match == 'NM'], digits = 0)

med_diff_rt_nm_m1_GN <- round(df_rt1_diff_hdi$Median[df_rt1_diff_hdi$Parameter == 'diff_GN_NM'], digits = 0)
ll_diff_rt_nm_m1_GN <- round(df_rt1_diff_hdi$CI_low[df_rt1_diff_hdi$Parameter == 'diff_GN_NM'], digits = 0)
ul_diff_rt_nm_m1_GN <- round(df_rt1_diff_hdi$CI_high[df_rt1_diff_hdi$Parameter == 'diff_GN_NM'], digits = 0)

med_diff_rt_nm_m1_BN <- round(df_rt1_diff_hdi$Median[df_rt1_diff_hdi$Parameter == 'diff_BN_NM'], digits = 0)
ll_diff_rt_nm_m1_BN <- round(df_rt1_diff_hdi$CI_low[df_rt1_diff_hdi$Parameter == 'diff_BN_NM'], digits = 0)
ul_diff_rt_nm_m1_BN <- round(df_rt1_diff_hdi$CI_high[df_rt1_diff_hdi$Parameter == 'diff_BN_NM'], digits = 0)

bayestestR::sexit(df_m1_plot_rt_diff_wide[,c(6:7,9:10)])
```

  <!-- plot all graphs form the first part together -->
(ref:fig1-caption) Effect of moral character on perceptual matching. (A) Experimental level (six experiments, with eight independent samples) and population level posterior distributions of RT under different matching conditions; (B) Experimental level and population level posterior distributions of *d*-prime under different conditions; (C) Experimental level and population level posterior distributions of the RT differences between conditions (left, Good vs. Neutral; right, Bad vs. Neutral); (D) Experimental level and population level posterior distributions of the *d*-prime differences between conditions (left, Good vs. Neutral; right, Bad vs. Neutral).

```{r plot-bayes-meta-1, fig.cap="(ref:fig1-caption)", fig.height=15, fig.width=18, warning=FALSE}
library(patchwork)
p_rt1 + p_dprime1 +
        p_rt1_diff + p_dprime1_diff + plot_annotation(tag_levels = 'A')  + plot_layout(nrow = 2, byrow = TRUE, guides = 'collect')

# p <- p_rt1 + p_dprime1 + plot_annotation(tag_levels = 'A') + plot_layout(guides = "collect") 

#  ggsave('part1_plot_posterior.png', p, width = 15, height = 7.5)
```

The results from reaction times also found a robust effect of moral character for both match trials (see figure \@ref(fig:plot-bayes-meta-1) C) and nonmatch trials (**see supplementary materials**). For match trials, shapes associated with good characters were faster (median = `r med_rt_m_m1_good` ms, 95% HDI = [`r ll_rt_m_m1_good` `r ul_rt_m_m1_good`]) than shapes associated with neutral characters (median = `r med_rt_m_m1_neut` ms, 95% HDI = [`r ll_rt_m_m1_neut` `r ul_rt_m_m1_neut`]), the effect ($median_{diff}$ =  -44, 95% HDI  [-67, -24]) has a 99.94% probability of being negative (< 0), 99.94% of being significant (< -0.05). We also found that RTs to shapes associated with bad characters (median = `r med_rt_m_m1_bad` ms, 95% HDI = [`r ll_rt_m_m1_bad` `r ul_rt_m_m1_bad`]) were slower as compared to the neutral character, the effect ($median_{diff}$ = 17, 95% HDI  [-6, 36]) has a 93.58% probability of being positive (> 0), 93.55% of being significant (> 0.05).

For the nonmatch trials, we found a similar pattern but a much smaller effect size. Shapes associated with good characters (median = `r med_rt_nm_m1_good` ms, 95% HDI = [`r ll_rt_nm_m1_good` `r ul_rt_nm_m1_good`]) were faster than shapes associated with neutral characters (median = `r med_rt_nm_m1_neut` ms, 95% HDI = [`r ll_rt_nm_m1_neut` `r ul_rt_nm_m1_neut`]), the difference ($median_{diff}$ = -18, 95% HDI  [-27, -8]) has a 99.91% probability of being negative (< 0), 99.91% of being significant (< -0.05). In contrast, the shapes associated with bad characters (median = `r med_rt_nm_m1_bad` ms, 95% HDI = [`r ll_rt_nm_m1_bad` `r ul_rt_nm_m1_bad`]) were slower than shapes associated with neutral characters, the effect ($median_{diff}$ = 5, 95% HDI  [-3, 13]) has a 92.43% probability of being positive (> 0), 92.31% of being significant (> 0.05).

## Modulation effect self-referential processing

To test the modulation effect of self-relevance, we also modeled data from three experiments (3a, 3b, and 6b) with three-level Bayesian models. These three experiments included 108 unique participants. We focused on the population-level effect of the interaction between self-referential processing and moral valence. Also, we examined the differences of differences, i.e., how the differences between good/bad characters and the neutral character under the self-referencing conditions differ from that under other-referencing conditions. The results of each experiment can be found in supplementary materials.

```{r prepare data for second meta, echo=FALSE, results='hide', warning=FALSE}
### try meta-analysis 1a, 1b, 1c, 2, 5 and 6a
selected_columns <- c('ExpID', 'Site', 'Subject','Age', 'Sex', 'Matchness', 'Identity', 'Valence', 'RESP', 'ACC','RT')
df_ms <- dplyr::bind_rows(df3a.v_meta[selected_columns],
                          df3b.v_meta[selected_columns],
                          df6b.v_meta[selected_columns]) %>%
  dplyr::mutate(ExpID_new = paste(ExpID, Site, sep = "_")) %>%
  dplyr::mutate(Valence = factor(Valence, levels = c('Neutral', 'Bad', 'Good')))

df_ms_subj <- df_ms %>%
  dplyr::group_by(ExpID_new, Site) %>%
  dplyr::summarize(N = n_distinct(Subject),
                   N_trial = length(Subject),
                   Exp_conds = 12,
                   trial_per_cond = round((length(Subject)/12)/N, 0))

df_ms <- df_ms %>%
        dplyr::filter(!is.na(RESP)) %>% # filter trials without response
        dplyr::filter(!(RT <= 200 & ACC == 1))  %>%     # filter trials with RT <= 200 ms
        dplyr::mutate(ismatch = ifelse(Matchness == 'Match', 1, 0),
                      saymatch = ifelse((Matchness == 'Match' & ACC == 1) | 
                                                (Matchness == 'Mismatch' & ACC == 0), 1, 0),
                      Valence = factor(Valence, levels = c('Neutral', 'Bad', 'Good')),
                      Identity = factor(Identity, levels = c('Self', 'Other'))) %>%
        dplyr::select(ExpID_new, Subject, Matchness, Identity,Valence, RESP, ACC, RT, ismatch, saymatch) %>%
        dplyr::mutate(ismatch_num = ifelse(Matchness == 'Match', 0.5, -0.5))

# plot the nested structure of the data
# with(df_ms, table(Subject, ExpID_new)) %>%
#   image(
#     col = grey.colors(80, start = 1, end = 0),
#     axes = TRUE,
#     xlab = "Subject",
#     ylab = "ExpID"
#   )
```

```{r second meta sdt, echo=FALSE, results='hide', warning=FALSE}
# fit a three-level hierarchical model for SDT of moral self, didn't specify the prior; dummy coding
# 
# Note: initialization failed for a few times for full model. need to re-consider the model
sdt_ms_m1 <- df_ms %>%
        dplyr::mutate(Subject = as.factor(Subject),
                      ExpID_new = as.factor(ExpID_new)) %>%
        brms::brm(saymatch ~ 0 + Identity:Valence + ismatch:Identity:Valence + 
                          (0 + Identity:Valence + ismatch:Identity:Valence | ExpID_new) + 
                          (0 + Identity:Valence + ismatch:Identity:Valence | ExpID_new:Subject),
                  family = bernoulli(link="probit"),
                  data = .,
                  chains = 4,
                  iter = 6000,
                  thin = 2,
                  control = list(adapt_delta = .90),
                  cores = parallel::detectCores(),
                  backend = 'cmdstanr',  # with cmdstanr
                  file = here::here("glmmModels/sdt_ms_DummyCode_3_level"))

#### plot both overall parameters and experimental levels.
df_ms_sdt_m1_post_exp <- sdt_ms_m1 %>% 
  tidybayes::spread_draws(r_ExpID_new[condition, term]) %>%
  dplyr::rename(value = r_ExpID_new)

df_ms_sdt_m1_pop_mean <- sdt_ms_m1 %>%
  tidybayes::gather_draws(`b_.*`, regex = TRUE) %>%
  group_by(.variable) %>%       # this line not necessary (done automatically by spread_draws)
  tidybayes::mean_hdci(.value)  # get the high density continuous intervals

# extract the population level parameters
df_ms_sdt_m1_pop <- sdt_ms_m1 %>% 
  tidybayes::gather_draws(`b_.*`, regex = TRUE) %>%
  dplyr::rename(term = .variable,
                pop_mean = .value) %>%
  #tidyr::separate(term, c(NA, 'term'), "_") 
  dplyr::ungroup() %>%
  dplyr::mutate(term = gsub("b_", "", term))

# Add the population mean to the experimental level and only keep the value after.
df_ms_sdt_m1_post_exp_update <- merge(df_ms_sdt_m1_pop, df_ms_sdt_m1_post_exp, by = c('term','.chain','.iteration', '.draw'), all = T) %>%
  dplyr::mutate(mean_value = pop_mean + value) %>%  # add the population level value to each experiment
  dplyr::select(condition, term, `.chain`, `.iteration`, `.draw`, mean_value) %>% # select the added value
  dplyr::rename(value = mean_value)  # rename the value

# Plot the difference between conditions 
df_ms_sdt_m1_plot <- df_ms_sdt_m1_pop %>%
  dplyr::mutate(condition = 'Overall') %>%
  dplyr::rename(value = pop_mean) %>%
  dplyr::select(condition, term, `.chain`, `.iteration`, `.draw`, value) %>%
  dplyr::bind_rows(., df_ms_sdt_m1_post_exp_update) %>%
  dplyr::mutate(condition = factor(condition, levels = c("Exp3a_THU", "Exp3b_WZU", "Exp6b_THU",
                                                         "Overall")),
                condition = forcats::fct_rev(condition), # reverse the order because the plot function auto reverse.
                Valence = dplyr::case_when(grepl("Neutral", term) ~ "Neutral",
                                           grepl("Bad", term) ~"Bad",
                                           grepl("Good", term) ~"Good"),
                Identity = dplyr::case_when(grepl("Self", term) ~ "Self",
                                           grepl("Other", term) ~"Other"),
                params = dplyr::case_when(grepl("ismatch", term) ~ "dprime",
                                           !grepl("ismatch", term) ~"c"),
                params = factor(params, levels = c('dprime', 'c')),
                Identity = factor(Identity, levels = c("Self", "Other")),
                Valence = factor(Valence, levels = c('Good', 'Neutral',  'Bad'))
                ) 

# plot the posterior of the d prime
# use the overall mean values as the vlines
p_ms_sdt_vlines <- df_ms_sdt_m1_plot %>% 
        #tidyr::separate(term, c('params', 'Identity', 'Valence')) %>% 
        dplyr::mutate(Valence = factor(Valence, levels = c('Good', 'Neutral', 'Bad'))) %>%
        dplyr::filter(params == 'dprime') %>%
        dplyr::filter(condition == "Overall") %>%
        dplyr::group_by(Identity, Valence) %>% 
        dplyr::summarize(Mean = mean(value)) # %>%
        # dplyr::arrange(Mean)

# THIS is the one which the final plot will based on!!!
p_ms_dprime1 <- df_ms_sdt_m1_plot %>%
        # tidyr::separate(term, c('params', 'Valence')) %>%
        dplyr::mutate(Valence = factor(Valence, levels = c('Good', 'Neutral', 'Bad')),
                      Identity = factor(Identity, levels = c("Self", "Other"))) %>%
        dplyr::filter(params == 'dprime') %>%
        dplyr::rename(Experiments = condition) %>%
        dplyr::filter((value >= 0) & (value <= 4)) %>%  # limit the x-axis's value
        ggplot2::ggplot(aes(y = Experiments, x = value, color = Valence)) +
        tidybayes::stat_halfeye(aes(fill = Valence), alpha = 0.7) +
        facet_wrap(~Identity) + 
        geom_vline(data = p_ms_sdt_vlines, aes(xintercept = Mean,colour = Valence), linetype = "dashed") +
        labs(x=expression("Posteior distribution of "~italic(d)~"'")) + 
        theme_apa(base_size = 20)

# plot the posterior of the difference between d prime
df_ms_sdt_m1_plot_diff <- df_ms_sdt_m1_plot %>%
        tidyr::unite(term, c('params', 'Identity', 'Valence')) %>%
        tidyr::pivot_wider(names_from = c(term), values_from = value) %>%   # long to wide
        dplyr::mutate(
                diff_GN_dprm_S = dprime_Self_Good - dprime_Self_Neutral,
                diff_BN_dprm_S = dprime_Self_Bad - dprime_Self_Neutral,
                diff_GN_dprm_O = dprime_Other_Good - dprime_Other_Neutral,
                diff_BN_dprm_O = dprime_Other_Bad - dprime_Other_Neutral,
                diff_SO_dprm_G = dprime_Self_Good - dprime_Other_Good,
                diff_SO_dprm_N =  dprime_Self_Neutral - dprime_Other_Neutral,
                diff_SO_dprm_B =  dprime_Self_Bad - dprime_Other_Bad,
                diff_diff_GN = diff_GN_dprm_S - diff_GN_dprm_O,
                diff_diff_BN = diff_BN_dprm_S - diff_BN_dprm_O
                ) %>%
        dplyr::select(condition, `.chain`, `.iteration`, `.draw`,
                      diff_GN_dprm_S, diff_BN_dprm_S, diff_GN_dprm_O, diff_BN_dprm_O,
                      diff_SO_dprm_G, diff_SO_dprm_N, diff_SO_dprm_B, diff_diff_GN, diff_diff_BN) 

# define face titles
contrast_names_ms_sdt <- c(`diff_GN_dprm_S` = "Self:\nGood - Neutral", 
                           `diff_BN_dprm_S` = "Self:\nBad - Neutral", 
                           `diff_GN_dprm_O` = "Other:\nGood - Neutral", 
                           `diff_BN_dprm_O` = "Other:\nBad - Neutral"
                           )

p_ms_dprime1_diff_val <- df_ms_sdt_m1_plot_diff %>%
        tidyr::pivot_longer(cols = diff_GN_dprm_S:diff_diff_BN, names_to = "term_diff", values_to =  "value") %>%  # wide to long
        dplyr::filter(str_detect(term_diff, '_dprm_S|_dprm_O')) %>%
        dplyr::mutate(term_diff = factor(term_diff, levels = c('diff_GN_dprm_S', 'diff_BN_dprm_S', 
                                                               'diff_GN_dprm_O', 'diff_BN_dprm_O'))) %>%
        dplyr::filter((value >= -1.5) & (value <= 1.5)) %>%
        dplyr::rename(Experiments = condition) %>%
        ggplot2::ggplot(aes(y = Experiments, x = value, fill = after_stat(x > 0))) +
        tidybayes::stat_halfeye() +
        geom_vline(xintercept = 0, linetype = "dashed") +
        scale_fill_manual(values = c('skyblue', 'gray80'),
                    breaks = c(TRUE, FALSE),
                    name = "Effect" , labels = c("Yes", "No")) + 
        # scale_fill_manual(values = c('gray80', 'skyblue')) +
        xlab(expression(paste("Valence effect on", italic("d"), "'", sep = ' '))) + 
        facet_wrap( ~ term_diff, # scales = "free_y",
               nrow = 1,
               labeller = as_labeller(contrast_names_ms_sdt)) + # label_parsed
        theme_apa(base_size = 20)  + 
        theme(strip.text.x = element_text(size = 16)) # colour = "orange", angle = 90))

# another way to present the results, not presented in the manuscript
p_ms_dprime1_diff_id <- df_ms_sdt_m1_plot_diff %>%
        tidyr::pivot_longer(cols = diff_GN_dprm_S:diff_diff_BN, names_to = "term_diff", values_to =  "value") %>%  # wide to long
        dplyr::filter(str_detect(term_diff, '_SO_')) %>%
        dplyr::mutate(term_diff = factor(term_diff, levels = c('diff_SO_dprm_G', 'diff_SO_dprm_N', 'diff_SO_dprm_B'))) %>%
        #dplyr::filter(!str_detect(term_diff, '_GB_')) %>%
        # tidyr::unite(term_diff, c('Identity', 'term_diff')) %>%
        dplyr::filter((value >= -1.5) & (value <= 1.5)) %>%
        dplyr::rename(Experiments = condition) %>%
        ggplot2::ggplot(aes(y = Experiments, x = value, fill = after_stat(x > 0))) +
        tidybayes::stat_halfeye() +
        geom_vline(xintercept = 0, linetype = "dashed") +
        scale_fill_manual(values = c('skyblue', 'gray80'),
                    breaks = c(TRUE, FALSE),
                    name = "Effect" , labels = c("Yes", "No")) + 
        # scale_fill_manual(values = c('gray80', 'skyblue')) +
        xlab(expression(paste("Self-referential effect on ", italic("d"), " prime", sep = ' '))) + 
        facet_wrap( ~ term_diff, # scales = "free_y",
               nrow = 1,
               labeller = label_parsed)

test_res_sdt_ms <- bayestestR::sexit(df_ms_sdt_m1_plot_diff[5:13])
test_res_sdt_ms
```

For the *d* prime, we found an interaction between the moral valence and self-relevance: the good-neutral differences are larger for the self-referencing condition than for the other-referencing condition, the difference ($median_{diff}$ = 0.48, 95% HDI [-0.62, 1.65]) has a 93.04% probability of being positive (> 0), 91.92% of being significant (> 0.05). However, the bad-neutral differences ($median_{diff}$ = 0.0087, 95% HDI [-0.96, 1.00]) only have a 51.85% probability of being positive (> 0), 41.29% of being significant (> 0.05). Further analyses revealed that the prioritization effect of good character (as compared to neutral) only appeared for self-referencing conditions but not other-referencing conditions. The estimated *d* prime for good-self was greater than neutral-self ($median_{diff}$ = 0.54, 95% HDI [-0.30, 1.41]), with a 95.99% probability of being positive (> 0), 95.36% of being significant (> 0.05). The differences between bad-self and neutral-self, good-other and neutral-other, and bad-other and neutral-other are all centered around zero (see Figure \@ref(fig:plot-bayes2), B, D). 

```{r second meta rt, echo=FALSE, results='hide', warning=FALSE}
# fit a three-level hierarchical model for RT, didn't specify the prior, lognormal, dummy coding, both match and mismatch trials
# may not converge event after 10000 samples
# Only matched trials
RT_ms_m1_match <- df_ms %>%
        dplyr::mutate(RT_sec = RT/1000) %>% # log RT in seconds
        dplyr::filter(ACC == 1) %>%         # only correct trials
        dplyr::filter(Matchness == "Match") %>%
        brms::brm(RT_sec ~ Identity*Valence + 
              (Identity*Valence | ExpID_new) +   
              (Identity*Valence | ExpID_new:Subject),
            family=lognormal(),
            data = .,
            chains = 4,
            control = list(adapt_delta = .90),
            iter = 10000,
            thin = 1,
            cores = parallel::detectCores(),
            backend = 'cmdstanr',  # with cmdstanr
            file = here::here("glmmModels/RT_ms_DummyCode_3_level_match"))
# summary(RT_ms_m1_match) # R-hat still has problem

# Get the variables in the model 1
# RT_var_name_m1 <- tidybayes::get_variables(RT_val_m1)
df_ms_m1_post_rt_exp <- RT_ms_m1_match %>% 
  tidybayes::spread_draws(r_ExpID_new[condition, term]) %>%
  dplyr::rename(value = r_ExpID_new)

df_ms_m1_rt_pop_mean <- RT_ms_m1_match %>%
  tidybayes::gather_draws(`b_.*`, regex = TRUE) %>%
  group_by(.variable) %>%       # this line not necessary (done automatically by spread_draws)
  tidybayes::mean_hdci(.value)  # get the high density continuous intervals

# extract the population level parameters
df_ms_m1_rt_pop <- RT_ms_m1_match %>% 
  tidybayes::gather_draws(`b_.*`, regex = TRUE) %>%
  dplyr::rename(term = .variable,
                pop_mean = .value) %>%
  dplyr::ungroup() %>%
  dplyr::mutate(term = gsub("b_", "", term))

# Add the population mean to the experimental level and only keep the value after.
df_ms_m1_rt_exp_update <- merge(df_ms_m1_rt_pop, df_ms_m1_post_rt_exp, 
                     by = c('term','.chain','.iteration', '.draw'), all = T) %>%
  dplyr::mutate(mean_value = pop_mean + value) %>%  # add the population level value to each experiment
  dplyr::select(condition, term, `.chain`, `.iteration`, `.draw`, mean_value) %>% # select the added value
  dplyr::rename(value = mean_value)  # rename the value

# Plot the difference between conditions 
df_ms_m1_plot_rt <- df_ms_m1_rt_pop %>%
        dplyr::mutate(condition = 'Overall') %>%
        dplyr::rename(value = pop_mean) %>%              # change the `pop_mean` as `value` for data frame merge
        dplyr::select(condition, term, `.chain`, `.iteration`, `.draw`, value) %>%
        dplyr::bind_rows(., df_ms_m1_rt_exp_update) %>%
        dplyr::mutate(condition = factor(condition, levels = c("Exp3a_THU", "Exp3b_WZU", "Exp6b_THU",
                                                               "Overall")),
                      condition = forcats::fct_rev(condition)) %>%
        tidyr::pivot_wider(names_from = c(term), values_from = value) %>%   # long to wide
        dplyr::mutate(M_Self_Neutral = Intercept,
                      M_Self_Bad = Intercept + ValenceBad,
                      M_Self_Good = Intercept  + ValenceGood,
                      M_Other_Neutral = Intercept  + IdentityOther,
                      M_Other_Bad = Intercept  + `IdentityOther:ValenceBad`,
                      M_Other_Good = Intercept + `IdentityOther:ValenceGood`) %>%
        dplyr::select(condition, `.chain`, `.iteration`, `.draw`,
               contains('M_')) %>%
  tidyr::pivot_longer(cols = M_Self_Neutral:M_Other_Good, names_to = "term", values_to =  "value") %>%  # wide to long
  dplyr::mutate(term = factor(term, levels = c('M_Self_Neutral', 'M_Self_Bad', 'M_Self_Good',
                                               'M_Other_Neutral', 'M_Other_Bad', 'M_Other_Good')),
                value = exp(value),
                value = value * 1000) 

# plot the posterior of the d prime
# use the overall mean values as the vlines
p_ms_rt1_vlines <- df_ms_m1_plot_rt %>% 
        tidyr::separate(term, c('Match', 'Identity', 'Valence')) %>% 
        dplyr::mutate(Valence = factor(Valence, levels = c('Good', 'Neutral', 'Bad')),
                      Identity = factor(Identity, levels = c("Self", "Other"))) %>%
        dplyr::filter(Match == 'M') %>%
        dplyr::filter(condition == "Overall") %>%
        dplyr::filter((value >=200) & (value <=1200)) %>%
        dplyr::group_by(Identity, Valence) %>% 
        tidybayes::median_hdci(value)

# THIS is the one which the final plot will based on!!!
p_ms_rt1 <- df_ms_m1_plot_rt %>%
        tidyr::separate(term, c('Match', 'Identity', 'Valence')) %>% 
        dplyr::mutate(Valence = factor(Valence, levels = c('Good', 'Neutral', 'Bad')),
                      Identity = factor(Identity, levels = c("Self", "Other"))) %>%
        dplyr::filter(Match == 'M') %>%
        dplyr::rename(Experiments = condition) %>%
        dplyr::filter((value >=200) & (value <=1200)) %>%
        ggplot2::ggplot(aes(y = Experiments, x = value, color = Valence)) +
        tidybayes::stat_halfeye(aes(fill = Valence), alpha = 0.7) +
        geom_vline(data = p_ms_rt1_vlines, aes(xintercept = value, colour = Valence), linetype = "dashed") +
        labs(x=expression('Posteior distribution of RTs')) + 
        facet_wrap(~Identity) + 
        theme_apa(base_size = 20)

df_ms_m1_plot_rt_diff <- df_ms_m1_plot_rt %>%
        tidyr::pivot_wider(names_from = c(term), values_from = value) %>%       # long to wide
        # calculate the difference between conditions, matched trials only
        dplyr::mutate(diff_GN_M_S = M_Self_Good - M_Self_Neutral,               # calculate the differences between conditions
                      diff_BN_M_S = M_Self_Bad - M_Self_Neutral,
                      diff_GN_M_O = M_Other_Good - M_Other_Neutral,  
                      diff_BN_M_O = M_Other_Bad - M_Other_Neutral,
                      diff_SO_G   = M_Self_Good - M_Other_Good, 
                      diff_SO_N   = M_Self_Neutral - M_Other_Neutral,
                      diff_SO_B   = M_Self_Bad - M_Other_Bad,
                      diff_diff_GN = diff_GN_M_S - diff_GN_M_O,
                      diff_diff_BN = diff_BN_M_S - diff_BN_M_O) %>%
        dplyr::select(condition, `.chain`, `.iteration`, `.draw`,
                      diff_GN_M_S, diff_BN_M_S, diff_GN_M_O,
                      diff_BN_M_O, diff_SO_G, diff_SO_N, diff_SO_B,
                      diff_diff_GN, diff_diff_BN) #  %>%

df_ms_rt1_diff_diff <- df_ms_m1_plot_rt_diff %>%
        tidyr::pivot_longer(cols = diff_GN_M_S:diff_diff_BN, names_to = "term_diff", values_to =  "value") %>%  # wide to long
        dplyr::filter(str_detect(term_diff, 'diff_diff')) %>%
        dplyr::mutate(term_diff = factor(term_diff, levels = c('diff_diff_GN', 'diff_diff_BN'))) %>% 
        dplyr::filter(condition == "Overall") %>%
        tidyr::pivot_wider(.,
                           id_cols = c('.chain', '.iteration', '.draw'),
                           names_from = term_diff) %>%
        dplyr::select(-c('.chain', '.iteration', '.draw')) %>%
        bayestestR::describe_posterior(.,
                                       ci = 0.95,
                                       ci_method = 'hdi',
                                       test = c("p_direction", "p_significance"),
                                       centrality = "median")

# define face titles
contrast_names_ms_rt <- c(`diff_GN_M_S` = "Self:\nGood - Neutral", 
                           `diff_BN_M_S` = "Self:\nBad - Neutral", 
                           `diff_GN_M_O` = "Other:\nGood - Neutral", 
                           `diff_BN_M_O` = "Other:\nBad - Neutral"
                           )
# plot the posterior of matching trials of valence
p_ms_rt1_diff_val <- df_ms_m1_plot_rt_diff %>%
        tidyr::pivot_longer(cols = diff_GN_M_S:diff_SO_B, names_to = "term_diff", values_to =  "value")  %>%  # wide to long
        dplyr::filter(str_detect(term_diff, '_M_')) %>%
        dplyr::mutate(term_diff = factor(term_diff, levels = c('diff_GN_M_S','diff_BN_M_S', 
                                                               'diff_GN_M_O', 'diff_BN_M_O'))) %>%
        dplyr::filter((value >= -200) & (value <= 300)) %>%
        dplyr::rename(Experiments = condition) %>%
        ggplot2::ggplot(aes(y = Experiments, x = value, fill = after_stat(x < 0))) +
        tidybayes::stat_halfeye() +
        geom_vline(xintercept = 0, linetype = "dashed") +
        scale_fill_manual(values = c('skyblue', 'gray80'),
                    breaks = c(TRUE, FALSE),
                    name = "Effect" , labels = c("Yes", "No")) + 
        # scale_fill_manual(values = c('gray80', 'skyblue')) + 
        xlab("Valence effect on RTs (matching trials)") +
        scale_x_continuous(breaks=seq(-200, 300, 200)) +  
        facet_wrap( ~ term_diff,
              # scales = "free_y", 
              nrow = 1,
              labeller = as_labeller(contrast_names_ms_rt)) + # label_parsed
        theme_apa(base_size = 20) + 
        theme(strip.text.x = element_text(size = 16)) 

df_ms_rt1_diff_val <- df_ms_m1_plot_rt_diff %>%
        tidyr::pivot_longer(cols = diff_GN_M_S:diff_SO_B, names_to = "term_diff", values_to =  "value")  %>%  # wide to long
        dplyr::filter(str_detect(term_diff, '_M_')) %>%
        dplyr::filter(condition == "Overall") %>%
        tidyr::pivot_wider(.,
                           id_cols = c('.chain', '.iteration', '.draw'),
                           names_from = term_diff) %>%
        dplyr::select(-c('.chain', '.iteration', '.draw')) %>%
        bayestestR::describe_posterior(.,
                                       ci = 0.95,
                                       ci_method = 'hdi',
                                       test = c("p_direction", "p_significance"),
                                       centrality = "median")

# plot the posterior of matching trials, diff between self and other
p_ms_rt1_diff_id <- df_ms_m1_plot_rt_diff %>%
        tidyr::pivot_longer(cols = diff_GN_M_S:diff_SO_B, names_to = "term_diff", values_to =  "value")  %>%  # wide to long
        dplyr::filter(str_detect(term_diff, '_SO_')) %>%
        dplyr::mutate(term_diff = factor(term_diff, levels = c('diff_SO_G', 'diff_SO_N', 'diff_SO_B'))) %>%
        dplyr::filter((value >= -250) & (value <= 250)) %>%
        dplyr::rename(Experiments = condition) %>%
        ggplot2::ggplot(aes(y = Experiments, x = value, fill = after_stat(x < 0))) +
        tidybayes::stat_halfeye() +
        geom_vline(xintercept = 0, linetype = "dashed") +
        scale_fill_manual(values = c('skyblue', 'gray80'),
                    breaks = c(TRUE, FALSE),
                    name = "Effect" , labels = c("Yes", "No")) + 
        # scale_fill_manual(values = c('gray80', 'skyblue')) + 
        xlab("Self-referential effect on RTs (Match trials)") +
        facet_wrap( ~ term_diff,
              # scales = "free_y", 
              nrow = 1,
              labeller = label_parsed)

df_ms_rt1_diff_id <- df_ms_m1_plot_rt_diff %>%
        tidyr::pivot_longer(cols = diff_GN_M_S:diff_SO_B, names_to = "term_diff", values_to =  "value")  %>%  # wide to long
        dplyr::filter(str_detect(term_diff, '_SO_')) %>%
        dplyr::mutate(term_diff = factor(term_diff, levels = c('diff_SO_G', 'diff_SO_N', 'diff_SO_B'))) %>%
        dplyr::filter(condition == "Overall") %>%
        tidyr::pivot_wider(.,
                           id_cols = c('.chain', '.iteration', '.draw'),
                           names_from = term_diff) %>%
        dplyr::select(-c('.chain', '.iteration', '.draw')) %>%
        bayestestR::describe_posterior(.,
                                       ci = 0.95,
                                       ci_method = 'hdi',
                                       test = c("p_direction", "p_significance"),
                                       centrality = "median")

test_res_rt_ms <- bayestestR::sexit(df_ms_m1_plot_rt_diff[,5:13])
test_res_rt_ms
```


(ref:fig2-caption) Interaction between moral character and self-referential. (A) Experimental level (three experiments) and population level posterior distributions of RT under different conditions; (B) Experimental level and population level posterior distributions of *d*-prime under different conditions; (C) Experimental level and population level posterior distributions of the RT differences between conditions, from left to right: Good-self vs. Neutral-self, Bad-self vs. Neutral-self, Good-other vs. Neutral-other, Bad-other vs. Neutral-other; (D) Experimental level and population level posterior distributions of the *d*-prime differences between conditions, from left to right: Good-self vs. Neutral-self, Bad-self vs. Neutral-self, Good-other vs. Neutral-other, Bad-other vs. Neutral-other.

```{r plot-bayes2, fig.cap="(ref:fig2-caption)", fig.height=12, fig.width=20, warning=FALSE}
library(patchwork)
# (p_rt1 | p_dprime1)
p_ms_rt1 + p_ms_dprime1 +
        p_ms_rt1_diff_val + p_ms_dprime1_diff_val + plot_annotation(tag_levels = 'A')  + plot_layout(nrow = 2, byrow = TRUE, guides = "collect") 
        # p_ms_rt1_diff_id + p_ms_dprime1_diff_id + 
```

For the RTs of matched trials, we also found an interaction between moral valence and self-relevance: the good-neutral differences were larger for the self- than the other-referencing conditions ($median_{diff}$ = -148, 95% HDI [-413, 73]) has a 96.05% probability of being negative (< 0), 96.05% of being significant (< -0.05). However, this pattern was much weaker for bad-neutral differences ($median_{diff}$ = -47, 95% HDI [-280, 182]) has a 79.91% probability of being negative (< 0) and 79.88% of being significant (< -0.05). Further analyses revealed a robust good-self prioritization effect as compared to neutral-self ($median_{diff}$ = -59, 95% HDI [-115, -22]) has a 98.87% probability of being negative (< 0) and 98.87% of being significant (< -0.05)) and good-other ($median_{diff}$ = -109, 95% HDI [-227, -31]) has a 98.65% probability of being negative (< 0) and 98.65% of being significant (< -0.05)) conditions. Similar to the results of *d'*, we found that participants responded slower for both good character than for the neutral character when they referred to others, $median_{diff}$ = 85.01, 95% HDI [-112, 328]) has a 92.16% probability of being positive (> 0) and 92.15% of being significant (> 0.05). A similar pattern was also found for the bad character when referred to others: bad-other responded slower than neutral-other,  $median_{diff}$ = 44, 95% HDI [-146, 268]) has an 80.03% probability of being positive (> 0) and 79.99% of being significant (> 0.05). See Figure \@ref(fig:plot-bayes2).

These results suggested that the prioritization of good character is not solely driven by the valence of moral character. Instead, self-relevance modulated the prioritization of good character: good character was prioritized only when it referred to the self. When the moral character referred to others, responses to both good and bad characters were slowed down.  

## The link between oneself and good character

Experiments 4a and 4b were designed to test whether the good character and the self bind together spontaneously. Because these two experiments have different experimental designs, we model their data separately.   

In experiment 4a, where “self” vs. “other” were task-relevant and moral character were task-irrelevant, we found the “self” conditions performed better than the “other” conditions for both *d* prime and reaction times. This pattern is consistent with previous studies (e.g., @Sui_2012_JEPHPP).

```{r 4a_BGLMM_sdt, echo=FALSE, results='hide', warning=FALSE, message=FALSE}
exp_name <- '4a'
exp4a_sdt_m1 <- fun_sdt_val_id(exp_name)

#summary(exp4a_sdt_m1)    # check summary

# check fixed and varying effect using bayestestR
# bayestestR::describe_posterior(
#   exp4a_sdt_m1,
#   effects = "all",
#   component = "all",
#   test = c("p_direction", "p_significance"),
#   centrality = "all"
# )
#pp_check(exp4a_sdt_m1)   # posterior predictive check
# extract the population level parameters
# criteria

df_exp4a_sdt_m1_plot <- exp4a_sdt_m1 %>%
  tidybayes::gather_draws(`b_.*`, regex = TRUE) %>% # get the traces of population level parameters
  # create two columns for two independent factors.
  dplyr::mutate(Valence = dplyr::case_when(grepl("Neutral", .variable) ~ "Neutral",
                                           grepl("Bad", .variable) ~"Bad",
                                           grepl("Good", .variable) ~"Good"),
                Identity = dplyr::case_when(grepl("Self", .variable) ~ "Self",
                                           grepl("Other", .variable) ~"Other"),
                params = dplyr::case_when(grepl("ismatch", .variable) ~ "dprime",
                                           !grepl("ismatch", .variable) ~"criterion"),
                params = factor(params, levels = c('dprime', 'criterion')),
                Identity = factor(Identity, levels = c("Self", "Other")),
                Valence = factor(Valence, levels = c('Good', 'Neutral',  'Bad'))) 

exp4a_sdt_p <- df_exp4a_sdt_m1_plot %>%
  dplyr::filter(params == 'dprime') %>%  # select only d prime
  ggplot2::ggplot(aes(x = Identity, y = .value, color = Valence)) +
  tidybayes::stat_halfeye(aes(fill = Valence), alpha = 0.7) + # position=position_dodge(width = 0.1)
  # geom_slabinterval(ymin = 0, ymax = 4) +
  stat_summary(aes(group = Valence, color = Valence), fun = mean, geom = "line") +
  labs(x = expression("Self-Referential"), 
      y = expression(paste("Posteior of sensitivity ", italic("d'"), sep = ' '))) +
  scale_colour_brewer(palette = "Dark2") +
  scale_fill_brewer(palette = "Dark2") +
  # facet_grid(~ params , scales = "free_y") +
  theme_apa(base_size = 20) 

df_exp4a_sdt_hdi <- df_exp4a_sdt_m1_plot %>%
        dplyr::ungroup() %>%
        dplyr::filter(params == 'dprime') %>%
        dplyr::select(-c('.variable', 'params')) %>%
        # dplyr::mutate(term_diff = stringr::str_sub(.$term_diff, end = -8)) %>%
        tidyr::pivot_wider(.,
                           id_cols = c('.chain', '.iteration', '.draw'),
                           values_from = '.value',
                           names_from = c('Valence', 'Identity')) %>%
        dplyr::select(-c('.chain', '.iteration', '.draw')) %>%
        bayestestR::describe_posterior(.,
                                       ci = 0.95,
                                       ci_method = 'hdi',
                                       test = c("p_direction", "p_significance"),
                                       BF = 1,
                                       centrality = "median")

### prepare results to report
# neutral self
rep_m_d_exp4a_neut_s <- df_exp4a_sdt_hdi %>% dplyr::filter(Parameter == "Neutral_Self") %>% dplyr::select(Median) %>% 
        dplyr::pull() %>%
        round(., digits = 3)
rep_ll_d_exp4a_neut_s <- df_exp4a_sdt_hdi %>% dplyr::filter(Parameter == "Neutral_Self") %>% dplyr::select(CI_low) %>% 
        dplyr::pull() %>%
        round(., digits = 3)
rep_ul_d_exp4a_neut_s <- df_exp4a_sdt_hdi %>% dplyr::filter(Parameter == "Neutral_Self") %>% dplyr::select(CI_high) %>% 
        dplyr::pull() %>%
        round(., digits = 3)

# good self
rep_m_d_exp4a_good_s <- df_exp4a_sdt_hdi %>% dplyr::filter(Parameter == "Good_Self") %>% dplyr::select(Median) %>% 
        dplyr::pull() %>%
        round(., digits = 3)
rep_ll_d_exp4a_good_s <- df_exp4a_sdt_hdi %>% dplyr::filter(Parameter == "Good_Self") %>% dplyr::select(CI_low) %>% 
        dplyr::pull() %>%
        round(., digits = 3)
rep_ul_d_exp4a_good_s <- df_exp4a_sdt_hdi %>% dplyr::filter(Parameter == "Good_Self") %>% dplyr::select(CI_high) %>% 
        dplyr::pull() %>%
        round(., digits = 3)

# bad self
rep_m_d_exp4a_bad_s <- df_exp4a_sdt_hdi %>% dplyr::filter(Parameter == "Bad_Self") %>% dplyr::select(Median) %>% 
        dplyr::pull() %>%
        round(., digits = 3)
rep_ll_d_exp4a_bad_s <- df_exp4a_sdt_hdi %>% dplyr::filter(Parameter == "Bad_Self") %>% dplyr::select(CI_low) %>% 
        dplyr::pull() %>%
        round(., digits = 3)
rep_ul_d_exp4a_bad_s <- df_exp4a_sdt_hdi %>% dplyr::filter(Parameter == "Bad_Self") %>% dplyr::select(CI_high) %>% 
        dplyr::pull() %>%
        round(., digits = 3)

# Neutral other
rep_m_d_exp4a_neut_o <- df_exp4a_sdt_hdi %>% dplyr::filter(Parameter == "Neutral_Other") %>% dplyr::select(Median) %>% 
        dplyr::pull() %>%
        round(., digits = 3)
rep_ll_d_exp4a_neut_o <- df_exp4a_sdt_hdi %>% dplyr::filter(Parameter == "Neutral_Other") %>% dplyr::select(CI_low) %>% 
        dplyr::pull() %>%
        round(., digits = 3)
rep_ul_d_exp4a_neut_o <- df_exp4a_sdt_hdi %>% dplyr::filter(Parameter == "Neutral_Other") %>% dplyr::select(CI_high) %>% 
        dplyr::pull() %>%
        round(., digits = 3)

# good other
rep_m_d_exp4a_good_o <- df_exp4a_sdt_hdi %>% dplyr::filter(Parameter == "Good_Other") %>% dplyr::select(Median) %>% 
        dplyr::pull() %>%
        round(., digits = 3)
rep_ll_d_exp4a_good_o <- df_exp4a_sdt_hdi %>% dplyr::filter(Parameter == "Good_Other") %>% dplyr::select(CI_low) %>% 
        dplyr::pull() %>%
        round(., digits = 3)
rep_ul_d_exp4a_good_o <- df_exp4a_sdt_hdi %>% dplyr::filter(Parameter == "Good_Other") %>% dplyr::select(CI_high) %>% 
        dplyr::pull() %>%
        round(., digits = 3)

# bad other
rep_m_d_exp4a_bad_o <- df_exp4a_sdt_hdi %>% dplyr::filter(Parameter == "Bad_Other") %>% dplyr::select(Median) %>% 
        dplyr::pull() %>%
        round(., digits = 3)
rep_ll_d_exp4a_bad_o <- df_exp4a_sdt_hdi %>% dplyr::filter(Parameter == "Bad_Other") %>% dplyr::select(CI_low) %>% 
        dplyr::pull() %>%
        round(., digits = 3)
rep_ul_d_exp4a_bad_o <- df_exp4a_sdt_hdi %>% dplyr::filter(Parameter == "Bad_Other") %>% dplyr::select(CI_high) %>% 
        dplyr::pull() %>%
        round(., digits = 3)

df_exp4a_sdt_m1_plot_diff_wide <- df_exp4a_sdt_m1_plot %>%
        tidyr::unite(term, c('params', 'Identity', 'Valence')) %>%
        dplyr::ungroup() %>%
        dplyr::select(term, `.chain`, `.iteration`, `.draw`, `.value`) %>%
        tidyr::pivot_wider(names_from = c(term), values_from = .value) %>%   # long to wide
        dplyr::mutate(
                diff_GN_dprm_S = dprime_Self_Good - dprime_Self_Neutral,
                diff_BN_dprm_S = dprime_Self_Bad - dprime_Self_Neutral,
                diff_GB_dprm_S = dprime_Self_Good - dprime_Self_Bad,
                diff_GN_dprm_O = dprime_Other_Good - dprime_Other_Neutral,
                diff_BN_dprm_O = dprime_Other_Bad - dprime_Other_Neutral,
                diff_GB_dprm_O = dprime_Other_Good - dprime_Other_Bad,
                diff_SO_dprm_G = dprime_Self_Good - dprime_Other_Good,
                diff_SO_dprm_N =  dprime_Self_Neutral - dprime_Other_Neutral,
                diff_SO_dprm_B =  dprime_Self_Bad - dprime_Other_Bad,
                diff_diff_GN_SO = diff_GN_dprm_S - diff_GN_dprm_O,
                diff_diff_GB_SO = diff_GB_dprm_S - diff_GB_dprm_O,
                diff_diff_BN_SO = diff_BN_dprm_S - diff_BN_dprm_O
                ) %>%
  dplyr::select(`.chain`, `.iteration`, `.draw`,
               diff_GN_dprm_S:diff_diff_BN_SO) 
## plot:
df_exp4a_sdt_m1_plot_diff <- df_exp4a_sdt_m1_plot_diff_wide %>%
  tidyr::pivot_longer(cols = diff_GN_dprm_S:diff_diff_BN_SO, names_to = "term_diff", values_to =  "value") %>%  # wide to long
  dplyr::mutate(term_diff = factor(term_diff, levels = c('diff_GB_dprm_S', 'diff_GN_dprm_S', 'diff_BN_dprm_S', 
                                                         'diff_GB_dprm_O', 'diff_GN_dprm_O', 'diff_BN_dprm_O', 
                                                         'diff_SO_dprm_G', 'diff_SO_dprm_N', 'diff_SO_dprm_B',
                                                         'diff_diff_GN_SO', 'diff_diff_GB_SO', 'diff_diff_BN_SO')))

p_exp4a_dprime1_diff_val <- df_exp4a_sdt_m1_plot_diff %>%
        dplyr::filter(str_detect(term_diff, '_dprm_S|_dprm_O')) %>%
        dplyr::mutate(Identity = dplyr::case_when(grepl("_S", term_diff) ~ "Self",
                                           grepl("_O", term_diff) ~"Other"),
                      Identity = factor(Identity, levels = c("Self", "Other"))) %>%
        dplyr::filter(!str_detect(term_diff, '_GB_|_BN')) %>%
        dplyr::mutate(term_diff = stringr::str_sub(.$term_diff, end = -8)) %>%
        dplyr::mutate(term_diff = dplyr::case_when(#grepl("_GB", term_diff) ~ "Good vs. Bad",
                                                   grepl("_GN", term_diff) ~"Good vs. Neutral",
                                                   #grepl("_BN", term_diff) ~ "Bad vs. Neutral",
                                                   ),
                      ) %>%
        dplyr::filter((value >= -1.5) & (value <= 1.5)) %>%
        dplyr::rename(conditions = term_diff) %>%
        ggplot2::ggplot(aes(x = value, fill = after_stat(x > 0))) + # y = fct_rev(conditions)
        tidybayes::stat_halfeye() +
        geom_vline(xintercept = 0, linetype = "dashed") +
        scale_fill_manual(values = c('skyblue', 'gray80'),
                    breaks = c(TRUE, FALSE),
                    name = "Effect" , labels = c("Yes", "No")) + 
        # scale_fill_manual(values = c('gray80', 'skyblue')) +
        labs(x = expression(paste("Effect of Valence (", italic("d"), "')", sep = ' ')),
             y = expression("Good vs. Neutral")) + 
        scale_x_continuous(breaks=seq(-0.25, 0.75, 0.5)) +
        facet_wrap( ~ Identity, nrow = 1) +
        theme_apa(base_size = 18)

df_exp4a_dprime1_diff_val <- df_exp4a_sdt_m1_plot_diff %>%
        dplyr::filter(str_detect(term_diff, '_dprm_S|_dprm_O')) %>%
        dplyr::mutate(Identity = dplyr::case_when(grepl("_S", term_diff) ~ "Self",
                                           grepl("_O", term_diff) ~"Other"),
                      Identity = factor(Identity, levels = c("Self", "Other"))) %>%
        dplyr::filter(!str_detect(term_diff, '_GB_')) %>%
        dplyr::mutate(term_diff = stringr::str_sub(.$term_diff, end = -8)) %>%
        tidyr::pivot_wider(.,
                           id_cols = c('.chain', '.iteration', '.draw'),
                           names_from = c('term_diff', 'Identity')) %>%
        dplyr::select(-c('.chain', '.iteration', '.draw')) %>%
        bayestestR::describe_posterior(.,
                                       ci = 0.95,
                                       ci_method = 'hdi',
                                       test = c("p_direction", "p_significance"),
                                       BF = 1,
                                       centrality = "median")
# the self-other difference is obvious and not presented in the manuscript
# p_exp4a_dprime1_diff_id <- df_exp4a_sdt_m1_plot_diff %>%
#         dplyr::filter(str_detect(term_diff, '_SO_')) %>%
#         dplyr::mutate(term_diff = dplyr::case_when(grepl("_B", term_diff) ~ "Bad",
#                                                    grepl("_N", term_diff) ~"Neutral",
#                                                    grepl("_G", term_diff) ~ "Good"),
#                       term_diff = factor(term_diff, levels = c("Good", "Neutral", "Bad"))) %>%
#         #dplyr::filter(!str_detect(term_diff, '_GB_')) %>%
#         # tidyr::unite(term_diff, c('Identity', 'term_diff')) %>%
#         dplyr::filter((value >= -1.5) & (value <= 1.5)) %>%
#         dplyr::rename(conditions = term_diff) %>%
#         ggplot2::ggplot(aes(x = value, y = fct_rev(conditions), fill = after_stat(x > 0))) +
#         tidybayes::stat_halfeye() +
#         geom_vline(xintercept = 0, linetype = "dashed") +
#         scale_fill_manual(values = c('skyblue', 'gray80'),
#                     breaks = c(TRUE, FALSE),
#                     name = "Effect" , labels = c("Yes", "No")) +
#         # scale_fill_manual(values = c('skyblue')) +   # note: only one value here, used blue here
#         labs(x = expression(paste("Self-referential effect on ", italic("d"), " prime", sep = ' ')),
#              y = expression("Contrasts (Self vs. Other)"))

p_exp4a_dprime1_diff_diff <- df_exp4a_sdt_m1_plot_diff %>%
        dplyr::filter(str_detect(term_diff, '_diff_')) %>%
        # dplyr::filter(!str_detect(term_diff, 'GB')) %>%
        dplyr::mutate(term_diff = dplyr::case_when(grepl("_GB", term_diff) ~ "GB",
                                                   grepl("_GN", term_diff) ~"GN",
                                                   grepl("_BN", term_diff) ~ "BN"),
                      term_diff = factor(term_diff, levels = c("GN", "GB", "BN"))) %>%
        dplyr::filter((value >= -1.5) & (value <= 1.5)) %>%
        dplyr::rename(conditions = term_diff) %>%
        ggplot2::ggplot(aes(x = value, y = fct_rev(conditions), fill = after_stat(x > 0))) +
        tidybayes::stat_halfeye() +
        geom_vline(xintercept = 0, linetype = "dashed") +
        scale_fill_manual(values = c('skyblue', 'gray80'),
                    breaks = c(TRUE, FALSE),
                    name = "Effect" , labels = c("Yes", "No")) +
        # scale_fill_manual(values = c('skyblue')) +   # note: only one value here, used blue here
        labs(x = expression(paste("Self-referential effect on ", italic("d"), " prime", sep = ' ')),
             y = expression("Difference (Self vs. Other) of difference (Valence)"))

df_exp4a_dprime1_diff_diff <- df_exp4a_sdt_m1_plot_diff %>%  # difference between differences
        dplyr::filter(str_detect(term_diff, '_diff_')) %>%
        # dplyr::filter(!str_detect(term_diff, 'GB')) %>%
        dplyr::mutate(term_diff = dplyr::case_when(grepl("_GB", term_diff) ~ "GB",
                                                   grepl("_GN", term_diff) ~"GN",
                                                   grepl("_BN", term_diff) ~ "BN"),
                      term_diff = factor(term_diff, levels = c("GN", "GB", "BN"))) %>%
        tidyr::pivot_wider(.,
                           id_cols = c('.chain', '.iteration', '.draw'),
                           names_from = term_diff) %>%
        dplyr::select(-c('.chain', '.iteration', '.draw')) %>%
        bayestestR::describe_posterior(.,
                                       ci = 0.95,
                                       ci_method = 'hdi',
                                       test = c("p_direction", "p_significance"),
                                       centrality = "median")

test_res_sdt_4a <- bayestestR::sexit(df_exp4a_sdt_m1_plot_diff_wide[, c(4, 7, 13)])
test_res_sdt_4a
```

More importantly, we found evidence that task-irrelevant moral character also played a role. For shapes associated with “self”, *d'* was greater when shapes had a good character inside (median = `r rep_m_d_exp4a_good_s`, 95% HDI [`r rep_ll_d_exp4a_good_s` `r rep_ul_d_exp4a_good_s`]) than shapes that have neutral character (median = `r rep_m_d_exp4a_neut_s`, 95% HDI [`r rep_ll_d_exp4a_neut_s` `r rep_ul_d_exp4a_neut_s`]), the difference (median = 0.08, 95% HDI [-0.10, 0.27]) has an 81.60% probability of being positive (> 0), 64.33% of being significant (> 0.05). For shapes associated with “other”, the pattern reversed: *d* prime was smaller when shapes had a good character inside (median = `r rep_m_d_exp4a_good_o`, 95% HDI [`r rep_ll_d_exp4a_good_o` `r rep_ul_d_exp4a_good_o`]) than had neutral (median = `r rep_m_d_exp4a_neut_o`, 95% HDI [`r rep_ll_d_exp4a_neut_o` `r rep_ul_d_exp4a_neut_o`]), the difference (median = -0.09, 95% HDI [-0.25, 0.05]) has an 89.03% probability of being negative (< 0), 71.38% of being significant (< -0.05). The difference between these two effects (median = 0.18, 95% HDI [-0.06, 0.43]) has a 92.88% probability of being positive (> 0), 85.08% being significant (> 0.05). See Figure \@ref(fig:plot-exp4-all).

```{r 4a_BGLMM_rt, echo=FALSE, results='hide', warning=FALSE, message=FALSE}
exp_name <- '4a'
exp4a_rt_m1 <- fun_rt_val_id(exp_name)

df_exp4a_m1_plot_rt <- exp4a_rt_m1 %>%
        tidybayes::gather_draws(`b_.*`, regex = TRUE)  %>%
        dplyr::mutate(.variable = gsub("b_", "", .variable)) %>%
        tidyr::pivot_wider(names_from = c(.variable), values_from = .value) %>%
        dplyr::mutate(NM_Self_Neutral = Intercept,               # calculate the differences between conditions
                      NM_Self_Bad = Intercept  + ValenceBad,
                      NM_Self_Good = Intercept  + ValenceGood ,
                      NM_Other_Neutral = Intercept  + IdentityOther,               # calculate the differences between conditions
                      NM_Other_Bad = Intercept  + ValenceBad  + `IdentityOther:ValenceBad`,
                      NM_Other_Good = Intercept  + ValenceGood + `IdentityOther:ValenceGood`,
                      M_Self_Neutral = Intercept  + ismatch,
                      M_Self_Bad = Intercept  + ismatch + `ismatch:ValenceBad`,
                      M_Self_Good = Intercept  + ismatch + `ismatch:ValenceGood`,
                      M_Other_Neutral = Intercept  + ismatch + `ismatch:IdentityOther`,
                      M_Other_Bad = Intercept  + ismatch + `ismatch:IdentityOther` + `ismatch:IdentityOther:ValenceBad`,
                      M_Other_Good = Intercept  + ismatch + `ismatch:IdentityOther` + `ismatch:IdentityOther:ValenceGood`) %>%
        dplyr::select(`.chain`, `.iteration`, `.draw`, contains('M_')) %>%
        tidyr::pivot_longer(cols = NM_Self_Neutral:M_Other_Good, names_to = "term", values_to =  "value") %>%  # wide to long
  dplyr::mutate(term = factor(term, levels = c('NM_Self_Neutral', 'NM_Self_Bad', 'NM_Self_Good',
                                               'NM_Other_Neutral', 'NM_Other_Bad', 'NM_Other_Good',
                                               'M_Self_Neutral', 'M_Self_Bad', 'M_Self_Good',
                                               'M_Other_Neutral', 'M_Other_Bad', 'M_Other_Good')),
                value = exp(value),
                value = value * 1000) 

p_exp4a_rt1 <- df_exp4a_m1_plot_rt %>%
        tidyr::separate(term, c('Match', 'Identity', 'Valence')) %>% 
        dplyr::mutate(Valence = factor(Valence, levels = c('Good', 'Neutral', 'Bad')),
                      Identity = factor(Identity, levels = c("Self", "Other"))) %>%
        dplyr::filter(Match == 'M') %>%
        # dplyr::rename(Experiments = condition) %>%
        dplyr::filter((value >=200) & (value <=1200)) %>%
        ggplot2::ggplot(aes(y = value, x = Identity, color = Valence)) +
        tidybayes::stat_halfeye(aes(fill = Valence), alpha = 0.7) +
        stat_summary(aes(group = Valence, color = Valence), fun = mean, geom = "line") +
        # geom_vline(data = p_ms_rt1_vlines, aes(xintercept = value, colour = Valence), linetype = "dashed") +
        labs(x = expression("Self-Referential"), 
             y = expression('Posteior of reaction times')) +
        scale_colour_brewer(palette = "Dark2") +
        scale_fill_brewer(palette = "Dark2") +
        # facet_grid(~ params , scales = "free_y") +
        theme_apa(base_size = 20)
        # facet_wrap(~Identity) + 
        #theme_apa(base_size = 20)

df_exp4a_rt1_hdi <- df_exp4a_m1_plot_rt %>%
        tidyr::separate(term, c('Match', 'Identity', 'Valence')) %>% 
        dplyr::mutate(Valence = factor(Valence, levels = c('Good', 'Neutral', 'Bad')),
                      Identity = factor(Identity, levels = c("Self", "Other"))) %>%
        dplyr::filter(Match == 'M') %>%
        dplyr::select(-c('Match')) %>%
        # dplyr::mutate(term_diff = stringr::str_sub(.$term_diff, end = -8)) %>%
        tidyr::pivot_wider(.,
                           id_cols = c('.chain', '.iteration', '.draw'),
                           values_from = 'value',
                           names_from = c('Valence', 'Identity')) %>%
        dplyr::select(-c('.chain', '.iteration', '.draw')) %>%
        bayestestR::describe_posterior(.,
                                       ci = 0.95,
                                       ci_method = 'hdi',
                                       test = c("p_direction", "p_significance"),
                                       BF = 1,
                                       centrality = "median")
# prepare for reporting results:
# neutral self
rep_m_exp4a_rt_neut_s <- df_exp4a_rt1_hdi %>% dplyr::filter(Parameter == "Neutral_Self") %>% dplyr::select(Median) %>% 
        dplyr::pull() %>%
        round(., digits = 0)
rep_ll_exp4a_rt_neut_s <- df_exp4a_rt1_hdi %>% dplyr::filter(Parameter == "Neutral_Self") %>% dplyr::select(CI_low) %>% 
        dplyr::pull() %>%
        round(., digits = 0)
rep_ul_exp4a_rt_neut_s <- df_exp4a_rt1_hdi %>% dplyr::filter(Parameter == "Neutral_Self") %>% dplyr::select(CI_high) %>% 
        dplyr::pull() %>%
        round(., digits = 0)

# good self
rep_m_exp4a_rt_good_s <- df_exp4a_rt1_hdi %>% dplyr::filter(Parameter == "Good_Self") %>% dplyr::select(Median) %>% 
        dplyr::pull() %>%
        round(., digits = 0)
rep_ll_exp4a_rt_good_s <- df_exp4a_rt1_hdi %>% dplyr::filter(Parameter == "Good_Self") %>% dplyr::select(CI_low) %>% 
        dplyr::pull() %>%
        round(., digits = 0)
rep_ul_exp4a_rt_good_s <- df_exp4a_rt1_hdi %>% dplyr::filter(Parameter == "Good_Self") %>% dplyr::select(CI_high) %>% 
        dplyr::pull() %>%
        round(., digits = 0)

# bad self
rep_m_exp4a_rt_bad_s <- df_exp4a_rt1_hdi %>% dplyr::filter(Parameter == "Bad_Self") %>% dplyr::select(Median) %>% 
        dplyr::pull() %>%
        round(., digits = 0)
rep_ll_exp4a_rt_bad_s <- df_exp4a_rt1_hdi %>% dplyr::filter(Parameter == "Bad_Self") %>% dplyr::select(CI_low) %>% 
        dplyr::pull() %>%
        round(., digits = 0)
rep_ul_exp4a_rt_bad_s <- df_exp4a_rt1_hdi %>% dplyr::filter(Parameter == "Bad_Self") %>% dplyr::select(CI_high) %>% 
        dplyr::pull() %>%
        round(., digits = 0)

# Neutral other
rep_m_exp4a_rt_neut_o <- df_exp4a_rt1_hdi %>% dplyr::filter(Parameter == "Neutral_Other") %>% dplyr::select(Median) %>% 
        dplyr::pull() %>%
        round(., digits = 0)
rep_ll_exp4a_rt_neut_o <- df_exp4a_rt1_hdi %>% dplyr::filter(Parameter == "Neutral_Other") %>% dplyr::select(CI_low) %>% 
        dplyr::pull() %>%
        round(., digits = 0)
rep_ul_exp4a_rt_neut_o <- df_exp4a_rt1_hdi %>% dplyr::filter(Parameter == "Neutral_Other") %>% dplyr::select(CI_high) %>% 
        dplyr::pull() %>%
        round(., digits = 0)

# good other
rep_m_exp4a_rt_good_o <- df_exp4a_rt1_hdi %>% dplyr::filter(Parameter == "Good_Other") %>% dplyr::select(Median) %>% 
        dplyr::pull() %>%
        round(., digits = 0)
rep_ll_exp4a_rt_good_o <- df_exp4a_rt1_hdi %>% dplyr::filter(Parameter == "Good_Other") %>% dplyr::select(CI_low) %>% 
        dplyr::pull() %>%
        round(., digits = 0)
rep_ul_exp4a_rt_good_o <- df_exp4a_rt1_hdi %>% dplyr::filter(Parameter == "Good_Other") %>% dplyr::select(CI_high) %>% 
        dplyr::pull() %>%
        round(., digits = 0)

# bad other
rep_m_exp4a_rt_bad_o <- df_exp4a_rt1_hdi %>% dplyr::filter(Parameter == "Bad_Other") %>% dplyr::select(Median) %>% 
        dplyr::pull() %>%
        round(., digits = 0)
rep_ll_exp4a_rt_bad_o <- df_exp4a_rt1_hdi %>% dplyr::filter(Parameter == "Bad_Other") %>% dplyr::select(CI_low) %>% 
        dplyr::pull() %>%
        round(., digits = 0)
rep_ul_exp4a_rt_bad_o <- df_exp4a_rt1_hdi %>% dplyr::filter(Parameter == "Bad_Other") %>% dplyr::select(CI_high) %>% 
        dplyr::pull() %>%
        round(., digits = 0)

df_exp4a_m1_plot_rt_diff_wide <- df_exp4a_m1_plot_rt %>%
        # tidyr::unite(term, c('params', 'Identity', 'Valence')) %>%
        dplyr::ungroup() %>%
        dplyr::select(term, `.chain`, `.iteration`, `.draw`, `value`) %>%
        tidyr::pivot_wider(names_from = c(term), values_from = value) %>%   # long to wide
        dplyr::mutate(
                diff_GN_S_RT_M = M_Self_Good - M_Self_Neutral,
                diff_BN_S_RT_M = M_Self_Bad - M_Self_Neutral,
                diff_GB_S_RT_M = M_Self_Good - M_Self_Bad,
                diff_GN_O_RT_M = M_Other_Good - M_Other_Neutral,
                diff_BN_O_RT_M = M_Other_Bad - M_Other_Neutral,
                diff_GB_O_RT_M = M_Other_Good - M_Other_Bad,
                diff_SO_G_RT_M = M_Self_Good - M_Other_Good,
                diff_SO_N_RT_M = M_Self_Neutral - M_Other_Neutral,
                diff_SO_B_RT_M = M_Self_Bad - M_Other_Bad,
                diff_diff_GN_SO = diff_GN_S_RT_M - diff_GN_O_RT_M,
                diff_diff_GB_SO = diff_GB_S_RT_M - diff_GB_O_RT_M,
                diff_diff_BN_SO = diff_BN_S_RT_M - diff_BN_O_RT_M
                ) %>%
  dplyr::select(`.chain`, `.iteration`, `.draw`,
               diff_GN_S_RT_M:diff_diff_BN_SO) 

df_exp4a_m1_plot_rt_diff<- df_exp4a_m1_plot_rt_diff_wide %>%
  tidyr::pivot_longer(cols = diff_GN_S_RT_M:diff_diff_BN_SO, names_to = "term_diff", values_to =  "value") %>%  # wide to long
  dplyr::mutate(term_diff = factor(term_diff, levels = c('diff_GN_S_RT_M', 'diff_BN_S_RT_M', 'diff_GB_S_RT_M', 
                                                         'diff_GN_O_RT_M', 'diff_BN_O_RT_M', 'diff_GB_O_RT_M', 
                                                         'diff_SO_G_RT_M', 'diff_SO_N_RT_M', 'diff_SO_B_RT_M',
                                                         'diff_diff_GN_SO', 'diff_diff_GB_SO', 'diff_diff_BN_SO')))
p_exp4a_rt_diff_val <- df_exp4a_m1_plot_rt_diff %>%
        dplyr::filter(str_detect(term_diff, '_S_RT_M|_O_RT_M')) %>%
        dplyr::mutate(Identity = dplyr::case_when(grepl("_S", term_diff) ~ "Self",
                                                  grepl("_O", term_diff) ~"Other"),
                      Identity = factor(Identity, levels = c("Self", "Other"))) %>%
        dplyr::filter(!str_detect(term_diff, '_GB_|_BN')) %>%
        dplyr::mutate(term_diff = stringr::str_sub(.$term_diff, end = -8)) %>%
        dplyr::mutate(term_diff = dplyr::case_when(#grepl("_GB", term_diff) ~ "Good vs. Bad",
                                                   grepl("_GN", term_diff) ~"Good vs. Neutral",
                                                   #grepl("_BN", term_diff) ~ "Bad vs. Neutral",
                                                   ),
                      #term_diff = factor(term_diff, levels = c("Good vs. Bad", "Good vs. Neutral", "Bad vs. Neutral")),
                      ) %>%
        # dplyr::filter(str_detect(term_diff, '_SO_')) %>%
        #dplyr::filter(!str_detect(term_diff, '_GB_')) %>%
        # tidyr::unite(term_diff, c('Identity', 'term_diff')) %>%
        # dplyr::filter((value >= -1.5) & (value <= 1.5)) %>%
        dplyr::rename(conditions = term_diff) %>%
        ggplot2::ggplot(aes(x = value, fill = after_stat(x < 0))) + # y = fct_rev(conditions), 
        tidybayes::stat_halfeye() +
        geom_vline(xintercept = 0, linetype = "dashed") +
        scale_fill_manual(values = c('skyblue', 'gray80'),
                    breaks = c(TRUE, FALSE),
                    name = "Effect" , labels = c("Yes", "No")) + 
        # scale_fill_manual(values = c('gray80', 'skyblue')) +
        labs(x = expression("Valence effect (RTs)"),
             y = expression("Good vs. Neutral")) + 
        scale_x_continuous(breaks=seq(-25, 50, 50)) +
        facet_wrap( ~ Identity, nrow = 1) + 
        theme_apa(base_size = 18)

p_exp4a_rt_diff_id <- df_exp4a_m1_plot_rt_diff %>%
        dplyr::filter(str_detect(term_diff, '_SO_')) %>%
        dplyr::mutate(term_diff = dplyr::case_when(grepl("_B", term_diff) ~ "Bad",
                                                   grepl("_N", term_diff) ~"Neutral",
                                                   grepl("_G", term_diff) ~ "Good"),
                      term_diff = factor(term_diff, levels = c("Good", "Neutral", "Bad"))) %>%
        # dplyr::filter(!str_detect(term_diff, '_GB_')) %>%
        # tidyr::unite(term_diff, c('Identity', 'term_diff')) %>%
        # dplyr::filter((value >= -1.5) & (value <= 1.5)) %>%
        dplyr::rename(conditions = term_diff) %>%
        ggplot2::ggplot(aes(x = value, y = fct_rev(conditions), fill = after_stat(x < 0))) +
        tidybayes::stat_halfeye() +
        geom_vline(xintercept = 0, linetype = "dashed") +
        scale_fill_manual(values = c('skyblue', 'gray80'),
                    breaks = c(TRUE, FALSE),
                    name = "Effect" , labels = c("Yes", "No")) + 
        # scale_fill_manual(values = c('skyblue')) +   # note: only one value here, used blue here
        labs(x = expression("Effect of Self-relevance (RT)"),
             y = expression("Self vs. Other"))

p_exp4a_rt1_diff_diff <- df_exp4a_m1_plot_rt_diff %>%
        dplyr::filter(str_detect(term_diff, '_diff_')) %>%
        dplyr::mutate(term_diff = dplyr::case_when(grepl("_GB", term_diff) ~ "GB",
                                                   grepl("_GN", term_diff) ~"GN",
                                                   grepl("_BN", term_diff) ~ "BN"),
                      term_diff = factor(term_diff, levels = c("GN", "GB", "BN"))) %>%
        #dplyr::filter(!str_detect(term_diff, '_GB_')) %>%
        # tidyr::unite(term_diff, c('Identity', 'term_diff')) %>%
        # dplyr::filter((value >= -1.5) & (value <= 1.5)) %>%
        dplyr::rename(conditions = term_diff) %>%
        ggplot2::ggplot(aes(x = value, y = fct_rev(conditions), fill = after_stat(x > 0))) +
        tidybayes::stat_halfeye() +
        geom_vline(xintercept = 0, linetype = "dashed") +
        scale_fill_manual(values = c('skyblue', 'gray80'),
                    breaks = c(TRUE, FALSE),
                    name = "Effect" , labels = c("Yes", "No")) +
        # scale_fill_manual(values = c('skyblue')) +   # note: only one value here, used blue here
        labs(x = expression("Interaction effect on RT"),
             y = expression("Difference (Self vs. Other) of difference (Valence)")) +
        theme_apa(base_size = 18)

test_res_rt_4a <- bayestestR::sexit(df_exp4a_m1_plot_rt_diff_wide[, c(4, 7, 13)])
test_res_rt_4a

```

```{r plot-exp4a-BGLM, fig.cap="exp4a: Results of Bayesian GLM analysis.",  fig.height=9, fig.width=12, warning=FALSE}
library(patchwork)
p1 <- (p_exp4a_rt1 + exp4a_sdt_p) #+ plot_layout(nrow = 1, guides = "collect") 
p2 <- (p_exp4a_rt_diff_val + p_exp4a_dprime1_diff_val) + plot_layout(nrow = 1, byrow = FALSE, guides = "collect") 
# p3 <- p_exp4a_rt_diff_id + p_exp4a_dprime1_diff_id + plot_layout(nrow = 1, byrow = FALSE, guides = "collect") 
#p1/ 
#        p2  +  plot_layout(nrow = 2)  
        # p3 +  plot_layout(nrow = 3) 
```

A similar but more robust pattern was found for RTs in matched trials. For the “self” condition, when a good character was presented inside the shapes, the RTs (median = `r rep_m_exp4a_rt_good_s`, 95% HDI [`r rep_ll_exp4a_rt_good_s` `r rep_ul_exp4a_rt_good_s`]) were faster than when a neutral character (median = `r rep_m_exp4a_rt_neut_s`, 95% HDI [`r rep_ll_exp4a_rt_neut_s` `r rep_ul_exp4a_rt_neut_s`]) was inside, the effect (median = -8, 95% HDI [-17, 2]) has a 94.55% probability of being negative (< 0) and 94.50% of being significant (< -0.05). In contrast, when the shapes referred to other, RTs for shapes with good character inside (median = `r rep_m_exp4a_rt_good_o`, 95% HDI [`r rep_ll_exp4a_rt_good_o` `r rep_ul_exp4a_rt_good_o`]) were slower than those with neutral character inside (median =  `r rep_m_exp4a_rt_neut_o`, 95% HDI [`r rep_ll_exp4a_rt_neut_o` `r rep_ul_exp4a_rt_neut_o`]), the effect (median = 12, 95% HDI [-4, 28]) has a 93.00% probability of being positive (> 0) and 92.83% of being significant (> 0.05). The difference between these effects (median = -19, 95% HDI [-43, 4]) has a 94.90% probability of being negative (< 0) and 94.88% of being significant (< -0.05).

In experiment 4b, where moral characters were task-relevant and “self” vs “other” were task-irrelevant, we found a main effect of moral character: performance for shapes associated with good characters was better than other-related conditions on both *d'* and reaction times. This pattern, again, shows a robust prioritization effect of good character.

```{r 4b_BGLMM_sdt, echo=FALSE, results='hide', warning=FALSE, message=FALSE}
exp_name <- '4b'
exp4b_sdt_m1 <- fun_sdt_val_id(exp_name)

# extract the population level parameters
# criteria
df_exp4b_sdt_m1_plot <- exp4b_sdt_m1 %>%
  tidybayes::gather_draws(`b_.*`, regex = TRUE) %>% # get the traces of population level parameters
  # create two columns for two independent factors.
  dplyr::mutate(Valence = dplyr::case_when(grepl("Neutral", .variable) ~ "Neutral",
                                           grepl("Bad", .variable) ~"Bad",
                                           grepl("Good", .variable) ~"Good"),
                Identity = dplyr::case_when(grepl("Self", .variable) ~ "Self",
                                           grepl("Other", .variable) ~"Other"),
                params = dplyr::case_when(grepl("ismatch", .variable) ~ "dprime",
                                           !grepl("ismatch", .variable) ~"criterion"),
                params = factor(params, levels = c('dprime', 'criterion')),
                Identity = factor(Identity, levels = c("Self", "Other")),
                Valence = factor(Valence, levels = c('Good', 'Neutral',  'Bad'))) 

exp4b_sdt_p <- df_exp4b_sdt_m1_plot %>%
  dplyr::filter(params == 'dprime') %>%  # select only d prime
  ggplot2::ggplot(aes(x = Valence, y = .value, color = Identity)) +
  tidybayes::stat_halfeye(aes(fill = Identity), alpha = 0.7) + # position=position_dodge(width = 0.1)
  stat_summary(aes(group = Identity, color = Identity), fun = mean, geom = "line") +
  labs(x = expression("Valence"), 
      y = expression(paste("Posteior of sensitivity ", italic("d'"), sep = ' '))) +
  #scale_colour_brewer(palette = "Dark2") +
  #scale_fill_brewer(palette = "Dark2") +
  theme_apa(base_size = 18) 

df_exp4b_sdt_hdi <- df_exp4b_sdt_m1_plot %>%
        dplyr::ungroup() %>%
        dplyr::filter(params == 'dprime') %>%
        dplyr::select(-c('.variable', 'params')) %>%
        # dplyr::mutate(term_diff = stringr::str_sub(.$term_diff, end = -8)) %>%
        tidyr::pivot_wider(.,
                           id_cols = c('.chain', '.iteration', '.draw'),
                           values_from = '.value',
                           names_from = c('Valence', 'Identity')) %>%
        dplyr::select(-c('.chain', '.iteration', '.draw')) %>%
        bayestestR::describe_posterior(.,
                                       ci = 0.95,
                                       ci_method = 'hdi',
                                       test = c("p_direction", "p_significance"),
                                       BF = 1,
                                       centrality = "median") %>%
        dplyr::select(Parameter, Median, CI_low, CI_high) %>%
        tidyr::pivot_longer(cols = Median:CI_high, names_to = "Index", values_to = "Values") %>%
        tidyr::unite("Indicies", Parameter:Index) %>%
        dplyr::mutate(Values = round(Values, digits = 3))

df_exp4b_sdt_m1_plot_diff_wide <- df_exp4b_sdt_m1_plot %>%
        tidyr::unite(term, c('params', 'Identity', 'Valence')) %>%
        dplyr::ungroup() %>%
        dplyr::select(term, `.chain`, `.iteration`, `.draw`, `.value`) %>%
        tidyr::pivot_wider(names_from = c(term), values_from = .value) %>%   # long to wide
        dplyr::mutate(
                diff_GN_dprm_S = dprime_Self_Good - dprime_Self_Neutral,
                diff_BN_dprm_S = dprime_Self_Bad - dprime_Self_Neutral,
                diff_GB_dprm_S = dprime_Self_Good - dprime_Self_Bad,
                diff_GN_dprm_O = dprime_Other_Good - dprime_Other_Neutral,
                diff_BN_dprm_O = dprime_Other_Bad - dprime_Other_Neutral,
                diff_GB_dprm_O = dprime_Other_Good - dprime_Other_Bad,
                diff_SO_dprm_G = dprime_Self_Good - dprime_Other_Good,
                diff_SO_dprm_N =  dprime_Self_Neutral - dprime_Other_Neutral,
                diff_SO_dprm_B =  dprime_Self_Bad - dprime_Other_Bad,
                #diff_diff_SO_GN = diff_SO_dprm_G - diff_SO_dprm_N
                ) %>%
  dplyr::select(`.chain`, `.iteration`, `.draw`,
               diff_GN_dprm_S, diff_BN_dprm_S, diff_GB_dprm_S,
               diff_GN_dprm_O, diff_BN_dprm_O, diff_GB_dprm_O, 
               diff_SO_dprm_G, diff_SO_dprm_N, diff_SO_dprm_B,
               # diff_diff_SO_GN
               )

df_exp4b_sdt_m1_plot_diff <- df_exp4b_sdt_m1_plot_diff_wide %>%
  tidyr::pivot_longer(cols = diff_GN_dprm_S:diff_SO_dprm_B, names_to = "term_diff", values_to =  "value") %>%  # wide to long
  dplyr::mutate(term_diff = factor(term_diff, levels = c('diff_GB_dprm_S', 'diff_GN_dprm_S', 'diff_BN_dprm_S', 
                                                         'diff_GB_dprm_O', 'diff_GN_dprm_O', 'diff_BN_dprm_O', 
                                                         'diff_SO_dprm_G', 'diff_SO_dprm_N', 'diff_SO_dprm_B')))

df_exp4b_sdt_m1_diff_hdi <- df_exp4b_sdt_m1_plot_diff %>%
        dplyr::group_by(term_diff) %>%
        median_hdi(value) %>%
        dplyr::ungroup() %>%
        dplyr::select(term_diff, value, `.lower`, `.upper`) %>%
        tidyr::pivot_longer(cols = value:`.upper`, names_to = "Index", values_to = "Values") %>%
        tidyr::unite("Indicies", term_diff:Index) %>%
        dplyr::mutate(Values = round(Values, digits = 3),
                      Indicies = str_remove_all(Indicies, "\\."))
        
p_exp4b_dprime1_diff_val <- df_exp4b_sdt_m1_plot_diff %>%
        dplyr::filter(str_detect(term_diff, '_dprm_S|_dprm_O')) %>%
        dplyr::mutate(Identity = dplyr::case_when(grepl("_S", term_diff) ~ "Self",
                                           grepl("_O", term_diff) ~"Other"),
                      Identity = factor(Identity, levels = c("Self", "Other"))) %>%
        dplyr::mutate(term_diff = stringr::str_sub(.$term_diff, end = -8)) %>%
        dplyr::mutate(term_diff = dplyr::case_when(grepl("_GB", term_diff) ~ "Good vs. Bad",
                                                   grepl("_GN", term_diff) ~"Good vs. Neutral",
                                                   grepl("_BN", term_diff) ~ "Bad vs. Neutral"),
                      term_diff = factor(term_diff, levels = c("Good vs. Bad", "Good vs. Neutral", "Bad vs. Neutral"))) %>%
        #dplyr::filter((value >= -1.5) & (value <= 1.5)) %>%
        dplyr::rename(conditions = term_diff) %>%
        ggplot2::ggplot(aes(x = value, y = fct_rev(conditions), fill = after_stat(x > 0))) +
        tidybayes::stat_halfeye() +
        geom_vline(xintercept = 0, linetype = "dashed") +
        scale_fill_manual(values = c('skyblue', 'gray80'),
                    breaks = c(TRUE, FALSE),
                    name = "Effect" , labels = c("Yes", "No")) + 
        # scale_fill_manual(values = c('gray80', 'skyblue')) +
        labs(x = expression(paste("Valence effect on ", italic("d"), " prime", sep = ' ')),
             y = expression("Contrasts")) + 
        facet_wrap( ~ Identity, nrow = 1) +
        theme_apa(base_size = 20)

p_exp4b_dprime1_diff_id <- df_exp4b_sdt_m1_plot_diff %>%
        dplyr::filter(str_detect(term_diff, '_SO_')) %>%
        dplyr::filter(!str_detect(term_diff, '_B')) %>%
        dplyr::mutate(term_diff = dplyr::case_when(# grepl("_B", term_diff) ~ "Bad",
                                                   grepl("_N", term_diff) ~"Neutral",
                                                   grepl("_G", term_diff) ~ "Good"),
                      term_diff = factor(term_diff, levels = c("Good", "Neutral", "Bad"))) %>%
        #dplyr::filter(!str_detect(term_diff, '_GB_')) %>%
        # tidyr::unite(term_diff, c('Identity', 'term_diff')) %>%
        dplyr::filter((value >= -1.5) & (value <= 1.5)) %>%
        dplyr::rename(conditions = term_diff) %>%
        ggplot2::ggplot(aes(x = value, fill = after_stat(x > 0))) + #y = fct_rev(conditions), 
        tidybayes::stat_halfeye() +
        geom_vline(xintercept = 0, linetype = "dashed") +
        scale_fill_manual(values = c('skyblue', 'gray80'),
                    breaks = c(TRUE, FALSE),
                    name = "Effect" , labels = c("Yes", "No")) +
        # scale_fill_manual(values = c('skyblue')) +   # note: only one value here, used blue here
        labs(x = expression(paste("Effect of self-relevance (", italic("d"), "')", sep = ' ')),
             y = expression("Self vs. Other"))+ 
        scale_x_continuous(breaks=seq(-0.25, 0.75, 0.5)) +
        facet_wrap( ~ conditions, nrow = 1) +
        theme_apa(base_size = 18)

test_res_sdt_4b <- bayestestR::sexit(df_exp4b_sdt_m1_plot_diff_wide[, c(10, 11, 12)])
test_res_sdt_4b
```

Most importantly, we found evidence that task-irrelevant labels, “self” or “other”, also played a role. For shapes associated with good character, the *d* prime was greater when shapes had a "self" inside than with "other" inside ($mean_{diff}$ = 0.14, 95% HDI [-0.05, 0.34]) has a 92.35% probability of being positive (> 0) and 81.80% of being significant (> 0.05). However, the difference did not occur when the target shape where associated with "neutral" ($mean_{diff}$ = 0.04, 95% HDI [-0.13, 0.22]) and has a 67.20% probability of being positive (> 0) and 44.80% of being significant (> 0.05). Neither for the "bad" person condition: $mean_{diff}$  = 0.10, 95% HDI [-0.16, 0.37]) has a 77.03% probability of being positive (> 0) and 64.62% of being significant (> 0.05). 

```{r 4b_BGLMM_rt, echo=FALSE, results='hide', warning=FALSE, message=FALSE}
exp_name <- '4b'
exp4b_rt_m1 <- fun_rt_val_id(exp_name)

df_exp4b_m1_plot_rt <- exp4b_rt_m1 %>%
        tidybayes::gather_draws(`b_.*`, regex = TRUE)  %>%
        dplyr::mutate(.variable = gsub("b_", "", .variable)) %>%
        tidyr::pivot_wider(names_from = c(.variable), values_from = .value) %>%
        dplyr::mutate(NM_Self_Neutral = Intercept,               # calculate the differences between conditions
                      NM_Self_Bad = Intercept  + ValenceBad,
                      NM_Self_Good = Intercept  + ValenceGood ,
                      NM_Other_Neutral = Intercept  + IdentityOther,               # calculate the differences between conditions
                      NM_Other_Bad = Intercept  + ValenceBad  + `IdentityOther:ValenceBad`,
                      NM_Other_Good = Intercept  + ValenceGood + `IdentityOther:ValenceGood`,
                      M_Self_Neutral = Intercept  + ismatch,
                      M_Self_Bad = Intercept  + ismatch + `ismatch:ValenceBad`,
                      M_Self_Good = Intercept  + ismatch + `ismatch:ValenceGood`,
                      M_Other_Neutral = Intercept  + ismatch + `ismatch:IdentityOther`,
                      M_Other_Bad = Intercept  + ismatch + `ismatch:IdentityOther` + `ismatch:IdentityOther:ValenceBad`,
                      M_Other_Good = Intercept  + ismatch + `ismatch:IdentityOther` + `ismatch:IdentityOther:ValenceGood`) %>%
        dplyr::select(`.chain`, `.iteration`, `.draw`, contains('M_')) %>%
        tidyr::pivot_longer(cols = NM_Self_Neutral:M_Other_Good, names_to = "term", values_to =  "value") %>%  # wide to long
  dplyr::mutate(term = factor(term, levels = c('NM_Self_Neutral', 'NM_Self_Bad', 'NM_Self_Good',
                                               'NM_Other_Neutral', 'NM_Other_Bad', 'NM_Other_Good',
                                               'M_Self_Neutral', 'M_Self_Bad', 'M_Self_Good',
                                               'M_Other_Neutral', 'M_Other_Bad', 'M_Other_Good')),
                value = exp(value),
                value = value * 1000) 

p_exp4b_rt1 <- df_exp4b_m1_plot_rt %>%
        tidyr::separate(term, c('Match', 'Identity', 'Valence')) %>% 
        dplyr::mutate(Valence = factor(Valence, levels = c('Good', 'Neutral', 'Bad')),
                      Identity = factor(Identity, levels = c("Self", "Other"))) %>%
        dplyr::filter(Match == 'M') %>%
        # dplyr::rename(Experiments = condition) %>%
        dplyr::filter((value >=200) & (value <=1200)) %>%
        ggplot2::ggplot(aes(y = value, x = Valence, color = Identity)) +
        tidybayes::stat_halfeye(aes(fill = Identity), alpha = 0.7) +
        stat_summary(aes(group = Identity, color = Identity), fun = mean, geom = "line") +
        # geom_vline(data = p_ms_rt1_vlines, aes(xintercept = value, colour = Valence), linetype = "dashed") +
        labs(x = expression("Valence"), 
             y = expression('Posteior of reaction times')) +
        #scale_colour_brewer(palette = "Dark2") +
        #scale_fill_brewer(palette = "Dark2")  +
        theme_apa(base_size = 18)

df_exp4b_m1_plot_rt_diff_wide <- df_exp4b_m1_plot_rt %>%
        # tidyr::unite(term, c('params', 'Identity', 'Valence')) %>%
        dplyr::ungroup() %>%
        dplyr::select(term, `.chain`, `.iteration`, `.draw`, `value`) %>%
        tidyr::pivot_wider(names_from = c(term), values_from = value) %>%   # long to wide
        dplyr::mutate(
                diff_GN_S_RT_M = M_Self_Good - M_Self_Neutral,
                diff_BN_S_RT_M = M_Self_Bad - M_Self_Neutral,
                diff_GB_S_RT_M = M_Self_Good - M_Self_Bad,
                diff_GN_O_RT_M = M_Other_Good - M_Other_Neutral,
                diff_BN_O_RT_M = M_Other_Bad - M_Other_Neutral,
                diff_GB_O_RT_M = M_Other_Good - M_Other_Bad,
                diff_SO_G_RT_M = M_Self_Good - M_Other_Good,
                diff_SO_N_RT_M = M_Self_Neutral - M_Other_Neutral,
                diff_SO_B_RT_M = M_Self_Bad - M_Other_Bad
                ) %>%
        dplyr::select(`.chain`, `.iteration`, `.draw`,
                       diff_GN_S_RT_M:diff_SO_B_RT_M) 

df_exp4b_m1_plot_rt_diff <- df_exp4b_m1_plot_rt_diff_wide %>%
  tidyr::pivot_longer(cols = diff_GN_S_RT_M:diff_SO_B_RT_M, names_to = "term_diff", values_to =  "value") %>%  # wide to long
  dplyr::mutate(term_diff = factor(term_diff, levels = c('diff_GN_S_RT_M', 'diff_BN_S_RT_M', 'diff_GB_S_RT_M', 
                                                         'diff_GN_O_RT_M', 'diff_BN_O_RT_M', 'diff_GB_O_RT_M', 
                                                         'diff_SO_G_RT_M', 'diff_SO_N_RT_M', 'diff_SO_B_RT_M')))

df_exp4b_m1_rt_diff_hdi <- df_exp4b_m1_plot_rt_diff %>%
        dplyr::group_by(term_diff) %>%
        median_hdi(value) %>%
        dplyr::ungroup() %>%
        dplyr::select(term_diff, value, `.lower`, `.upper`) %>%
        tidyr::pivot_longer(cols = value:`.upper`, names_to = "Index", values_to = "Values") %>%
        tidyr::unite("Indicies", term_diff:Index) %>%
        dplyr::mutate(Values = round(Values, digits = 0),
                      Indicies = str_remove_all(Indicies, "\\."))

p_exp4b_rt_diff_val <- df_exp4b_m1_plot_rt_diff %>%
        dplyr::filter(str_detect(term_diff, '_S_RT_M|_O_RT_M')) %>%
        dplyr::mutate(Identity = dplyr::case_when(grepl("_S", term_diff) ~ "Self",
                                                  grepl("_O", term_diff) ~"Other"),
                      Identity = factor(Identity, levels = c("Self", "Other"))) %>%
        dplyr::mutate(term_diff = stringr::str_sub(.$term_diff, end = -8)) %>%
        dplyr::mutate(term_diff = dplyr::case_when(grepl("_GB", term_diff) ~ "Good vs. Bad",
                                                   grepl("_GN", term_diff) ~"Good vs. Neutral",
                                                   grepl("_BN", term_diff) ~ "Bad vs. Neutral"),
                      term_diff = factor(term_diff, levels = c("Good vs. Bad", "Good vs. Neutral", "Bad vs. Neutral"))) %>%
        # dplyr::filter((value >= -1.5) & (value <= 1.5)) %>%
        dplyr::rename(conditions = term_diff) %>%
        ggplot2::ggplot(aes(x = value, y = fct_rev(conditions), fill = after_stat(x < 0))) +
        tidybayes::stat_halfeye() +
        geom_vline(xintercept = 0, linetype = "dashed") +
        scale_fill_manual(values = c('skyblue', 'gray80'),
                    breaks = c(TRUE, FALSE),
                    name = "Effect" , labels = c("Yes", "No")) + 
        # scale_fill_manual(values = c('gray80', 'skyblue')) +
        labs(x = expression("Valence effect (RTs)"),
             y = expression("Contrasts")) + 
        facet_wrap( ~ Identity, nrow = 1) +
        theme_apa(base_size = 20)

p_exp4b_rt_diff_id <- df_exp4b_m1_plot_rt_diff %>%
        dplyr::filter(str_detect(term_diff, '_SO_')) %>%
        dplyr::filter(!str_detect(term_diff, '_B')) %>%
        dplyr::mutate(term_diff = dplyr::case_when(# grepl("_B", term_diff) ~ "Bad",
                                                   grepl("_N", term_diff) ~"Neutral",
                                                   grepl("_G", term_diff) ~ "Good"),
                      term_diff = factor(term_diff, levels = c("Good", "Neutral"))) %>%
        # dplyr::filter(!str_detect(term_diff, '_GB_')) %>%
        # tidyr::unite(term_diff, c('Identity', 'term_diff')) %>%
        # dplyr::filter((value >= -1.5) & (value <= 1.5)) %>%
        dplyr::rename(conditions = term_diff) %>%
        ggplot2::ggplot(aes(x = value, fill = after_stat(x < 0))) + # , y = fct_rev(conditions)
        tidybayes::stat_halfeye() +
        geom_vline(xintercept = 0, linetype = "dashed") +
        scale_fill_manual(values = c('skyblue', 'gray80'),
                    breaks = c(TRUE, FALSE),
                    name = "Effect" , labels = c("Yes", "No")) + 
        # scale_fill_manual(values = c('skyblue')) +   # note: only one value here, used blue here
        labs(x = expression("Effect of self-relevance (RTs)"),
             y = expression("Self vs. Other")) +
        scale_x_continuous(breaks=seq(-75, 50, 50)) +
        facet_wrap( ~ conditions, nrow = 1) +
        theme_apa(base_size = 18)

test_res_rt_4b <- bayestestR::sexit(df_exp4b_m1_plot_rt_diff_wide[, c(10, 11, 12)])
test_res_rt_4b
```

```{r plot-exp4b-BGLM, fig.cap="exp4b: Results of Bayesian GLM analysis.",  fig.height=9, fig.width=12, warning=FALSE}
library(patchwork)
p3 <- (p_exp4b_rt1 + exp4b_sdt_p ) + plot_layout(nrow = 1, guides = "collect") 
# p2 <- p_exp4b_rt_diff_val + p_exp4b_dprime1_diff_val + plot_layout(nrow = 1, byrow = FALSE, guides = "collect") 
p4 <- (p_exp4b_rt_diff_id + p_exp4b_dprime1_diff_id) + plot_layout(nrow = 1, byrow = FALSE, guides = "collect") 
```

(ref:fig3-caption) Implicit binding between self and good characters. (A) Posterior distributions of RT under different conditions of Experiment 4a; (B) Posterior distributions of *d*-prime under different conditions of Experiment 4a; (C) Posterior distributions of RT under different conditions of Experiment 4b; (D) Posterior distributions of *d*-prime under different conditions of Experiment 4b; (E) Posterior distributions of the RT differences between good character and neutral character when self (left) and other (right) were presented inside the shapes; (F) Posterior distributions of the *d*-prime differences between good character and neutral character when self (left) and other (right) were presented inside the shapes; (G) Posterior distributions of the RT differences between self- and other-referencing conditions when good character (left) and neutral character (right) were presented inside the shapes; (H) Posterior distributions of the *d*-prime differences between self- and other-referencing conditions when good character (left) and neutral character (right) were presented inside the shapes.

```{r plot-exp4-all, fig.cap="(ref:fig3-caption)",  fig.height=10, fig.width=20, warning=FALSE}
library(patchwork)

design <- "
  1234
  5678
"
p_exp4a_rt1 + exp4a_sdt_p + p_exp4b_rt1 + exp4b_sdt_p + p_exp4a_rt_diff_val + p_exp4a_dprime1_diff_val + p_exp4b_rt_diff_id + p_exp4b_dprime1_diff_id + plot_annotation(tag_levels = 'A') + plot_layout(design = design, guides = "collect", heights = c(1, 1))

# ((p1 +plot_layout(guides = "collect")) + p3 + plot_layout(guides = "collect")) + plot_layout(ncol = 3, widths = c(1, 1, 4), guides = 'keep')
# 
# (((p_exp4a_rt1 + exp4a_sdt_p + plot_layout(guides = "collect", nrow = 1)) + 
#                 (p_exp4b_rt1 + exp4b_sdt_p + plot_layout(guides = "collect", nrow = 1)) + plot_layout(nrow = 1)) + 
#         ((p_exp4a_rt_diff_val + p_exp4a_dprime1_diff_val + plot_layout(guides = "collect", nrow = 1)) 
#          + (p_exp4b_rt_diff_id + p_exp4b_dprime1_diff_id + plot_layout(guides = "collect", nrow = 1)) + plot_layout(nrow = 1))) + 
#         plot_layout(design = design, guides = "keep") +  plot_annotation(tag_levels = 'A')  

```

The same trend appeared for the RT data. For shapes associated with good character, having a "self" inside shapes reduced the reaction times as compared to having an "other" inside the shapes ($mean_{diff}$ = -55, 95% HDI [-75, -35]) has a 100% probability of being negative (< 0) and 100.00% of being significant (< -0.05). However, when the shapes were associated with the neutral character, having a "self" inside shapes increased the RTs: $mean_{diff}$ = 11, 95% HDI [1, 21]) has a 98.20% probability of being positive (> 0) and 98.15% of being significant (> 0.05). While having "self" slightly increased the RT than having "other" inside the shapes for the bad character: $mean_{diff}$ = 5, 95% HDI [-17, 27]) has a 69.45% probability of being positive (> 0) and 69.27% of being significant (> 0.05), See Figure \@ref(fig:plot-exp4-all).

# Discussion
In this study, we investigated the primacy of morality in cognitive processes through systematically manipulating the factors that are central to the information processing of morality. First, we found a robust prioritization of good character in response times and *d*’ scores for the shape-label matching tasks across experiments. Second, to pinpoint the underlying processes of the effect, the analyses revealed that a self-referencing process was the fundamental driver of these effects, consistent with the self-binding account; that is, when a stimulus refers to the self, activation of self-representation enhances the binding of external input with internal knowledge through which self-related information can be integrated and optimized. The valence account, on the other hand, which posits that the prioritization effect was derived from a general positivity bias towards all (self and others), was not supported by the findings. Importantly, the prioritization effects emerged regardless of whether the relationship between moral character and oneself was task relevant. Collectively, participants tend to attribute moral character to themselves rather than others, leading to prioritized responses to self perceived moral character in decision making. 

The current study provided robust evidence for the prioritization of good character in perceptual decision-making. Though the primacy of morality has been argued in social psychology, whether morality is prioritized in information processing has been disputed. For instance, @anderson_visual_2011 reported that faces associated with bad social behavior capture attention more rapidly, but an independent team failed to replicate the effect [@stein_no_2017]. In another study, @gantman_moral_2014 found that moral words are more likely to be judged as words when it was presented subliminally. But this effect may be caused by semantic priming instead of morality [@firestone_cognition_2015; @jussim_interpretations_2016]. To overcome this issue, we employed a shape-label matching task to eliminate the semantic priming effect for two reasons. First, associations between shapes and moral characters were acquired during the instruction phase, semantic priming from pre-existed knowledge was impossible [@lee_pre-existing_2021]. Second, there were only a few pairs of stimuli that were used and each stimulus represented different conditions, making it impossible for priming between trials. Importantly, a series of control experiments (1b, 1c, and 2) excluded other confounding factors such as familiarity, presenting sequence, or words-based associations, suggesting that it was the moral content that drove the perceived prioritization of good character. These results are in line with a growing literature on the social and relational nature of perception [@hafri_perception_2021; @xiao_perceiving_2016].

The prioritization of good character found in the current study was incongruent with previous moral perception studies, which typically reported a negativity bias, i.e., information related to bad character is processed preferentially [@anderson_visual_2011; @eiserbeck_visual_2020]. This discrepancy may result from different task types employed: while in many moral perception studies, the participants were asked to detect the existence of a stimulus, the current task asked participants to judge the associations between a shape and a person. In other words, previous studies targeted the early stages of perception, while the current task focused more on perceptual decision-making, consistent with previous work [@sui_boundaries_2013]. This discrepancy is consistent with the positivity bias in studies with emotional stimuli [@pool_attentional_2016].

The current study expanded previous moral perception studies by testing a novel account that self-referencing processing is the critical driver of the effects. Our results revealed that prioritization of good character is modulated by self-relevance: good character was prioritized when it was referred to oneself. In contrast, good character information was not prioritized when it was referred to others. The modulation effect of self-relevance was amplified when the relationship between moral character and oneself was explicit, consistent with previous studies that only positive aspects of the self are prioritized [@Hu_2020_goodme]. More importantly, the effect persisted even when the relationship between moral character and oneself was task-irrelevant, indicating  an implicit self-referencing process emerged from presenting good character and self-related information in the same display. A possible explanation for this spontaneous self-referencing of good character is that the positive moral self-view is central to our identity [@freitas_origins_2017; @strohminger_true_2017] and the motivation to maintain a moral self-view influences how we perceive [e.g., @Ma_JEPHPP_2010] and remember [e.g., @carlson_nat_comm_2020; @stanley_remembering_2019], with implications for the quality of life and wellbeing. 

Although the results here revealed the prioritization of good character in perceptual decision-making, we did not claim that the motivation of a moral self-view *penetrates* perception. The perceptual decision-making process involves processes more than just encoding the sensory inputs [@scheller_power_2022]. To fully account for the nuance of behavioral data and/or related data collected from other modules [e.g., @sui_electrophysiological_2023], we may need computational models and an integrative experimental approach [@Almaatouq_BBS_2022]. For example, sequential sampling models suggest that, when making a perceptual decision, the agent continuously accumulates evidence until the amount of evidence passes a threshold, and then a decision is made [@ratcliff_tics_2016; @forstmann_ann_rev_2016; @Hu_hitchhikers_2022]. In these models, the evidence, or decision variable, can accumulate from both sensory information but also memory [@shadlen_decision_2016]. Recently, applications of sequential sample models to perceptual matching tasks also suggest that different processes may contribute to the prioritization effect of self [@golubickis_self-prioritization_2017] or good self [@Hu_2020_goodme]. Similarly, reinforcement learning models revealed that the initial discrimination between self- and other-referencing learning lies in the learning rate [@lockwood_neural_2018]. These investigations suggest that computational models are required to disentangle the cognitive processes underlying the prioritization of good character.

# References
```{r create_r-references, echo=FALSE,results='hide'}
#r_refs(file = "r-references.bib"))
```

\begingroup
\setlength{\parindent}{-0.5in}
\setlength{\leftskip}{0.5in}

<div id = "refs"></div>
\endgroup
